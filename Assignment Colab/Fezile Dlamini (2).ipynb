{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Importing Libraries"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"###Importing libraries\nimport pandas\nimport scipy\nimport numpy\nimport matplotlib\nimport sklearn\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Loading some libraries\n### These are some of the libraries I think I will need"},{"metadata":{"trusted":true},"cell_type":"code","source":"from pandas import read_csv #for reading in csv files\nfrom pandas.plotting import scatter_matrix #for showing how one variable is affected by another\nfrom matplotlib import pyplot #for plotting graphs\nfrom sklearn.model_selection import train_test_split  # for splitting my data into train and test\nfrom sklearn.model_selection import cross_val_score  #to estimate the skill of a machine learning model on unseen data\nfrom sklearn.model_selection import StratifiedKFold  #he folds are selected so that the mean response value is approximately equal in all the folds. In the case of a dichotomous classification, this means that each fold contains roughly the same proportions of the two types of class labels\nfrom sklearn.metrics import classification_report #Visual classification reports are used to compare classification models to select models that are “redder”, e.g. have stronger classification metrics or that are more balanced.\nfrom sklearn.metrics import confusion_matrix #A confusion matrix is a table that is often used to describe the performance of a classification model (or “classifier”) on a set of test data for which the true values are known. It allows the visualization of the performance of an algorithm.\nfrom sklearn.metrics import accuracy_score  #It is the ratio of number of correct predictions to the total number of input samples\nfrom sklearn.linear_model import LogisticRegression #an algorithm for classification\nfrom sklearn.tree import DecisionTreeClassifier #create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features.\nfrom sklearn.neighbors import KNeighborsClassifier #for classification\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis  #used for modeling differences in groups i.e. separating two or more classes\nfrom sklearn.naive_bayes import GaussianNB #an algorithm that estimates the mean and the standard deviation from your training data,\nfrom sklearn.svm import SVC #an algorithm that creates a line or a hyperplane which separates the data into classes\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.neighbors import NearestCentroid\nfrom sklearn.neural_network import MLPClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#this suprreses unnecesary warnings from my output\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Loading My Dataset\n\n##### I am going to first use my training dataset, then the test dataset last."},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pandas.read_csv('/kaggle/input/ace-class-assignment/AMP_TrainSet.csv')#reading in my train dataset\ntrain\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test= pandas.read_csv('/kaggle/input/ace-class-assignment/Test.csv') #reading in my test dataset\ntest","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Inspecting my train dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape #a tuple that gives you an indication of the number of dimensions in the array.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.isnull().sum()  ###this will show the number of null values in my data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.count()  #returns number of non-null values in my data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### It seems i have no missing values in my data"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.describe()  #returns summary of the whole data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info() #It returns range, column, number of non-null objects of each column, datatype and memory usage","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### This shows that my dataset has 3038 rows (instances) and 12 columns (attributes)"},{"metadata":{},"cell_type":"markdown","source":"##### I will also take a look at how many instances i have for each class"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.groupby('CLASS').size()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.groupby('CLASS').size().plot(kind='bar') #i can also show this in a graph form\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### I have two groups of classes, each with 1519 instances"},{"metadata":{},"cell_type":"markdown","source":"# DATA VISUALISATION"},{"metadata":{},"cell_type":"markdown","source":"#### I will start with univariate plots to see each individual variable"},{"metadata":{"trusted":true},"cell_type":"code","source":"#visualizing using histograms\ntrain.hist(figsize=(16,16))\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### These histograms show that FullAcidicMolPerc is exponentially distributed while the rest are have a Gaussian distribution except for NT_EFC195 and CLASS. Histograms also help us identify outliers. From this output, I can say there are no outliers."},{"metadata":{"trusted":true},"cell_type":"code","source":"train.corr(method='pearson')['CLASS'] #Here I tried to see the correlation of all attributes with the class","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Multivariate "},{"metadata":{"trusted":true},"cell_type":"code","source":"#plotting a heatmap to show correlation of data\npyplot.figure(figsize=(10,10))\nsns.heatmap(train.corr(method='pearson'))\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### i did this plot in order to see which features are highly correlated as this can be a problem in soome models if some features are used together whereas they are highly correlated."},{"metadata":{"trusted":true},"cell_type":"code","source":"#scatter plot\n\nimport seaborn as sns\nsns.pairplot(train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### A scatter plot shows the relationship between two variables as dots in two dimensions. So from this output, i can be able to see the relationship between variables. If they show a good correlation, then they ccan be removed in feature selection."},{"metadata":{},"cell_type":"markdown","source":"# EVALUATING ALGORITHMS\n#### I will now evaluate some algorithms and estimate their accuracy on unseen data."},{"metadata":{},"cell_type":"markdown","source":"### Building models"},{"metadata":{},"cell_type":"markdown","source":"#### I will first split my train data into test and train, so that I test the effectiveness of my model using the test data."},{"metadata":{"trusted":true},"cell_type":"code","source":"array = train.values #first create a variable for extracting the values from the train dataset to be used\nX = array[:,0:11]  #selecting which columns to use, in this case all of them\nY = array[:,11] #selecting the label for our data, which is the last column\ntest_size = 0.32 #this is the size of my test data, meaning my train data is 68%\nseed = 3 #this is to initialize the random generator. So everytime i run this with a different seed number, i will get a different output\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size,\nrandom_state=seed)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## IMPORTING MODELS\n\n#### I will now use the models I imported in the beginning from sklearn to use for my algorithm."},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\nmodels = []\nmodels.append(('LR', LogisticRegression(solver='liblinear', multi_class='ovr')))\nmodels.append(('LDA', LinearDiscriminantAnalysis()))\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('CART', DecisionTreeClassifier()))\nmodels.append(('NB', GaussianNB()))\nmodels.append(('SVM', SVC(gamma='auto')))\nmodels.append(('RTC', RandomForestClassifier()))\nmodels.append(('SGD',SGDClassifier()))\nmodels.append(('NC', NearestCentroid()))\nmodels.append(('MLPC',MLPClassifier()))\n# evaluate each model in turn\nresults = []\nnames = []\nfor name, model in models:\n    kfold = StratifiedKFold(n_splits=10)\n    cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring='accuracy')\n    results.append(cv_results)\n    names.append(name)\n    print('%s: %f (%f)' % (name, cv_results.mean(), cv_results.std()))\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compare Algorithms\npyplot.boxplot(results, labels=names)\npyplot.title('Algorithm Comparison')\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### from here, RTC is the best performing, followed by NB."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make predictions on validation dataset, using my selected model from above(NB)\nmodel = GaussianNB()\nmodel.fit(X_train, Y_train)\npredictions = model.predict(X_test)\n\nfrom sklearn.metrics import matthews_corrcoef\nprint('MCC', matthews_corrcoef(model.predict(X_test), Y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make predictions on validation dataset, using my selected model from above(RTC)\nmodel = RandomForestClassifier()\nmodel.fit(X_train, Y_train)\npredictions = model.predict(X_test)\n\nfrom sklearn.metrics import matthews_corrcoef\nprint('MCC', matthews_corrcoef(model.predict(X_test), Y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### The MCC gives a score close to 100."},{"metadata":{},"cell_type":"markdown","source":"#### I will now test the performance of my model first using all the features, then with some selected ones"},{"metadata":{"trusted":true},"cell_type":"code","source":"#with all features\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import GaussianNB\n\narray = train.values\nX = array[:,0:11]\nY = array[:,11]\ntest_size = 0.32\nseed = 3\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size,\nrandom_state=seed)\nmodel = GaussianNB()\nmodel.fit(X_train, Y_train)\nresult = model.score(X_test, Y_test)\nprint(\"Accuracy: \",  (result*100.0))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Evaluate predictions\nprint(accuracy_score(Y_test, predictions))\nprint(confusion_matrix(Y_test, predictions))\nprint(classification_report(Y_test, predictions))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### This gives me a good score of 93.6%. I will now try selecting some features based on feature importance."},{"metadata":{},"cell_type":"markdown","source":"### This will be my first submission. It gave me a score of 99%"},{"metadata":{"trusted":true},"cell_type":"code","source":"Y=train.CLASS\nX=train.drop(\"CLASS\",axis=1)\nOUTPUT=model.fit(X, Y).predict(test.values)\nOUTPUT_1=pd.DataFrame(OUTPUT)\nOUTPUT_1.columns=[\"CLASS\"]\nOUTPUT_1.index.name=\"Index\"\nOUTPUT_1[\"CLASS\"]=OUTPUT_1[\"CLASS\"].map({0.0:False,1.0:True})  #changing 0 values to False, 1 to True\nOUTPUT_1.to_csv(\"output\") #converting my output file into a csv\nprint(OUTPUT_1[\"CLASS\"].unique()) #printing out the unique values, i expect to get 2\nprint(OUTPUT_1[\"CLASS\"].nunique()) #the sum of unique values\nprint(OUTPUT_1.groupby(\"CLASS\").size()[0].sum())\nprint(OUTPUT_1.groupby(\"CLASS\").size()[1].sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#feature selection using feature importance\nX = train.iloc[:,0:11]  #independent columns\ny = train.iloc[:,-1]    #target column\nfrom sklearn.ensemble import ExtraTreesClassifier\nimport matplotlib.pyplot as plt\nmodel = ExtraTreesClassifier()\nmodel.fit(X,y)\nprint(model.feature_importances_) #use inbuilt class feature_importances of tree based classifiers\n#plot graph of feature importances for better visualization\nfeat_importances = pd.Series(model.feature_importances_, index=X.columns)\nfeat_importances.nlargest(10).plot(kind='barh')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### I will select the features with the highest bars. The number of features selected will depend on the accuracy."},{"metadata":{"trusted":true},"cell_type":"code","source":"train.columns\nnewtrain=train.drop([  'FULL_AURR980107', 'FULL_OOBM850104', 'NT_EFC195', 'AS_DAYM780201', 'AS_FUKS010112', 'CT_RACS820104'], axis=1)\nnewtest=test.drop([  'FULL_AURR980107', 'FULL_OOBM850104', 'NT_EFC195', 'AS_DAYM780201', 'AS_FUKS010112', 'CT_RACS820104'], axis=1)\narray = newtrain.values\nX = array[:,0:5]\nY = array[:,-1]\ntest_size = 0.32\nseed = 3\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size,\nrandom_state=seed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_new=newtrain.CLASS\nX_new=newtrain.drop(\"CLASS\",axis=1)\nOUTPUT=model.fit(X_new, Y_new).predict(newtest.values)\nOUTPUT_new=pd.DataFrame(OUTPUT)\nOUTPUT_new.columns=[\"CLASS\"]\nOUTPUT_new.index.name=\"Index\"\nOUTPUT_new[\"CLASS\"]=OUTPUT_new[\"CLASS\"].map({0.0:False,1.0:True})\nOUTPUT_new.to_csv(\"out1\")\nprint(OUTPUT_new[\"CLASS\"].unique())\nprint(OUTPUT_new[\"CLASS\"].nunique())\nprint(OUTPUT_new.groupby(\"CLASS\").size()[0].sum())\nprint(OUTPUT_new.groupby(\"CLASS\").size()[1].sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### This gave me a score of 85%"},{"metadata":{"trusted":true},"cell_type":"code","source":"#this is how i checked the accuracy from the features selected\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import GaussianNB\n\narray = train.values\nX = array[:,[0,2,3,5,7]]\nY = array[:,11]\ntest_size = 0.32\nseed = 3\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size,\nrandom_state=seed)\nmodel = GaussianNB()\nmodel.fit(X_train, Y_train)\nresult = model.score(X_test, Y_test)\nprint(\"Accuracy: \",  (result*100.0))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#i will now select 6 features to see if my score improves\n\narray = train.values\nX = array[:,[0,1,2,3,5,7]]\nY = array[:,11]\ntest_size = 0.32\nseed = 3\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size,\nrandom_state=seed)\nmodel = GaussianNB()\nmodel.fit(X_train, Y_train)\nresult = model.score(X_test, Y_test)\nprint(\"Accuracy: \",  (result*100.0))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### the accuracy deacreases. let me try 4 features instead"},{"metadata":{"trusted":true},"cell_type":"code","source":"array = train.values\nX = array[:,[1,2,3,7]]\nY = array[:,11]\ntest_size = 0.32\nseed = 3\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size,\nrandom_state=seed)\nmodel = GaussianNB()\nmodel.fit(X_train, Y_train)\nresult = model.score(X_test, Y_test)\nprint(\"Accuracy: \",  (result*100.0))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### accuracy deacreases even further. so if i weere to feature select, I would use 5 features because they give me the best accuracy."},{"metadata":{},"cell_type":"markdown","source":"## Let me try the RTC model"},{"metadata":{"trusted":true},"cell_type":"code","source":"#with all features\nfrom sklearn.model_selection import train_test_split\n\n\narray = train.values\nX = array[:,0:11]\nY = array[:,11]\ntest_size = 0.32\nseed = 3\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size,\nrandom_state=seed)\nmodel = RandomForestClassifier()\nmodel.fit(X_train, Y_train)\nresult = model.score(X_test, Y_test)\nprint(\"Accuracy: \",  (result*100.0))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### this actually gives me a better score than the one from GaussianNB. Let me submit it"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Evaluate predictions\nprint(accuracy_score(Y_test, predictions))\nprint(confusion_matrix(Y_test, predictions))\nprint(classification_report(Y_test, predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_new2=train.CLASS\nX_new2=train.drop(\"CLASS\",axis=1)\nOUTPUT2=model.fit(X_new2, Y_new2).predict(test.values)\nOUTPUT_new1=pd.DataFrame(OUTPUT2)\nOUTPUT_new1.columns=[\"CLASS\"]\nOUTPUT_new1.index.name=\"Index\"\nOUTPUT_new1[\"CLASS\"]=OUTPUT_new1[\"CLASS\"].map({0.0:False,1.0:True})  #changing 0 values to False, 1 to True\nOUTPUT_new1.to_csv(\"outputRTC\") #converting my output file into a csv\nprint(OUTPUT_new1[\"CLASS\"].unique()) #printing out the unique values, i expect to get 2\nprint(OUTPUT_new1[\"CLASS\"].nunique()) #the sum of unique values\nprint(OUTPUT_new1.groupby(\"CLASS\").size()[0].sum())\nprint(OUTPUT_new1.groupby(\"CLASS\").size()[1].sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### This gave me a score of 83%. let me feature select the 5 "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n\narray = train.values\nX = array[:,[0,2,3,5,7]]\nY = array[:,11]\ntest_size = 0.32\nseed = 3\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size,\nrandom_state=seed)\nmodel = RandomForestClassifier()\nmodel.fit(X_train, Y_train)\nresult = model.score(X_test, Y_test)\nprint(\"Accuracy: \",  (result*100.0))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### this gives me a lower score than when all features are selected. So i will stop here."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}