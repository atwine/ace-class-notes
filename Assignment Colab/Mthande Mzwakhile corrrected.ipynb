{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Mthande Sibonakaliso\n# Reg #:2019/HD07/30569X\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> > # **Step** **1**\n****So after running this cell am expecting to see the versions of the imported libraries and if installed , they will run without any error.\nNumpy- gives us the N-dimensional array object, broadcasting functions , linear algebra and random number capabilities\nMatplot- Plots and histograms , bar charts , scatterplots( for visualizatio of our dataset)\nPandas- powerful data structure since we are dealing with dataframes,handling of data."},{"metadata":{"trusted":true},"cell_type":"code","source":"import scipy as scipy\nimport seaborn as sb\nimport  matplotlib as mat\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings \nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **STEP 2**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#load the libraries we will use during the algorithn\nfrom pandas import read_csv# to be able to read our csv files into our space\nfrom pandas.plotting import scatter_matrix# is used in the visualization(multivarites)\nfrom matplotlib import pyplot# to show our different plots after we have plotted them , eg histograms\nfrom sklearn.model_selection import train_test_split# thsi is for spliting our data when we want to get the best model , into train and validate\nfrom sklearn.model_selection import cross_val_score#is used to access the predictive performance of the models and to judge how they perform to the test data.When we fit a model we fit it into training dataset\nfrom sklearn.model_selection import StratifiedKFold# its a cross validation, selection of folds so that the mean response value is approx equal in all folds\nfrom sklearn.metrics import classification_report# so here we get to get the precision,F1 and the scores for that model, True negatives and False negatives are used to create the classification report\nfrom sklearn.metrics import confusion_matrix#A confusion matrix is a table that is often used to describe the performance of a classification model (or “classifier”) on a set of test data for which the true values are known. It allows the visualization of the performance of an algorithm.\nfrom sklearn.metrics import accuracy_score#accuracy is the fraction of predictions our model got right.\nfrom sklearn.feature_selection import SelectKBest#for feature selection \nfrom sklearn.feature_selection import chi2# for feature selection\nfrom sklearn.feature_selection import f_classif# for feature selection\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So what i have tried to do is to import every function i think i will need with all the classification models that i will use , i will later explain the significance and why am using that model when i have runned them****"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import matthews_corrcoef as mcc\n#from sklearn.metrics import matthews_corrcoef","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The Matthews Correlation Coefficient (MCC) has a range of -1 to 1 where -1 indicates a completely wrong binary classifier while 1 indicates a completely correct binary classifier. Using the MCC allows one to gauge how well their classification model/function is performing.So a good one will have a number close to 1"},{"metadata":{"trusted":true},"cell_type":"code","source":"TRAIN=pd.read_csv('../input/ace-class-assignment/AMP_TrainSet.csv')#importing the data into our workspace\nprint(TRAIN)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **STEP** **3**"},{"metadata":{},"cell_type":"markdown","source":"********\n**** # So after here my train data should be here and i get to know it and visualize as follows.In the variable TRAIN"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Getting to know my data\nTRAIN.columns# getting to see how the person was thinking who was inputting the data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TRAIN.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Getting to know many rows and columns does our dataset has"},{"metadata":{"trusted":true},"cell_type":"code","source":"TRAIN.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"getting to know some statistics , mean , average,median ,max and min value for each attribute in our data"},{"metadata":{"trusted":true},"cell_type":"code","source":"TRAIN.head(20)#get the first 20 records for our dataset , just to know","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(TRAIN.groupby('CLASS').size())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Grouping by CLASS , i want to see how many counts for each of the classes.Getting to know this is very important in machine learning for our algorithm not to be biased.So here i can tell that my counts are the same , which makes it ideal. If my counts were different , i would have to apply smote in order to make them the same , its either i increase the class having less counts or i decrease the class having many counts. But in this case , i dont have to do any of those."},{"metadata":{},"cell_type":"markdown","source":"> > # STEP 4"},{"metadata":{},"cell_type":"markdown","source":"# Data visualization"},{"metadata":{},"cell_type":"markdown","source":"\n\nSo we what we do is to view our data into two dimensions\n****Univariate plots to better understand each attribute.What am hoping to see here is more like the statistics, the mean , upper and lower quatile for each attribute in our dataset. This will also help us see how the data is distributed for that attribute, is it normal , is it skewed\nMultivariate plots to better understand the relationships between attributes.It will help us see which ones are correlated and which one are not , so for our model we will like to have the correlated ones."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Univariate Plots- one attribute(we shal use the box and whisker)\nTRAIN.plot(kind='box',subplots=True, layout=(4,3), sharex=False, sharey=False , figsize=(16,16))\npyplot.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"****So in this code am expecting to see the statistics of each attribute independent. This will give us the mean and which is the middle line and the upper and lower quatile,It is often used in explanatory data analysis. This type of graph is used to show the shape of the distribution, its central value, and its variability."},{"metadata":{"trusted":true},"cell_type":"code","source":"#we shall use also the histogram\nTRAIN.hist(figsize=(16,16))\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**So this is another way of showing the distribution, we can see that AS_DAYM780201,AS_FUKS010112 and many attributes are normal distributed **\nThis is to tell us which of the attributes should be used for the algorithm."},{"metadata":{"trusted":true},"cell_type":"code","source":"TRAIN.plot(kind='density',subplots=True, layout=(4,3), sharex=False, sharey=False,figsize=(16,16))\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":">*We can see clearly the distribution of the attributes , those that are normal distributed and those that are not normally distributed*So i think this will help us to see which features will affect the algorithm , so for the machine to work well , and to learn well , the data needs to be normal distributed , hence we should do something or get rid of the features that are not normal distributed."},{"metadata":{"trusted":true},"cell_type":"code","source":"sb.distplot(TRAIN, bins=10, kde=False, rug=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is a density plot for the features, they show how the data is distributed , it has combined the hist function from matplotlib , i think it also showing us the distribution of our data"},{"metadata":{"trusted":true},"cell_type":"code","source":"sb.kdeplot(TRAIN)\nsb.kdeplot(TRAIN, bw=.2, label=\"bw: 0.2\")\nsb.kdeplot(TRAIN, bw=2, label=\"bw: 2\")\npyplot.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The kernel density estimate may be less familiar, but it can be a useful tool for plotting the shape of a distribution. Like the histogram, the KDE plots encode the density of observations on one axis with height along the other axis:So here we can tell that most of the distribution is close to one and zero where thats where our class is clustered hence why we see much distribution there.\nContour plot for bivariate KDE: By estimating probability densitity we see two clusters although not separated distinctly, at modes 0 and 1 which represents the classes we have."},{"metadata":{"trusted":true},"cell_type":"code","source":"pyplot.hist(TRAIN['CLASS'], bins = 20)\npyplot.xlabel('CLASS')\npyplot.ylabel('Count')\npyplot.title('Class Distribution')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I have done that when i wanted to check the class , so this is a graphical presentation for thet since graphs are better than words."},{"metadata":{"trusted":true},"cell_type":"code","source":"columns = TRAIN.columns.drop(['CLASS'])\n# create x data\nx_data = range(0, TRAIN.shape[0])\n# create figure and axis\nfig, ax = pyplot.subplots()\n# plot each column\nfor column in columns:\n    ax.plot(x_data, TRAIN[column], label=column)\n# set title and legend\nax.set_title('TRAIN Dataset')\nax.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So here i was trying to see where most of the values from each attribute lie , so as to others we can see that they have the negative values hence we can see that they a skewed negatively .Am not sure if its the right interpretation for this but i think."},{"metadata":{"trusted":true},"cell_type":"code","source":"#multivariate plots- as mentioned ealier we will see the relationship amnong the attributes, so here we will consider\n#scatter plot matrix\nscatter_matrix(TRAIN, figsize=(30,30))\npyplot.show() #the diagonal distribution(comparing itseld against itself) is some which suggest a high correlation and a predictable realtion. as this one goes up , also this one goes up or down.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So here this tells me about the correlation of one attribute against the other, so somr do have correlation and others dont hence this telles me that if they are correlated , i dont have to have both attriutes i have to get rid ofb one , but i have a questioon here, WHY ARE WE HAVING HISTOGRAMS IN THE DIAGONALS and HOW DO WE EXPLAIN THIS LINES FOR EXAMPLE HERE , THE BOTTOM ROW ?, but i will do a PCA and also see that too."},{"metadata":{"trusted":true},"cell_type":"code","source":"sb.pairplot(TRAIN)\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So this is the pair plot for all the variables in our dataset. Scatter plots are useful for knowing structured relationships between variables, but its not yet clear , because i have used all the attributes"},{"metadata":{"trusted":true},"cell_type":"code","source":"pyplot.figure(figsize=(10,10))\nsb.heatmap(TRAIN.corr(method='pearson'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So above i have found the correlation of the attributes , you know why they  are important ? Because when i find two features that are correlated , i have to get rid of one feauture in order to increase the accuarcy and the speed of my algorithm. But i did the IRIS dataset and as i saw other attributes being dropped , both of them , i dont know didnt we take one instead of dropping all???So some are positivey correlated , while others are negative correlated, that says we have to work on them using square roots or logs on them to have them at normal."},{"metadata":{"trusted":true},"cell_type":"code","source":"TRAIN.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nsb.violinplot(x = 'AS_MeanAmphiMoment', y = 'FULL_AURR980107', data=TRAIN)\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Violin Plots are a combination of the box plot with the kernel density estimates. So, these plots are easier to analyze and understand the distribution of the data.\nSo here i was trying to plot a violin for the selected attributes but we can not see clearly beacuse we have so many values to incorporate but if we had so little values , it would have much clear.So we could have seen all the stats we see in a box plot."},{"metadata":{"trusted":true},"cell_type":"code","source":"TRAIN.corr(method=\"pearson\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Skew of Univariate Distribution\nTRAIN.skew().plot(kind='bar')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Working with skewed data is not good for an algorithm so here we find out how is the data skewed. So after that when i have seen how much skewness , i will know which attributes to apply the skewness solving , maybe cube root and square root for those that are positively skewd. \nSo my data  , i can see some that are negatively skewed , but provided the features are important for my algorithm , if they are not , i will drop them** So hence i will have to solve them before i continue."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Evaluate some algorithms-- create some models of the data and estimate thier accuracy on unseen data\n#Steps\n#1.Separate validation dataset\n#2.Set up test harness to use 10.fold cross validation\n#3.Build multiple models to predict species from flower measurements\n#4.Select the best model\n\n\n#Create a validation Dataset- to see if the model we created is good, and later try statistical methods to estimate the accuracy of the models we created on unseen data.\n#and also estimate accuracy of the best model on unseen data by evaluating it on unseen data, hence we shall hold some of the data that the alsorithm will not see , so we will use this data for an independent idea of how accurate our model is\n#so split it into 80% which we will tra our models with then 20% which we shall test with(validation dataset)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> > # **STEP** **5**\n"},{"metadata":{},"cell_type":"markdown","source":"# Splitting the TRAIN data into Train and validate , so here we will train our moe on the train set and then validate it on the Validate set to see if is it that accurate or not."},{"metadata":{"trusted":true},"cell_type":"code","source":"array= TRAIN.values# this is are the values of the dataset that will be used in the training\nX= array[:,0:11]# so here we want all our columns to be included in the training and validation\ny=array[:,11] # our training will be based on the last column which is in this case is the \"variety\" column (our class(as represented in WEKA ) or its our label)\nX_train, X_validation, Y_train, Y_validation= train_test_split(X, y, test_size=0.2, random_state=2)# so here we now now spliting using the function (train_test_split),\n#test size is the size of our validation which is 20% and 80% of the training, then the random _state to mantain the same values through out, if you remove it , your algorithm will be confused because its tested on dfferent","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So what i have  done here is to split my validate and train data as needed for the train data so now i will have to run on the validate set and as seen here this are the lenghts of the data  that will be used as explained in the comments.\nThe train data will be used to train the algorithm on how the the algorithm should memorize the data. So then the validation set , will be used to see how well the model has trained on the set , called the validation set hence will be used."},{"metadata":{"trusted":true},"cell_type":"code","source":"#xtrain\nprint(X_train)# this the 80% of train\nprint(len(X_train))\n#ytarin\nprint(Y_train)#80% for the validation\nprint(len(Y_train))\n#y vali\nprint(Y_validation)#20% for the validation(labels)\nprint(len(Y_validation))\n#xvali\nprint(X_validation)#20% for the validation\nprint(len(X_validation))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As seen from the output, these are the lengths and the values for both the test and validate sets."},{"metadata":{},"cell_type":"markdown","source":"# Step 6\nImporting the models to be used on the algorithm from sklearn"},{"metadata":{"trusted":true},"cell_type":"code","source":"#These are the classifications models to be used, they are explained briefly below\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import NearestCentroid\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.ensemble import GradientBoostingRegressor\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. KNeighbourClassifiers\nit takes a bunch of labelled points and uses them to learn how to label other points. To label a new point, it looks at the labelled points closest to that new point (those are its nearest neighbors), and has those neighbors vote, so whichever label the most of the neighbors have is the label for the new point (the “k” is the number of neighbors it checks).\n2. Logistic Regression\nIt is a statistical method for analysing a data set in which there are one or more independent variables that determine an outcome. The outcome is measured with a dichotomous variable (in which there are only two possible outcomes). The goal of logistic regression is to find the best fitting model to describe the relationship between the dichotomous characteristic of interest (dependent variable = response or outcome variable) and a set of independent (predictor or explanatory) variables\n3. Decision Tree Classifier\nIt breaks down a data set into smaller and smaller subsets while at the same time an associated decision tree is incrementally developed. The final result is a tree with decision nodes and leaf nodes\n4. Linear Discriminant Analysis\nworks by reducing the dimensionality of the dataset, projecting all of the data points onto a line. Then it combines these points into classes based on their distance from a chosen point or centroid.\n5. Support Vector Machines\nwork by drawing a line between the different clusters of data points to group them into classes. Points on one side of the line will be one class and points on the other side belong to another class.\n6. A Naive Bayes Classifier \ndetermines the probability that an example belongs to some class, calculating the probability that an event will occur given that some input event has occurred\n7. RandomForest\nThe random forest is a classification algorithm consisting of many decisions trees. It uses bagging and feature randomness when building each individual tree to try to create an uncorrelated forest of trees whose prediction by committee is more accurate than that of any individual tree\n\n8. SGDClassifier\nStochastic Gradient Descent (sgd) is a solver. It is a simple and efficient approach for discriminative learning of linear classifiers under convex loss functions such as (linear) Support Vector Machines and Logistic Regression\n9. MLPClassifier\nA multilayer perceptron (MLP) is a feedforward artificial neural network model that maps sets of input data onto a set of appropriate outputs\n10. Nearest Centroid Classifier\nis a classification model that assigns to observations the label of the class of training samples whose mean (centroid) is closest to the observation"},{"metadata":{},"cell_type":"markdown","source":"# Step 7\nTraining each model on our set in order to see which one is suiatble for our case"},{"metadata":{"trusted":true},"cell_type":"code","source":"#testing all the models at once and taking tthe one with a higher mean and the lowest standard deviation\nmodels = []\nmodels.append(('LR', LogisticRegression(solver='liblinear', multi_class='ovr')))\nmodels.append(('LDA', LinearDiscriminantAnalysis()))\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('CART', DecisionTreeClassifier()))\nmodels.append(('NB', GaussianNB()))\nmodels.append(('SVM', SVC(gamma='auto')))\nmodels.append(('RTC', RandomForestClassifier()))\nmodels.append(('SGD',SGDClassifier()))\nmodels.append(('NC', NearestCentroid()))\nmodels.append(('MLPC',MLPClassifier()))\n#models.append(('GB',GradientBoostingRegressor(n_estimators=20)))\n# evaluate each model in turn\nresults = []\nnames = []\nfor name, model in models:\n    kfold = StratifiedKFold(n_splits=10, random_state=42, shuffle=True)\n    cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring='accuracy')\n    results.append(cv_results)\n    names.append(name)\n    print('%s: %f (%f)' % (name, cv_results.mean()*100, cv_results.std()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So what i have done here, i have used the k fold validation so meaning i have split my dataset into 10 folds , this is to have 9 folds and then test on one , so this will iterate for every fold until all the 10 has got the chance.\nThe random state is to make sure that i have the same points even when someone runs it , they will get the same values.(But i used the iris dataset , and when i changed the random state , one number would give me everything to be 1.00 especially the accuracy , so am not sure why ?)\nSo every model will iterate on all this and give the cross validation score for every model and hence i will see which model is working well for that dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"#comparing the algorithms\npyplot.boxplot(results, labels=names)\npyplot.title('Algorithm Comparison')\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is to make sure that i compare the algorithms in a plot so as to see the upper quatile and upper and also the mean for each algorithm. Its just a presentation of the results in a plot , so we are expecting the model that gave the accuracy in terms of mean hence it should have a higher mean presentation even in the box plot with the title given"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#make predictions on the validated dataset\nmodel= GaussianNB()# the model i had chose is the Naive Bayes\ny=model.fit(X_train, Y_train)#fitting the Gaussian model in the X and Y train \npredictions= model.predict(X_validation)# then make presictions on the X validation\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"What i have done here is to use the model that had the highest mean when i was running the algorithms , and used it here to check how is it performing on the validated set that i had split from the TRAIN dataset.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import matthews_corrcoef as mcc\nmodel.fit(X_validation,Y_validation)\nmcc=mcc(model.predict(X_validation),Y_validation)\nprint(\"MCC:\",mcc)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Computing for the MCC so as explained from the cell where i ran the cell ,if  the value is close to 1 , good classifer , closer -1 , bad classifier. "},{"metadata":{"trusted":true},"cell_type":"code","source":"#evaluating predictions - comparion them to the expected results in the validation set, then calculate classificationn accuracy , as well as a confusion matrix and a classification report\n#Evaluate predictions\nprint(accuracy_score(Y_validation, predictions))\nprint(confusion_matrix(Y_validation, predictions))\nprint(classification_report(Y_validation, predictions))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since i imported the accuracy score , i will the accuarcy of the model that is mentioned, based on the Y validation and the predictions variable created in the above cell.\nThe confusion matrix will tell us about the Type1 and Type 2 errors , false positives ,false negatives,etc. This will tell us how mant points it has been able to classifier them , with respect to the two errors, the major diagonal for the Type2 errors , when yiu add them , they should more than the other diagonal."},{"metadata":{"trusted":true},"cell_type":"code","source":"Test=pd.read_csv(\"../input/ace-class-assignment/Test.csv\")\nTest.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Test.shape\nTRAIN.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The following step will be used to produce the csv file that will be used in the prediction submission , so here i have dropped the CLASS , because its what the predictions should be about in the score prediction and also the giving the two columns the index and the CLASS"},{"metadata":{"trusted":true},"cell_type":"code","source":"Y=TRAIN.CLASS\nX=TRAIN.drop(\"CLASS\",axis=1)\nOUTPUT=model.fit(X, Y).predict(Test.values)\nOUTPUT_1=pd.DataFrame(OUTPUT)\nOUTPUT_1.columns=[\"CLASS\"]\nOUTPUT_1.index.name=\"Index\"\nOUTPUT_1[\"CLASS\"]=OUTPUT_1[\"CLASS\"].map({0.0:False,1.0:True})\nOUTPUT_1.to_csv(\"NB\")\nprint(OUTPUT_1[\"CLASS\"].unique())\nprint(OUTPUT_1[\"CLASS\"].nunique())\nprint(OUTPUT_1.groupby(\"CLASS\").size()[0].sum())\nprint(OUTPUT_1.groupby(\"CLASS\").size()[1].sum())\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The Gaussian gave me a score of 99% and 100% , the Random Tree Classifier gave me 87% and the KNN model gave 82% hence the GuassianNB comes on top , when all the features are used for my model."},{"metadata":{},"cell_type":"markdown","source":"# THE END FOR WHEN I USED ALL THE FEATURES"},{"metadata":{"trusted":true},"cell_type":"code","source":"###########################################So here am trying to use the selected features#############################################","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#FEATURE SELCTION\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LogisticRegression\n\narray= TRAIN.values\nX = array[:,0:11]\nY = array[:,11]\n# feature extraction\nmodel = LogisticRegression()\nrfe = RFE(model, 7)\nfit = rfe.fit(X, Y)\nprint(\"Num Features: \",  fit.n_features_)\nprint(\"Selected Features:\",  fit.support_)\nprint(\"Feature Ranking: \",  fit.ranking_)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"As you can see from the output , we have  TRUES that means those are the features that should be used for the algorithm,  so hence below we have dropped them.So below is the syntax for dropping of features. The reason while am dropping this attributes is for mr to see which ones are important for the speed and my accuracy, so beacuse i ahd selected 4 features which i cannot explain why i choose four , but i think you can choose any features that you would want to use for your algorithm , am i correct ?"},{"metadata":{"trusted":true},"cell_type":"code","source":"TRAIN.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"New7=TRAIN.drop(['FULL_AcidicMolPerc','FULL_DAYM780201','AS_DAYM780201', 'AS_FUKS010112', 'CT_RACS820104'],axis=1)\nNew7# The selected 7 new features according to thier importance","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"New7.shape# getting to know if have the columns been dropped","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"New7.plot(kind='density',subplots=True, layout=(3,3), sharex=False, sharey=False,figsize=(16,16))\npyplot.show()# using the density plot to view thier distribution","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Checking the distribution of the remaining characters , so i can tell from here that most of the attributes are normal distributed, so my algorithm wont deviate that much.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"sb.jointplot(x = 'AS_MeanAmphiMoment',y = 'FULL_Charge',data = New7,kind = 'hex')\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Hexagonal binning is used in bivariate data analysis when the data is sparse in density i.e., when the data is very scattered and difficult to analyze through scatterplots.\nSo this is just a plot comparing how the distribution of two variables are, so here we are looking for the relationship for two attributes and we can see that the related.How one feature behaves in the presence of that other."},{"metadata":{"trusted":true},"cell_type":"code","source":"New7.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#So for the DATA VISUALIZATION of the selected attribute ,which are given as TRUES from the output above\npyplot.figure(figsize=(10,10))\nsb.heatmap(New7.corr(method='pearson'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the correaltion plot as am still trying to decrease my attributes and to see which ones are correlated and which ones are not , so i can even continue drop them if its okay."},{"metadata":{"trusted":true},"cell_type":"code","source":"sb.pairplot(New7)\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So now in the scatter platter i can tell the distribution too , but i dont know why am getting histograms for like when i plot an attributes agaisnt itself, shouldent i get maybe a line straight to show that its actually itself , so every poitn on the other side is found even on the other one? Also i can see that some attributes are not correlated with the other. "},{"metadata":{"trusted":true},"cell_type":"code","source":"AS=sb.PairGrid(New7, hue=\"CLASS\")\nsb.PairGrid(New7, hue=\"CLASS\", vars=New7[[\"CLASS\",\"AS_MeanAmphiMoment\"]])\nAS.map(pyplot.scatter)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So what i have plotted here is the most important feature which is AS_MeanAmphiMoment against the other features to see how they are correlated or related to that feature."},{"metadata":{"trusted":true},"cell_type":"code","source":"New7.drop(['CLASS'], axis=1).plot.line(title='New Dataset')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr = New7.corr()\nsb.heatmap(New7.corr(), annot=True,)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So this is another type of visualization , i want to see how is the attribute that is considered to be more important because every time i select the features, its always there so now i want to see how is it related with the othere attributes in the dataset]"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Data visualization- in order to find the correlation and the realtionship between the attributes , we shall do even the univariate and also the multivariate plots\n#Univariate as mentioned above , we have saw how they are correlated in the multivariate plots so now we go try and see what we can get if we get a lower accuracy , we come back and drop again unt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"array= New7.values# this is are the values of the dataset that will be used in the training\nX= array[:,0:6]# so here we want all our columns to be included in the training and validation\ny=array[:,6] # our training will be based on the last column which is in this case is the \"variety\" column (our class(as represented in WEKA ) or its our label)\nX_train, X_validation, Y_train, Y_validation= train_test_split(X, y, test_size=0.2, random_state=2)# so here we now now spliting using the function (train_test_split),\n#test size is the size of our validation which is 20% and 80% of the training, then the random _state to mantain the same values through out, if you remove it , your algorithm will be confused because its tested on dfferent","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is as seen before from when we have used all the features , spliting the data set , but now using the the new variable with the selected features.Am expecting to see reduced lenghth of this ones because i have reduced mydata set."},{"metadata":{"trusted":true},"cell_type":"code","source":"#xtrain\nprint(X_train)# this the 80% of train\nprint(len(X_train))\n#ytarin\nprint(Y_train)#80% for the validation\nprint(len(Y_train))\n#y vali\nprint(Y_validation)#20% for the validation(labels)\nprint(len(Y_validation))\n#xvali\nprint(X_validation)#20% for the validation\nprint(len(X_validation))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Checking the length and the values themselves. Yes they have reduced as compared to when i was having the whole dataset from the beginning."},{"metadata":{"trusted":true},"cell_type":"code","source":"#testing all the models at once and taking tthe one with a higher mean and the lowest standard deviation\nmodels = []\nmodels.append(('LR', LogisticRegression(solver='liblinear', multi_class='ovr')))\nmodels.append(('LDA', LinearDiscriminantAnalysis()))\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('CART', DecisionTreeClassifier()))\nmodels.append(('NB', GaussianNB()))\nmodels.append(('SVM', SVC(gamma='auto')))\nmodels.append(('RTC', RandomForestClassifier()))\nmodels.append(('SGD',SGDClassifier()))\nmodels.append(('NC', NearestCentroid()))\nmodels.append(('MLPC',MLPClassifier()))\n# evaluate each model in turn\nresults = []\nnames = []\nfor name, model in models:\n    kfold = StratifiedKFold(n_splits=35, random_state=42, shuffle=True)\n    cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring='accuracy')\n    results.append(cv_results)\n    names.append(name)\n    print('%s: %f (%f)' % (name, cv_results.mean()*100, cv_results.std()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#make predictions on the validated dataset\nmodel=GaussianNB()\ny=model.fit(X_train, Y_train)\npredictions= model.predict(X_validation)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import matthews_corrcoef as mcc\nmodel.fit(X_validation,Y_validation)\nmcc=mcc(model.predict(X_validation),Y_validation)\nprint(\"MCC:\",mcc)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Even here the MCC is 0.799 which is also close to one for the Gaussian"},{"metadata":{},"cell_type":"markdown","source":"Since the GaussianNB model gave me the higest percentage , i will use it as my model in this dataset and also try maybe 2 or 3 other models"},{"metadata":{"trusted":true},"cell_type":"code","source":"#evaluating predictions - comparion them to the expected results in the validation set, then calculate classificationn accuracy , as well as a confusion matrix and a classification report\n#Evaluate predictions\nprint(accuracy_score(Y_validation, predictions))\nprint(confusion_matrix(Y_validation, predictions))\nprint(classification_report(Y_validation,predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Test=pd.read_csv(\"../input/ace-class-assignment/Test.csv\")\nTest.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nTest7=Test.drop(['FULL_AcidicMolPerc','FULL_DAYM780201','AS_DAYM780201', 'AS_FUKS010112', 'CT_RACS820104'],axis=1)\nTest7\nY=New7.CLASS\nX=New7.drop(\"CLASS\",axis=1)\nOUTPUT=model.fit(X, Y).predict(Test7.values)\nOUTPUT_1=pd.DataFrame(OUTPUT)\nOUTPUT_1.columns=[\"CLASS\"]\nOUTPUT_1.index.name=\"Index\"\nOUTPUT_1[\"CLASS\"]=OUTPUT_1[\"CLASS\"].map({0.0:False,1.0:True})\nOUTPUT_1.to_csv(\"SVC7\")\nprint(OUTPUT_1[\"CLASS\"].unique())\nprint(OUTPUT_1[\"CLASS\"].nunique())\nprint(OUTPUT_1.groupby(\"CLASS\").size()[0].sum())\nprint(OUTPUT_1.groupby(\"CLASS\").size()[1].sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So in my scoring , i got 88% for the Naive Bayes model , and 82 % using Random Tree Classifier and 76% using the Support vector Machine , so hence the Gaussian NB model is also appearing to be suitable even when we selected the features , hebce its suitable for this type of problem , but let me do feature imporatnce and see ."},{"metadata":{},"cell_type":"markdown","source":"# The end after RFE "},{"metadata":{},"cell_type":"markdown","source":"# USING FEATURE IMPORTANCE TO SEE TOO THEN WE CAN CONCLUDE AND SAY ITS GAUSSIANNB  "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import ExtraTreesClassifier\narray = TRAIN.values\nX = array[:,0:11]\nY = array[:,-1]\n# feature extraction\nmodel = ExtraTreesClassifier()\nmodel.fit(X, Y)\nprint(model.feature_importances_)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TRAIN.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Impo=TRAIN.drop([ 'FULL_GEOR030101', 'NT_EFC195', 'AS_DAYM780201', 'AS_FUKS010112', 'CT_RACS820104'],axis=1)\n   \nImpo# checking our new data with the selected features.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Viewing our data\nImpo.plot(kind='density',subplots=True, layout=(3,3), sharex=False, sharey=False,figsize=(16,16))\npyplot.show()# using the density plot to view thier distribution","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is are our new attributes that are used , so i can tell most are normal distributed , but am noticing something about attribute number 8 , whenever i do selection , its always there, cant we use if only (if possible) because its clear thats the output or the end result is very much dependent on it .(just wondering though)"},{"metadata":{"trusted":true},"cell_type":"code","source":"sb.pairplot(Impo)\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Multivariate plot","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"array= Impo.values# this is are the values of the dataset that will be used in the training\nX= array[:,0:6]# so here we want all our columns to be included in the training and validation\ny=array[:,6] # our training will be based on the last column which is in this case is the \"variety\" column (our class(as represented in WEKA ) or its our label)\nX_train, X_validation, Y_train, Y_validation= train_test_split(X, y, test_size=0.2, random_state=2)# so here we now now spliting using the function (train_test_split),\n#test size is the size of our validation which is 20% and 80% of the training, then the random _state to mantain the same values through out, if you remove it , your algorithm will be confused because its tested on dfferent","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#testing all the models at once and taking tthe one with a higher mean and the lowest standard deviation\nmodels = []\nmodels.append(('LR', LogisticRegression(solver='liblinear', multi_class='ovr')))\nmodels.append(('LDA', LinearDiscriminantAnalysis()))\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('CART', DecisionTreeClassifier()))\nmodels.append(('NB', GaussianNB()))\nmodels.append(('SVM', SVC(gamma='auto')))\nmodels.append(('RTC', RandomForestClassifier()))\nmodels.append(('SGD',SGDClassifier()))\nmodels.append(('NC', NearestCentroid()))\nmodels.append(('MLPC',MLPClassifier()))\n# evaluate each model in turn\nresults = []\nnames = []\nfor name, model in models:\n    kfold = StratifiedKFold(n_splits=10, random_state=42, shuffle=True)\n    cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring='accuracy')\n    results.append(cv_results)\n    names.append(name)\n    print('%s: %f (%f)' % (name, cv_results.mean()*100, cv_results.std()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#make predictions on the validated dataset\nmodel=GaussianNB()\ny=model.fit(X_train, Y_train)\npredictions= model.predict(X_validation)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#evaluating predictions - comparion them to the expected results in the validation set, then calculate classificationn accuracy , as well as a confusion matrix and a classification report\n#Evaluate predictions\nprint(accuracy_score(Y_validation, predictions))\nprint(confusion_matrix(Y_validation, predictions))\nprint(classification_report(Y_validation,predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import matthews_corrcoef as mcc\nmodel.fit(X_validation,Y_validation)\nmcc=mcc(model.predict(X_validation),Y_validation)\nprint(\"MCC:\",mcc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Test=pd.read_csv(\"../input/ace-class-assignment/Test.csv\")\nTest.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TestImpo=Test.drop([ 'FULL_GEOR030101', 'NT_EFC195', 'AS_DAYM780201', 'AS_FUKS010112', 'CT_RACS820104'],axis=1)\nTestImpo# dropping the same features as we did to the train dataset so we keep consistency","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#TestImpo=Test.drop(['FULL_AcidicMolPerc','FULL_DAYM780201','AS_DAYM780201', 'AS_FUKS010112', 'CT_RACS820104'],axis=1)\n#ImpoTest\nY=Impo.CLASS\nX=Impo.drop(\"CLASS\",axis=1)\nOUTPUT=model.fit(X, Y).predict(TestImpo.values)\nOUTPUT_1=pd.DataFrame(OUTPUT)\nOUTPUT_1.columns=[\"CLASS\"]\nOUTPUT_1.index.name=\"Index\"\nOUTPUT_1[\"CLASS\"]=OUTPUT_1[\"CLASS\"].map({0.0:False,1.0:True})\nOUTPUT_1.to_csv(\"imp.RC\")\nprint(OUTPUT_1[\"CLASS\"].unique())\nprint(OUTPUT_1[\"CLASS\"].nunique())\nprint(OUTPUT_1.groupby(\"CLASS\").size()[0].sum())\nprint(OUTPUT_1.groupby(\"CLASS\").size()[1].sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So here i tested with MPLClassifier which gave me 79% , GaussianNB gave me 83% and RandomTreeeClassier gave me 80 % so hence i can say the GaussianNB has come out on top in all the ways i tried so i think its the right model for this type of problem , but am yet to do the Normalizing , Standarzing , Binarizing problem is , i havent got all the info how to , am still lacking on that part hence i ddint Normalize or Standadize my data."},{"metadata":{},"cell_type":"markdown","source":"# I think GaussianNB is the right model "},{"metadata":{},"cell_type":"markdown","source":"# Using the Principal Component Analysis\n"},{"metadata":{},"cell_type":"markdown","source":"Each and every time we try to do machine learning, its best to work with fewer attributes like tw dim other than the high dimensional data.So since in our data we have 11 attributes it can be better to jave at least two principal components. So what we will do is to scale and fit our data , which is the TRAIN in this case."},{"metadata":{"trusted":true},"cell_type":"code","source":"Data = TRAIN.drop(columns='CLASS')# drop the class since it is our dependent variable/target\n\n#Scale the Data\nfrom sklearn.preprocessing import StandardScaler#importing the scaler\nSC = StandardScaler()\nSC.fit(Data)#fitting our data into the Standard Scaler\n\nMyScale = SC.transform(Data)# trabsforming it since we have scaled","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is more like the scalling then transforming the data , because every time you scale , make sure you then transform your scaled data."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Components specification\nfrom sklearn.decomposition import PCA# importing our PCA\nCA = PCA(n_components=2)#specifying our components , since we have two classes , which is 0 and 1 , so we sjhall have two components.\nCA.fit(MyScale)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So we have specified the componets, so as to reduce our attributes to only two attributes."},{"metadata":{"trusted":true},"cell_type":"code","source":"TPCA = CA.transform(MyScale)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking if our datasets have been reduced\n#print(TPCA.shape())#after the PCA\n#print(MyScale.shape())# before principal compoent analysis","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pyplot.figure(figsize=(8,6))\npyplot.scatter(TPCA[:,0],TPCA[:,1],c=TRAIN['CLASS'],cmap='rainbow')\npyplot.xlabel('FIRST COMPONENT')\npyplot.ylabel('SECOND COMPONENT')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So now that my data has been rescaled , now i want to see thier plots to see how they are distributed.What i can say is , this components are dependent to other variables in my original dataset so hence i now have to see which ones are closley related and which are not , so as to if they are related i keep them and if they are not i get rid of them for my algorithm. So now i will use a heat map to view the realtionship using the correlation(feature selection) using the correlation coeffecient."},{"metadata":{"trusted":true},"cell_type":"code","source":"TRAIN.corr()['CLASS'].sort_values()# looking for the correlations with the other attributes ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So i will select the attributes that affect the CLASS looking at how much they correlate.Then use the drop function.SO what i will use are the top 5 features  regardless of thier correlation , wheater negative our positive since they both affect the dependent variable.So i will take the 1,2,3,9,10 characters(According the ouput above)\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"TRAIN.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MyPCA=TRAIN.drop(['CT_RACS820104','NT_EFC195', 'AS_DAYM780201','FULL_GEOR030101','AS_DAYM780201','FULL_OOBM850104'], axis=1)\nMyPCA\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nsb.set_style(\"ticks\")\nsb.pairplot(MyPCA,hue = 'CLASS',diag_kind = \"kde\",kind = \"scatter\",palette = \"husl\")\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can observe the variations in each plot. The plots are in matrix format where the row name represents x axis and column name represents the y axis.Its just another way of visualization on how there attributes is related to the other, so they are just scattered.Its a pairwise relationship between the one at"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(MyPCA.shape)#shape of the new dataset\nprint(MyPCA.head())#first 5 lines of new dataset","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is the shape of my new dataset and indeed there selected attributes were dropped "},{"metadata":{"trusted":true},"cell_type":"code","source":"Test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"PCA_test=Test.drop(['CT_RACS820104','NT_EFC195', 'AS_DAYM780201','FULL_GEOR030101','AS_DAYM780201','FULL_OOBM850104'], axis=1)\nPCA_test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Dropping the selected attributes we dropped from the TRAIN set to make everything uniform and consistency."},{"metadata":{"trusted":true},"cell_type":"code","source":"pyplot.figure(figsize=(10,10))\nsb.heatmap(MyPCA.corr(method='pearson'))#having a small peep on how the features are correlated with other featuresz","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"array= MyPCA.values\nX= array[:,0:6]\ny=array[:,6] \nX_train, X_validation, Y_train, Y_validation= train_test_split(X, y, test_size=0.2, random_state=2)# splitting t0 80% train and 20% test\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"models = []\nmodels.append(('LR', LogisticRegression(solver='liblinear', multi_class='ovr')))\nmodels.append(('LDA', LinearDiscriminantAnalysis()))\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('CART', DecisionTreeClassifier()))\nmodels.append(('NB', GaussianNB()))\nmodels.append(('SVM', SVC(gamma='auto')))\nmodels.append(('RTC', RandomForestClassifier()))\nmodels.append(('SGD',SGDClassifier()))\nmodels.append(('NC', NearestCentroid()))\nmodels.append(('MLPC',MLPClassifier()))\n# evaluate each model in turn\nresults = []\nnames = []\nfor name, model in models:\n    kfold = StratifiedKFold(n_splits=10, random_state=42, shuffle=True)\n    cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring='accuracy')\n    results.append(cv_results)\n    names.append(name)\n    print('%s: %f (%f)' % (name, cv_results.mean()*100, cv_results.std()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This the accuarcy i get using the PCA data and now i will use the MLPC since it came top on the validated set"},{"metadata":{"trusted":true},"cell_type":"code","source":"model=GaussianNB()\ny=model.fit(X_train, Y_train)\npredictions= model.predict(X_validation)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(accuracy_score(Y_validation, predictions))\nprint(confusion_matrix(Y_validation, predictions))\nprint(classification_report(Y_validation,predictions))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Giving an accuracy of 89 and the confusion matrix ,the major diagonal having more vlaues than the minor diagonal. "},{"metadata":{"trusted":true},"cell_type":"code","source":"Y=MyPCA.CLASS\nX=MyPCA.drop(\"CLASS\",axis=1)\nOUTPUT=model.fit(X, Y).predict(PCA_test.values)\nOUTPUT_1=pd.DataFrame(OUTPUT)\nOUTPUT_1.columns=[\"CLASS\"]\nOUTPUT_1.index.name=\"Index\"\nOUTPUT_1[\"CLASS\"]=OUTPUT_1[\"CLASS\"].map({0.0:False,1.0:True})\nOUTPUT_1.to_csv(\"NBP\")\nprint(OUTPUT_1[\"CLASS\"].unique())\nprint(OUTPUT_1[\"CLASS\"].nunique())\nprint(OUTPUT_1.groupby(\"CLASS\").size()[0].sum())\nprint(OUTPUT_1.groupby(\"CLASS\").size()[1].sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The MPLC gave me 78% and the GaussianNB gave me 83% in the train dataset."},{"metadata":{},"cell_type":"markdown","source":"# Boosting my model"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfrom sklearn.ensemble import GradientBoostingClassifier\narray = MyPCA.values\n\nX = array[:,0:6]\ny = array[:,6]\n\nkfold = StratifiedKFold(n_splits=10, random_state=42)\nmodel = GradientBoostingClassifier(n_estimators=50, random_state=42)\nresults = cross_val_score(model, X, y, cv=kfold)\nprint(results.mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So using the GradientBoosting Classifier am getting a mean of 85% which shows that my model has been boosted by the model."},{"metadata":{"trusted":true},"cell_type":"code","source":"NB=GaussianNB()\nLDA=LinearDiscriminantAnalysis()\nMLPC=MLPClassifier()\n\nfrom sklearn.ensemble import  VotingClassifier as VC\nVoting=VC(estimators=[('NB',NB),('LDA',LDA),('MPLC',MLPC)],voting='hard')\nPREDICT=Voting.fit(X_train,Y_train)\npredictions= Voting.predict(X_validation)\nprint(accuracy_score(Y_validation, predictions))\nprint(confusion_matrix(Y_validation, predictions))\nprint(classification_report(Y_validation,predictions))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is the output of the combined nodels, so we get an accuracy of 89% when using the VotingClassifier  to evaluate our models. To see how the models perform better when combined together with the best performing models."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import matthews_corrcoef as mcc\nmodel.fit(X_validation,Y_validation)\nmcc=mcc(model.predict(X_validation),Y_validation)\nprint(\"MCC:\",mcc)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The MCC is 0.93 which is not that bad , but because we have combined them together, so because the models had different MCC , it has now tried to find the average of those."},{"metadata":{},"cell_type":"markdown","source":"# References\nhttps://machinelearningmastery.com/faq/single-faq/what-algorithm-config-should-i-use\nhttps://machinelearningmastery.com/start-here/#nlp\nhttps://machinelearningmastery.com/start-here/#process\nhttps://machinelearningmastery.com/faq/single-faq/what-value-should-i-set-for-the-random-number-seed\nhttps://machinelearningmastery.com/start-here/#statistical_methods\nhttps://machinelearningmastery.com/confusion-matrix-machine-learning/\nhttp://machinelearningmastery.com/an-introduction-to-feature-selection/\nhttps://machinelearningmastery.com/compare-machine-learning-algorithms-python-scikit-learn/\nhttps://machinelearningmastery.com/stochastic-gradient-boosting-xgboost-scikit-learn-python/\nhttps://www.kaggle.com/startupsci/titanic-data-science-solutions"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}