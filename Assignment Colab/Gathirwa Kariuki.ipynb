{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        #import the necessary libraries you are going to use\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Importing the datasets `AMP_TrainSt.csv` and `Test.csv` and separating them into testing and training data."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"Test = pd.read_csv (\"../input/ace-class-assignment/Test.csv\")#Importing the test data\nTrain = pd.read_csv (\"../input/ace-class-assignment/AMP_TrainSet.csv\")# Importing the TrainSet\n                     ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Checking the dimensions of the datasets"},{"metadata":{"trusted":true},"cell_type":"code","source":"Test.shape, Train.shape #Getting the shape of the datasets","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Viewing a section of the data:"},{"metadata":{"trusted":true},"cell_type":"code","source":"Test.head(10) #Displaying a section of the Test data ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Train.head(10) # Displaying a section of the Training set","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Checking the data types of each of the attributes of the `Train` and `Test` data sets."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking the data types of the variables\nTrain.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Test.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Descriptive Statistics\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"Test.describe() #Used to provide descriptive statistics of the Test data set","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Train.describe() #Used to provide descriptive statistics of the Test data set","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Checking for the ** 'na' ** values from both the Test and Train datasets"},{"metadata":{"trusted":true},"cell_type":"code","source":"Test.isna() ##Checking for na values in the Test Set","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Train.isna() #Checking for na values in the Training Set","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Classification Distribution\n#### Classification distribution is done to know how balanced class values are."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plotting a bar graph to check the distribution of the class\nTrain.groupby('CLASS').size().plot(kind='bar') \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **Correlation between Attributes**\nThis is used to test for the correlation between two variables and whether the change of one influences the change of the other. The Pearson's Correlation coefficient is commonly used to achieve this. It assumes a normal distribution of attributes involved.\nA correlation of 1 shows very strong positive correlation while a correlation of -1 shows a very strong negative correlation."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Testing for correlation between attributes\nTrain.corr(method='pearson')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### A heat map is used to well depict the correlation between attributes.\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting a heat map to show the correkation between the data\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(6,6))\nsns.heatmap(Train.corr(method='pearson'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### To check the correlation of the Attributes in regards to the class."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Performing Pearson correlation\nTrain.corr(method='pearson')['CLASS']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **Determining the Skew of Univariate Distributions**\n#### Normal skew assumes a Gaussian distribution, which is a normal bell curve that is shifted or squashed in one direction or another.\n#### The skew() function can be used to calculate the skew of each attribute from the Pandas library.\n#### If the skewness lies above 1 or below -1, then it is correct to infer that the data is highly skewed.If it lies between +0.5 to -0.5 then it is moderately skewed. The data is said to be symmetric if the skewness value is zero.\n#### Positively skewed data can be transformed using the `log`, `cube root` and `square root` functions."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plotting a bar chart that shows the skewness of the data\nTrain.skew().plot(kind='bar')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Visualization \n#### The `seaborn` library is used to generate plots to help us better understand the data."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting Histograms to represent the distribution of various features\nplt.figure(figsize=(17,17))\nTrain.hist()\nplt.subplots_adjust(bottom=1,right=2, top=3)# fit the plots and adjust them for visibility\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Box and Whisker Plots\n#### Boxplots summarize the distribution of each attribute drawing a line for the median (mid value) and a box that spans from the first quartile (25th percentile) to the third quartile (75th percentile)."},{"metadata":{"trusted":true},"cell_type":"code","source":"Train.plot(kind='box', subplots=True, layout=(5,5), sharex=False, sharey=False)\nplt.subplots_adjust(bottom=1,right=2, top=3)# fit the plots and adjust them for visibility\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Density Plots\n\n#### The Density plots are another alternative way of getting a quick idea of the distribution of each attribute. They are more or less a histogram with a smooth curve drawn through the top of each bin.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"Train.plot(kind='density', subplots=True, layout=(4,4), sharex=False)\nplt.show","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Multivariate Plots\n#### Multivariate plots provide examples of two plots showing multiple interactions between multiple variables. Such plots include:\n* Scatter plot matrix\n* correlation matrix plot"},{"metadata":{},"cell_type":"markdown","source":"### Scatter Plot\n#### A scatter plot shows the relationship between two variables as dots in two dimensions, one axis for each attribute."},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#importing pairplot from seaborn for the Train data\nsns.pairplot(Train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"correlations = Train.corr()\n# plot correlation matrix\nfig = plt.figure()\nax = fig.add_subplot(111)\ncax = ax.matshow(correlations, vmin=-1, vmax=1)\nfig.colorbar(cax)\nticks = np.arange(0,9,1)\n#setting the x and y axis\nax.set_xticks(ticks)\nax.set_yticks(ticks)\nax.set_xticklabels(Train.columns)\nax.set_yticklabels(Train.columns)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"g = sns.PairGrid(Train, hue=\"CLASS\")\nsns.PairGrid(Train,hue='CLASS', vars=Train[['CLASS','NT_EFC195']]) #Hue must be a categorical variable\ng.map(plt.scatter)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data Preparation for Machine Learning\n#### Since most machine learning algorithms make a lot of assumptions about the data we provide them with, it is very essential to prepare the data in such a way that best exposes the structure of the problem you need to solve using these algorithms. Different types of data however require different transforms during pre-processing.\n#### The normally followed procedure involves:\n* Splitting the dataset into input.output (I/O)variables for machine learning.\n* Applying a pre-processing transform to the input variables.\n* Data summarization to reveal the change.\n\n#### The fit and multiple transform method from sci-kit learn is normally preferred during data pre-processing.\n#### The `fit()` function is used to prepare the parameters of the data. The `transform()` function on the same dataset is used to prepare it for modeling and later on the test data or the validation dataset for any new data generated in the future. This is useful in representing the data using plots."},{"metadata":{},"cell_type":"markdown","source":"### Rescaling/Normalizing Data\n#### Depending on the skewness of the data, data with many varying scales, machine learning algorithms benefit from rescaling this data so that all attributes can have the same scale. Attreibutes are normally scales in the range between 0 and 1.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from numpy import set_printoptions\nfrom sklearn.preprocessing import MinMaxScaler\n\narray = Train.values\n# separate array into input and output components\nX = array[:,0:10]\nY = array[:,10]\nscaler = MinMaxScaler(feature_range=(0, 1))\nrescaledX = scaler.fit_transform(X)\n# summarize transformed data\nset_printoptions(precision=3)\nprint(rescaledX[0:5,:])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data Standaridization \n  #### Standardization of data is useful in the transformation of attributes of data with a Gaussian (normal) distribution with a standard deviation of 1 and a mean of 0. The StandardScaler library of sci-kit learn is used to perform this function. "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\narray2 = Train.values\n#  array is separated into input and output components\nX = array2[:,0:10]\nY = array2[:,10]\nscaler = StandardScaler().fit(X)\nrescaledX = scaler.transform(X)\n# summarizing the transformed data\nset_printoptions(precision=3)\nprint(rescaledX[0:5,:])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data Normalization\n#### Not to be confused with rescaling where data is often made to fit between values of 0 and 1. This type of normalization involves to rescaling each row/observation to have a length of 1. Often useful for sparse datasets with attributes of varying scales using algorithms such as k-Nearest neighbors and neural networks.The Normalizer class4 pakage is employed for this task."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import Normalizer\n\narray_3 = Train.values\n# separate array into input and output components\nX = array_3[:,0:10]\nY = array_3[:,10]\nscaler = Normalizer().fit(X)\nnormalizedX = scaler.transform(X)\n# summarize transformed data\nset_printoptions(precision=3)\nprint(normalizedX[0:5,:])\nprint(type(normalizedX))#print the data type so we can know what we are \n#working with in the dataset.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Binarizing Data\n#### This is a useful technique especially when data has different probabilities. The data is normally transformed by creating a binary threshold where all values above this threshold are marked as `1` while those below are marked as `0`. The package used for this is Binarizer from scikit-learn. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Data binarization\nfrom sklearn.preprocessing import Binarizer\n\narray_4 = Train.values\n# separate array into input and output components\nX = array_4[:,0:11]\nY = array_4[:,11]\nbinarizer = Binarizer(threshold=0.0).fit(X)\nbinaryX = binarizer.transform(X)\n# summarize transformed data\nset_printoptions(precision=3)\nprint(binaryX[0:5,:])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feature Selection\n#### Feature selection is a technique employed to choose the features that highly influence and contribute the overall predictive performance of the model while dropping those features that negatively affect model performance. \n* #### Feature selection reduces overfitting and ensures that redundant and unuseful data (noise) is eliminated from the model.\n* #### This goes a long way in improving the accuracy/ predictive capacity of the model.\n* #### Appropraite feature selection also ensures that the algorithms train faster. "},{"metadata":{},"cell_type":"markdown","source":"#### Statistical tests are used to select the features that have the strongest relationship to the class/output variable. The SlectKBest class2 library from sci-kit learn can be used with several statistical tests to select a specific number of wanted or unwanted features.\n#### The most commonly employed feature selection methods are `Wrapper feature selection methods` and `Filter feature selective methods`. Wrapper methods are known to evaluate multiple models with different subsets of input features and select those features that result in the best performing model according to a performance metric. These techniques are normally computationally expensive. Such a method is Recursive Feature Elimination(RFE). Filter methods evaluate the relevance of the predictors outside of the predictive models using statistical techniques such as the chi square test mentioned above to evaluate the relationship between input and target variable."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import SelectKBest, f_classif\narray_5=Train.values\nX = array_5[:,0:11]\nY = array_5[:,11]\nsel_f = SelectKBest (f_classif, k=6)\narray_4= sel_f.fit_transform(X, Y)\nprint(sel_f.get_support())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Printing out the selected columns from the ftest from the previous step and saving them in a new variable.\nxtrain = X[:, sel_f.get_support()]\nxtrain\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Recursive Feature Elimination\n#### This is a wrapper method that works by recursively removing attributes and building a model on the remaining attributes.\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#importing the RFE library from sklearn \nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LogisticRegression\n\n#creating an array for training values\narray_6 = Train.values\nX = array_6[:,0:11]\nY = array_6[:,11]\n\n# feature extraction\nmodel = LogisticRegression()\nrfe = RFE(model, 7)\n\n# fitting and predicting the model\nfit = rfe.fit(X, Y)\nprint(\"Num Features: \",  fit.n_features_)\nprint(\"Selected Features:\",  fit.support_)\nprint(\"Feature Ranking: \",  fit.ranking_)\nmodel.fit(Train.values[:,:11][:,fit.support_],Y) \nmodel.predict(Test.values[:,fit.support_])\notrfe = model.predict(Test.values[:,:11][:,fit.support_])\notrfe\nprint(np.count_nonzero(otrfe==1))\nprint(np.count_nonzero(otrfe==0))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Evaluating the Machine Learning Algorithm\n#### We should always split the training data from the test data to prevent `overfitti1ng`. An overfit algorithm works perfectly as it can remember the entire dataset rather than work predictively.\n#### The evaluation is an estimate that we can use to determine how well we think the algorithm would perform in practice.\n#### Once this is done, we can then re-train the final algorithm on the entire training set and get it ready for operational use. Some of the splitting methods used for machine learning algorithms are:\n* k-fold cross validation\n* Leave one out cross validation\n* Repeated Random Test-Train Splits\n"},{"metadata":{},"cell_type":"markdown","source":"#### In this case the K-fold cross validation was used. To split the data to several k-parts(7 folds). Each fold is given a chance to be a test set.The algorithm is trained and tested multiple times which i`ncreases the accuracy. The choice of k must be large enough to allow the size of each test partition to be a reasonable sample of the problem, while allowing enough repetitions of the train-test evaluation of the algorithm to provide a fair estimate of the algorithms performance on unseen data. "},{"metadata":{"trusted":true},"cell_type":"code","source":"#Kfold cross validation\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\n\nnum_folds = 97 #number of folds to use\nseed = 7 #reproducibility\n\nkfold = KFold(n_splits=num_folds, random_state=seed)\nmodel = LogisticRegression()\nresults = cross_val_score(model, xtrain, Y, cv=kfold)\nprint(f\"Accuracy:\", (results.mean()*100.0, results.std()*100.0))\nmodel.fit(Train.values[:,:11][:,fit.support_],Y) \nmodel.predict(Test.values[:,fit.support_])\notk = model.predict(Test.values[:,:11][:,fit.support_])\notk\nprint(np.count_nonzero(otk==1))\nprint(np.count_nonzero(otk==0))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Returning the output in a dataframe"},{"metadata":{"trusted":true},"cell_type":"code","source":"repo_0 = pd.DataFrame(otk)\nrepo_0.columns=['CLASS'] # Creating a class column\nrepo_0.index.name= 'index' #Creating a culumn index\nrepo_0['CLASS']= repo_0['CLASS'].map({0.0:False, 1.0:True}) # Map function to change the 0.0 and 1.0 into False and True repectively\nrepo_0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"repo_0.to_csv(\"kcsv\") #convert the dataframe into csv","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Applying more classifiers\n\n#### SVM\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Importing SVC library from sklearn\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import cross_val_score\n#Splitting the training data into 20 folds with a random state of 7\nkfold = KFold(n_splits=20, random_state=7)\nmodel = SVC()\nresults = cross_val_score(model, Train.values[:,0:11][:,fit.support_], Y, cv=kfold)\nprint(results.mean())\n\n#Fitting and predicting the model using slected feaetures from the Training data\nmodel.fit(Train.values[:,:11][:,fit.support_], Y)\nmodel.predict(Test.values[:,:11][:,fit.support_])\notsvm = model.predict(Test.values[:,:11][:,fit.support_])\notsvm\nprint(np.count_nonzero(otsvm==1))\nprint(np.count_nonzero(otsvm==0))\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Returning the SVM output in a dataframe"},{"metadata":{"trusted":true},"cell_type":"code","source":"repo_1 = pd.DataFrame(otsvm)\nrepo_1.columns=['CLASS'] # Creating a class column\nrepo_1.index.name= 'index' #Creating a culumn index\nrepo_1['CLASS']= repo_1['CLASS'].map({0.0:False, 1.0:True}) # Map function to change the 0.0 and 1.0 into False and True repectively\nrepo_1\n\n#returning the svm output in a csv file\nrepo_1.to_csv(\"nbmcsv1\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Naive Bayes\n#### This is a probabilistic classifier, that assumes Gaussian distribution and calculates the probability of each class and the conditional probability of the class given an input value/feature. Assuming these probabilities are all independent, they are estimated for new data and multiplied together.\n#### The model gave better results when using the entire dataset (instead of a few features from the Training dataset. The computational time was not greatly affected."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB #import naive bayes library from sklearn\n\n#Split the data into 40 folds with a random state of 7\nkfold = KFold(n_splits=40, random_state=7)\nmodel = GaussianNB()\nresults = cross_val_score(model, Train.values[:,0:11], Y, cv=kfold)\nprint(results.mean())\n\n#fitting the model\nmodel.fit(Train.values[:,:11],Y) \n#predicting using the Test data\notp = model.predict(Test.values[:,:11])\notp\n\nprint(np.count_nonzero(otp==1))\nprint(np.count_nonzero(otp==0))\n\n#Returning the Naive Bayes output in a dataframe\nrepo_2 = pd.DataFrame(otp)\nrepo_2.columns=['CLASS'] # Creating a class column\nrepo_2.index.name= 'index' #Creating a culumn index\nrepo_2['CLASS']= repo_2['CLASS'].map({0.0:False, 1.0:True}) # Map function to change the 0.0 and 1.0 into False and True repectively\nrepo_2\n\n#Storing the dataframe output in a csv file.\nrepo_2.to_csv(\"nbmcsv1\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}