{"cells":[{"metadata":{},"cell_type":"markdown","source":" # KAKOOZA TREVOR\n## Kaggle assignment\n### started on the 28th /feb/2020"},{"metadata":{},"cell_type":"markdown","source":"# 1. Prepare Problem\n### a) Load libraries\n### b) Load dataset"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Import the necessary tools required to work on the datasets"},{"metadata":{"trusted":true},"cell_type":"code","source":"#import the necessary libraries you are going to use\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# -----> Put your code here below:\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n# This first session is about loading the give dataset and viewing them.  \n\n* The two data set provided are\n1. test.csv\n2. AMP_TrainSet.csv \n\n### Now load the data set using.\n```\nTrain = pd.read_csv(\"../input/amp-data-set/AMP_TrainSet.csv\")\nTest = pd.read_csv(\"../input/amp-data-set/Test.csv\")\n```\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#using pandas we call the data set\nTest = pd.read_csv('/kaggle/input/amp-data-set/Test.csv')\nTrain = pd.read_csv(\"../input/amp-data-set/AMP_TrainSet.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Summarize Data\n\n## View the first five raws of the datasets"},{"metadata":{"trusted":true},"cell_type":"code","source":"#head.() is ued to view the first five raws of the data set.\nTest.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Shape function\n* This function helps to view the total raws and columns (dimension) of the dataset. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# check the dimensions of your data\n\nTrain.shape, Test.shape\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data type\n* This returns a Series with the data type of each column.\n* The result‚Äôs index is the original DataFrame‚Äôs columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"Test.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Train.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## isnull().sum\n* This function helps to see if the dataset has any missing values."},{"metadata":{"trusted":true},"cell_type":"code","source":"Train.isnull().sum","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Test.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* When we look at the data ,it shows that both the test and train dataset does not have any missing values"},{"metadata":{},"cell_type":"markdown","source":"## Then in column of class we check and find out,\n* How is the data divied into.\n* Here the unique function does that."},{"metadata":{"trusted":true},"cell_type":"code","source":"Train['CLASS'].unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# a) Descriptive statistics\n## Descriptive statistics of the data\n* Generate descriptive statistics that summarize the central tendency,\ndispersion and shape of a dataset's distribution, excluding\n``NaN`` values.\n\nAnalyzes both numeric and object series, as well\nas ``DataFrame`` column sets of mixed data types. The output\nwill vary depending on what is provided.\n\n### For example this function will provide the following summary\n* Count.\n* Mean.\n* Standard Deviation.\n* Minimum Value.\n* 25th Percentile.\n* 75th Percentile(Median).\n* Maximum Value.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"Train.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# (c)Check the dataset info"},{"metadata":{"trusted":true},"cell_type":"code","source":"Train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Test.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# b) Data visualizations\n## Class Distribution\n\nA groupby operation involves some combination of splitting the\nobject, applying a function, and combining the results. This can be\nused to group large amounts of data and compute operations on these\ngroups."},{"metadata":{"trusted":true},"cell_type":"code","source":"Train.groupby('CLASS').size().plot(kind='bar')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Correlations Between Attributes\nIs the a mutual relationship or connection between two or more things.\n\nNote: Correlation refers to the relationship between two variables and how they may or may notchange together. The most common method for calculating correlation is Pearson's Correlation Coefficient, that assumes a normal distribution of the attributes involved.\n\nA correlation of -1 or 1 shows a full negative or positive correlation respectively. Whereas a value of 0 shows no correlation at all. "},{"metadata":{"trusted":true},"cell_type":"code","source":"Train.corr(method='pearson')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Plotting to show the correlation\n\nplot a heat map to show us the correlation of the data.\n\nWith the help of ; seaborn\nthe we can use spearman or pearson method.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#The first step is to choose the figure size then use the heatmap to plot.\nplt.figure(figsize=(7,7))\nsns.heatmap(Train.corr(method='pearson'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## we can also check the correlation in regards to the CLASS"},{"metadata":{"trusted":true},"cell_type":"code","source":"Train.corr(method='pearson')['CLASS']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Skew of Univariate Distributions\n### Definition of skew.\n* Is the suddenly change direction or position.\n* or Skew refers to a distribution that is assumed Gaussian (normal or bell curve) that is shifted or squashed in one direction or another. \n\n\nYou can calculate the skew of each attribute using the skew() function on the Pandas DataFrame.\n\n#### NOTE: If skewness value lies above +1 or below -1, data is highly skewed. If it lies between +0.5 to -0.5, it is moderately skewed. If the value is 0, then the data is symmetric\n\n### Positively skewed data:\nIf tail is on the right as that of the second image in the figure, it is right skewed data. It is also called positive skewed data. Common transformations of this data include square root, cube root, and log.\n\n### Cube root transformation:\nThe cube root transformation involves converting x to  ùë•(1/3) . This is a fairly strong transformation with a substantial effect on distribution shape: but is weaker than the logarithm. It can be applied to negative and zero values too. Negatively skewed data.\n\n### Square root transformation:\nApplied to positive values only. Hence, observe the values of column before applying.\n\n### Logarithm transformation:\nThe logarithm, x to log base 10 of x, or x to log base e of x (ln x), or x to log base 2 of x, is a strong transformation and can be used to reduce right skewness.\n\n### Negatively skewed data:\nIf the tail is to the left of data, then it is called left skewed data. It is also called negatively skewed data. Common transformations include square , cube root and logarithmic. We will discuss what square transformation is as others are already discussed."},{"metadata":{"trusted":true},"cell_type":"code","source":"Train.skew().plot(kind='bar')\n#When we look to the data its highly positively skewed","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data visualization\n## Understand Your Data With Visualization\nYou must understand your data in order to get the best results from machine learning algorithms. The fastest way to learn more about your data is to use data visualization.\n\n### Univariate Plots\nIn this section we will look at three techniques that you can use to understand each attribute of your dataset independently.\n\nHistograms.\nDensity Plots.\nBox and Whisker Plots.\n\n\n# Histograms"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16,16))\nTrain.hist()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Density Plots\nDensity plots are another way of getting a quick idea of the distribution of each attribute. The plots look like an abstracted histogram with a smooth curve drawn through the top of each bin, much like your eye tried to do with the histograms.\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"Train.plot(kind='density', subplots=True, layout=(3,4), sharex=False)\nplt.show","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Box and Whisker Plots\nAnother useful way to review the distribution of each attribute is to use Box and Whisker Plots or boxplots for short."},{"metadata":{"trusted":true},"cell_type":"code","source":"#boxplots help to represent outlies\nTrain.plot(kind='box', subplots=True, layout=(3,4), sharex=False, sharey=False)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Multivariate Plots\nThis section provides examples of two plots that show the interactions between multiple variables in your dataset.\n\n* Correlation Matrix Plot.\n* Scatter Plot Matrix.\n* Correlation Matrix Plot\n\nCorrelation gives an indication of how related the changes are between two variables. If two variables change in the same direction they are positively correlated. If they change in opposite directions together (one goes up, one goes down), then they are negatively correlated. You can calculate the correlation between each pair of attributes. This is called a correlation matrix. You can then plot the correlation matrix and get an idea of which variables have a high correlation with each other. This is useful to know, because some machine learning algorithms like linear and logistic regression can have poor performance if there are highly correlated input variables in your data."},{"metadata":{"trusted":true},"cell_type":"code","source":"Train.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"correlations = Train.corr()\n# plot correlation matrix\nfig = plt.figure()\nax = fig.add_subplot(111)\ncax = ax.matshow(correlations, vmin=-1, vmax=1)\nfig.colorbar(cax)\nticks = np.arange(0,9,1)\nax.set_xticks(ticks)\nax.set_yticks(ticks)\nax.set_xticklabels(Train.columns)\nax.set_yticklabels(Train.columns)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Scatter Plot Matrix\n\nA scatter plot shows the relationship between two variables as dots in two dimensions, one axis for each attribute. You can create a scatter plot for each pair of attributes in your data. Drawing all these scatter plots together is called a scatter plot matrix. Scatter plots are useful for spotting structured relationships between variables, like whether you could summarize the relationship between two variables with a line. Attributes with structured relationships may also be correlated and good candidates for removal from your dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"#you can use vars to compare two variable and hue to put colours\nsns.pairplot(Train,hue='CLASS',vars=['FULL_Charge','FULL_AcidicMolPerc'])\n#check on the seaborn(seaborn.pydata.org-dev/generated/seaborn.boxplot.html)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"```mo = pd.DataFrame(out_model)\nmo.to_csv(\"xyz.csv\")\n```"},{"metadata":{},"cell_type":"markdown","source":"# 3. Prepare Data\n\n# Prepare Your Data For Machine Learning\nMany machine learning algorithms make assumptions about your data. It is often a very good idea to prepare your data in such way to best expose the structure of the problem to the machine learning algorithms that you intend to use. In this chapter you will discover how to prepare your data for machine learning in Python using scikit-learn. After completing this lesson you will know how to:\n\n1. Rescale data.\n2. Standardize data.\n3. Normalize data.\n4. Binarize data.\n### Need For Data Pre-processing\nYou almost always need to pre-process your data. It is a required step. A difficulty is that different algorithms make different assumptions about your data and may require different transforms. Further, when you follow all of the rules and prepare your data, sometimes algorithms can deliver better results without pre-processing. Generally, I would recommend creating many different views and transforms of your data, then exercise a handful of algorithms on each view of your dataset. This will help you to ush out which data transforms might be better at exposing the structure of your problem in general.\n\n### The steps involved are as below:\n\nSplit the dataset into the input and output variables for machine learning.\nApply a pre-processing transform to the input variables.\nSummarize the data to show the change.\nThe scikit-learn library provides two standard idioms for transforming data. Each are useful in di\u000berent circumstances. The transforms are calculated in such a way that they can be applied to your training data and any samples of data you may have in the future. The scikit-learn documentation has some information on how to use various di\u000berent pre-processing methods:\n\nThe Fit and Multiple Transform method is the preferred approach. You call the fit() function to prepare the parameters of the transform once on your data. Then later you can use the transform() function on the same data to prepare it for modeling and again on the test or validation dataset or new data that you may see in the future. The Combined Fit-And-Transform is a convenience that you can use for one o\u000b tasks. This might be useful if you are interested in plotting or summarizing the transformed data.\n\n### Rescale Data\nWhen your data is comprised of attributes with varying scales, many machine learning algorithms can bene\ft from rescaling the attributes to all have the same scale. Often this is referred to as normalization and attributes are often rescaled into the range between 0 and 1. This is useful for optimization algorithms used in the core of machine learning algorithms like gradient descent. It is also useful for algorithms that weight inputs like regression and neural networks and algorithms that use distance measures like k-Nearest Neighbors. You can rescale your data using scikit-learn using the MinMaxScaler class\n\n### NOTE: Since my graph and summary data shows varying means and Gaussian distribution. i would use Standardize data method\n\n\n\n### Standardize of Data\nStandardization is a useful technique to transform attributes with a Gaussian distribution and differing means and standard deviations to a standard Gaussian distribution with a mean of 0 and a standard deviation of 1. It is most suitable for techniques that assume a Gaussian distribution in the input variables and work better with rescaled data, such as linear regression, logistic regression and linear discriminate analysis. You can standardize data using scikit-learn with the StandardScaler class3."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\narray2 = Train.values\n# separate array into input and output components\nX = array2[:,0:11]\nY = array2[:,11]\nscaler = StandardScaler().fit(X)\nrescaledX = scaler.transform(X)\n# summarize transformed data\n#set_printoptions(precision=3)\nprint(rescaledX[0:5,:])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# a) Data Cleaning(not done)\n# b) Feature Selection\n\n## Feature Selection For Machine\nLearning The data features that you use to train your machine learning models have a huge in uence on the performance you can achieve. Irrelevant or partially relevant features can negatively impact model performance. In this chapter you will discover automatic feature selection techniques that you can use to prepare your machine learning data in Python with scikit-learn. After completing this lesson you will know how to use:\n\n## Univariate Selection.\nRecursive Feature Elimination.\nPrinciple Component Analysis.\nFeature Importance.\nFeature Selection\nFeature selection is a process where you automatically select those features in your data that contribute most to the prediction variable or output in which you are interested. Having irrelevant features in your data can decrease the accuracy of many models, especially linear algorithms like linear and logistic regression. Three benets of performing feature selection before modeling your data are:\n\nReduces Overffitting: Less redundant data means less opportunity to make decisions based on noise.\nImproves Accuracy: Less misleading data means modeling accuracy improves.\nReduces Training Time: Less data means that algorithms train faster.\nUnivariate Selection\nStatistical tests can be used to select those features that have the strongest relationship with the output variable. The scikit-learn library provides the SelectKBest class2 that can be used with a suite of different statistical tests to select a specific number of features. The example below uses the chi-squared  (ùëê‚Ñéùëñ2)  statistical test for non-negative features to select 4 of the best features from the Pima Indians onset of diabetes dataset.\n\nYou can see the scores for each attribute and the 4 attributes chosen (those with the highest scores): plas, test, mass and age. I got the names for the chosen attributes by manually mapping the index of the 4 highest scores to the index of the attribute names.\n\n# Recursive Feature Elimination method.\nThe Recursive Feature Elimination (or RFE) works by recursively removing attributes and building a model on those attributes that remain. It uses the model accuracy to identify which attributes (and combination of attributes) contribute the most to predicting the target attribute. You can learn more about the RFE class3 in the scikit-learn documentation. The example below uses RFE with the logistic regression algorithm to select the top 3 features. The choice of algorithm does not matter too much as long as it is skillful and consistent.\n\n### NOTE: Since i dont have much information about the interpretetion of my varaibles, Recursive Feature Elimination method would be the best. \n\n# c) Data Transforms"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LogisticRegression\n\narray_1 = Train.values\nX = array_1[:,0:11]\nY = array_1[:,11]\n# feature extraction\nmodel = LogisticRegression()\nrfe = RFE(model, 10)\nfit = rfe.fit(X, Y)\nprint(\"Num Features: \",  fit.n_features_)\nprint(\"Selected Features:\",  fit.support_)\nprint(\"Feature Ranking: \",  fit.ranking_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feature Importance\nBagged decision trees like Random Forest and Extra Trees can be used to estimate the importance\nof features."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import ExtraTreesClassifier\nimport warnings\nwarnings.filterwarnings('ignore')\n\narray = Train.values\nX = array[:,0:11]\nY = array[:,11]\n# feature extraction\nmodel = ExtraTreesClassifier()\nmodel.fit(X, Y)\nprint(model.feature_importances_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4. Evaluate Algorithms\n\n\n## Evaluate the Performance of Machine Learning Algorithms with Resampling\nYou need to know how well your algorithms perform on unseen data. The best way to evaluate the performance of an algorithm would be to make predictions for new data to which you already know the answers. The second best way is to use clever techniques from statistics called resampling methods that allow you to make accurate estimates for how well your algorithm will perform on new data. In this chapter you will discover how you can estimate the accuracy of your machine learning algorithms using resampling methods in Python and scikit-learn on the Pima Indians dataset. Let's get started.\n\n## Evaluate Machine Learning Algorithms\nWhy can't you train your machine learning algorithm on your dataset and use predictions from this same dataset to evaluate machine learning algorithms? The simple answer is overffitting. Imagine an algorithm that remembers every observation it is shown during training. If you evaluated your machine learning algorithm on the same dataset used to train the algorithm, then an algorithm like this would have a perfect score on the training dataset. But the predictions it made on new data would be terrible. We must evaluate our machine learning algorithms on data that is not used to train the algorithm. The evaluation is an estimate that we can use to talk about how well we think the algorithm may actually do in practice. It is not a guarantee of performance. Once we estimate the performance of our algorithm, we can then re-train the final algorithm on the entire training dataset and get it ready for operational use. Next up we are going to look at four different techniques that we can use to split up our training dataset and create useful estimates of performance for our machine learning algorithms:\n\n* Train and Test Sets.\n* k-fold Cross Validation.\n* Leave One Out Cross Validation.\n* Repeated Random Test-Train Splits.\n\n# a) Split-out validation dataset\n\n### Split into Train and Test Sets\nThe simplest method that we can use to evaluate the performance of a machine learning algorithm is to use different training and testing datasets. We can take our original dataset and split it into two parts. Train the algorithm on the train part, make predictions on the second part and evaluate the predictions against the expected results. The size of the split can depend on the size and species of your dataset, although it is common to use 67% of the data for training and the remaining 33% for testing. This algorithm evaluation technique is very fast. It is ideal for large datasets (millions of records) where there is strong evidence that both splits of the data are representative of the underlying problem. Because of the speed, it is useful to use this approach when the algorithm you are investigating is slow to train. A downside of this technique is that it can have a high variance. This means that di\u000berences in the training and test dataset can result in meaningful diferences in the estimate of accuracy. In the example below we split the Pima Indians dataset into 70%/30% splits for training and test and evaluate the accuracy of a Logistic Regression model."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\narray = Train.values\nX = array[:,0:11]\nY = array[:,11]\ntest_size = 0.30\nseed = 7\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size,\nrandom_state=seed)\nmodel = LogisticRegression()\nmodel.fit(X_train, Y_train)\nresult = model.score(X_test, Y_test)\nprint(\"Accuracy: \",  (result*100.0))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# b) Test options and evaluation metric"},{"metadata":{},"cell_type":"markdown","source":"# K-fold Cross Validation\nCross validation is an approach that you can use to estimate the performance of a machine learning algorithm with less variance than a single train-test set split. It works by splitting the dataset into k-parts (e.g. k = 5 or k = 10). Each split of the data is called a fold. The algorithm is trained on k 1 folds with one held back and tested on the held back fold. This is repeated so that each fold of the dataset is given a chance to be the held back test set. After running cross validation you end up with k diferent performance scores that you can summarize using a mean and a standard deviation. The result is a more reliable estimate of the performance of the algorithm on new data. It is more accurate because the algorithm is trained and evaluated multiple times on different data. The choice of k must allow the size of each test partition to be large enough to be a reasonable sample of the problem, whilst allowing enough repetitions of the train-test evaluation of the algorithm to provide a fair estimate of the algorithms performance on unseen data. For modest sized datasets in the thousands or tens of thousands of records, k values of 3, 5 and 10 are common."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\n\narray = Train.values\nX = array[:,0:11]\nY = array[:,11]\n\nnum_folds = 10 #number of folds to use are 10\nseed = 7 #reproducibility\n\nkfold = KFold(n_splits=num_folds, random_state=seed)\nmodel = LogisticRegression()\nresults = cross_val_score(model, X, Y, cv=kfold)\n\nprint(f\"Accuracy:\", (results.mean()*100.0, results.std()*100.0))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Leave One Out Cross Validation\nYou can configure cross validation so that the size of the fold is 1 (k is set to the number of observations in your dataset). This variation of cross validation is called leave-one-out cross validation. The result is a large number of performance measures that can be summarized in an effort to give a more reasonable estimate of the accuracy of your model on unseen data. A downside is that it can be a computationally more expensive procedure than k-fold cross validation. In the example below we use leave-one-out cross validation."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import LeaveOneOut\nfrom sklearn.model_selection import cross_val_score\n\narray = Train.values\nX = array[:,0:11]\nY = array[:,11]\nnum_folds = 10\nloocv = LeaveOneOut()\nmodel = LogisticRegression()\nresults = cross_val_score(model, X, Y, cv=loocv)\nprint(\"Accuracy:\",  (results.mean()*100.0, results.std()*100.0))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Repeated Random Test-Train Splits\nAnother variation on k-fold cross validation is to create a random split of the data like the train/test split described above, but repeat the process of splitting and evaluation of the algorithm multiple times, like cross validation. This has the speed of using a train/test split and the reduction in variance in the estimated performance of k-fold cross validation. You can also repeat the process many more times as needed to improve the accuracy. A down side is that repetitions may include much of the same data in the train or the test split from run to run, introducing redundancy into the evaluation. The example below splits the data into a 70%/30% train/test split and repeats the process 10 times."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import ShuffleSplit\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import matthews_corrcoef\n\narray = Train.values\nX = array[:,0:11]\nY = array[:,11]\nn_splits = 10\ntest_size = 0.30\nseed = 7\nkfold = ShuffleSplit(n_splits=n_splits, test_size=test_size, random_state=seed)\nmodel = LogisticRegression()\nmodel.fit(X_train, Y_train)\nresults = cross_val_score(model, X, Y, cv=kfold)\nprint(\"Accuracy: \" , (results.mean()*100.0, results.std()*100.0))\n\n\nmcc = matthews_corrcoef(model.predict(X), Y)\nprint('MCC: ',mcc)\n\ncross_validation_report = pd.DataFrame(output)\ncross_validation_report.columns = ['CLASS']\ncross_validation_report.index.name = 'Index'\ncross_validation_report['CLASS'] = cross_validation_report['CLASS'].map({0.0:False, 1.0:True})\n\ncross_validation_report.to_csv('cross_validation_report.csv')\n\nprint(cross_validation_report['CLASS'].unique())\nprint('False: ',cross_validation_report.groupby('CLASS').size()[0].sum())\nprint('True: ',cross_validation_report.groupby('CLASS').size()[1].sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# NOTE: What Techniques to Use When\nThis section lists some tips to consider what resampling technique to use in diferent circum- stances.\n\nGenerally k-fold cross validation is the gold standard for evaluating the performance of amachine learning algorithm on unseen data with k set to 3, 5, or 10.\nUsing a train/test split is good for speed when using a slow algorithm and producesperformance estimates with lower bias when using large datasets.\nTechniques like leave-one-out cross validation and repeated random splits can be usefulintermediates when trying to balance variance in the estimated performance, modeltraining speed and dataset size.\nThe best advice is to experiment and find a technique for your problem that is fast and produces reasonable estimates of performance that you can use to make decisions. If in doubt, use 10-fold cross validation.\n# Machine Learning Algorithm Performance Metrics\n\nThe metrics that you choose to evaluate your machine learning algorithms are very important.\nChoice of metrics in\nuences how the performance of machine learning algorithms is measured\nand compared. They in\nuence how you weight the importance of different characteristics in\nthe results and your ultimate choice of which algorithm to choose.\n\n## Algorithm Evaluation Metrics\nIn this lesson, various different algorithm evaluation metrics are demonstrated for both classification and regression type machine learning problems. In each recipe, the dataset is downloaded\ndirectly from the Machine Learning repository.\n\n## Classiffication Metrics\nClassiffication problems are perhaps the most common type of machine learning problem and as such there are a myriad of metrics that can be used to evaluate predictions for these problems. In this section we will review how to use the following metrics:\n\n* Classiffication Accuracy.\n* Logarithmic Loss.\n* Area Under ROC Curve.\n* Confusion Matrix.\n* Classiffication Report.\n\n## Classiffication Accuracy\nClassiffication accuracy is the number of correct predictions made as a ratio of all predictions made. This is the most common evaluation metric for classi\fcation problems, it is also the most misused. It is really only suitable when there are an equal number of observations in each class (which is rarely the case) and that all predictions and prediction errors are equally important, which is often not the case. Below is an example of calculating classiffication accuracy."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import matthews_corrcoef\narray = Train.values\nX = array[:,0:11]\nY = array[:,11]\nkfold = KFold(n_splits=10, random_state=7)\nmodel = LogisticRegression()\nscoring = 'accuracy'\nresults = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\nprint(\"Accuracy:\", (results.mean(), results.std()))\n\ntest_set = Test.values\nmodel.fit(X, Y)\noutput = model.predict(test_set)\n\nmcc = matthews_corrcoef(model.predict(X), Y)\nprint('MCC: ',mcc)\n\nclassification_accuracy_report = pd.DataFrame(output)\nclassification_accuracy_report.columns = ['CLASS']\nclassification_accuracy_report.index.name = 'Index'\nclassification_accuracy_report['CLASS'] = classification_accuracy_report['CLASS'].map({0.0:False, 1.0:True})\n\nclassification_accuracy_report.to_csv('classification_accuracy_report.cp.csv')\n\nprint(classification_accuracy_report['CLASS'].unique())\nprint('False: ',classification_accuracy_report.groupby('CLASS').size()[0].sum())\nprint('True: ',classification_accuracy_report.groupby('CLASS').size()[1].sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Confusion Matrix\nThe confusion matrix is a handy presentation of the accuracy of a model with two or more classes. The table presents predictions on the x-axis and accuracy outcomes on the y-axis. The cells of the table are the number of predictions made by a machine learning algorithm. For example, a machine learning algorithm can predict 0 or 1 and each prediction may actually have been a 0 or 1. Predictions for 0 that were actually 0 appear in the cell for prediction = 0 and actual = 0, whereas predictions for 0 that were actually 1 appear in the cell for prediction = 0 and actual = 1. And so on."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import matthews_corrcoef\n\narray = Train.values\nX = array[:,0:11]\nY = array[:,11]\ntest_size = 0.30\nseed = 7\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size,\nrandom_state=seed)\n\nmodel = LogisticRegression()\nmodel.fit(X_train, Y_train)\n\npredicted = model.predict(X_test)\nmatrix = confusion_matrix(Y_test, predicted)\nprint(matrix)\n\ntest_set = Test.values\nmodel.fit(X, Y)\noutput = model.predict(test_set)\n\nmcc = matthews_corrcoef(model.predict(X), Y)\nprint('MCC: ',mcc)\n\nconfusion_matrix_report = pd.DataFrame(output)\nconfusion_matrix_report.columns = ['CLASS']\nconfusion_matrix_report.index.name = 'Index'\nconfusion_matrix_report['CLASS'] = confusion_matrix_report['CLASS'].map({0.0:False, 1.0:True})\n\nconfusion_matrix_report.to_csv('confusion_matrix_report.csv')\n\nprint(confusion_matrix_report['CLASS'].unique())\nprint('False: ',confusion_matrix_report.groupby('CLASS').size()[0].sum())\nprint('True: ',confusion_matrix_report.groupby('CLASS').size()[1].sum())\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Classiffication Report\nThe scikit-learn library provides a convenience report when working on classiffication problems to give you a quick idea of the accuracy of a model using a number of measures. The classification report() function displays the precision, recall, F1-score and support for each class. "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import matthews_corrcoef\n\narray = Train.values\nX = array[:,0:11]\nY = array[:,11]\ntest_size = 0.30\nseed = 7\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size,\nrandom_state=seed)\nmodel = LogisticRegression()\nmodel.fit(X_train, Y_train)\npredicted = model.predict(X_test)\nreport = classification_report(Y_test, predicted)\nprint(report)\n\n\ntest_set = Test.values\nmodel.fit(X, Y)\noutput = model.predict(test_set)\n\nmcc = matthews_corrcoef(model.predict(X), Y)\nprint('MCC: ',mcc)\n\nClassification_report = pd.DataFrame(output)\nClassification_report.columns = ['CLASS']\nClassification_report.index.name = 'Index'\nClassification_report['CLASS'] = Classification_report['CLASS'].map({0.0:False, 1.0:True})\n\nClassification_report.to_csv('Classification_report.csv')\n\nprint(Classification_report['CLASS'].unique())\nprint('False: ',Classification_report.groupby('CLASS').size()[0].sum())\nprint('True: ',Classification_report.groupby('CLASS').size()[1].sum())\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Spot-Check Classiffication Algorithms.\nSpot-checking is a way of discovering which algorithms perform well on your machine learning problem. You cannot know which algorithms are best suited to your problem beforehand. You must trial a number of \nmethods and focus attention on those that prove themselves the most promising.\n\n1. How to spot-check machine learning algorithms on a classification problem.\n2. How to spot-check two linear classification algorithms.\n3. How to spot-check four nonlinear classification algorithms.\n\n## Algorithm Spot-Checking\nYou cannot know which algorithm will work best on your dataset beforehand. You must use trial and error to discover a shortlist of algorithms that do well on your problem that you can then double down on and tune further. I call this process spot-checking. The question is not: What algorithm should I use on my dataset? Instead it is: What algorithms should I spot-check on my dataset? You can guess at what algorithms might do well on your dataset, and this can be a good starting point. I recommend trying a mixture of algorithms and see what is good at picking out the structure in your data. Below are some suggestions when spot-checking algorithms on your dataset:\n\nTry a mixture of algorithm representations (e.g. instances and trees).\n\nTry a mixture of learning algorithms (e.g. di\u000berent algorithms for learning the same type of representation).\n\nTry a mixture of modeling types (e.g. linear and nonlinear functions or parametric and nonparametric).\n\nAlgorithms Overview\nWe are going to take a look at six classi\fcation algorithms that you can spot-check on your dataset. Starting with two linear machine learning algorithms:\n\nLogistic Regression.\nLinear Discriminant Analysis.\nThen looking at four nonlinear machine learning algorithms:\n\nk-Nearest Neighbors.\nNaive Bayes.\nClassi\fcation and Regression Trees.\nSupport Vector Machines.\nLinear Machine Learning Algorithms\nThis section demonstrates minimal recipes for how to use two linear machine learning algorithms: logistic regression and linear discriminant analysis.\n\nLogistic Regression\nLogistic regression assumes a Gaussian distribution for the numeric input variables and can model binary classiffication problems. You can construct a logistic regression model using the LogisticRegression class."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Logistic Regression Classification\nfrom pandas import read_csv\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import matthews_corrcoef\n\narray = Train.values\nX = array[:,0:11]\nY = array[:,11]\nnum_folds = 10\nkfold = KFold(n_splits=10, random_state=7)\nmodel = LogisticRegression()\nresults = cross_val_score(model, X, Y, cv=kfold)\nprint(results.mean())\n\ntest_set = Test.values\nmodel.fit(X, Y)\noutput = model.predict(test_set)\n\nmcc = matthews_corrcoef(model.predict(X), Y)\nprint('MCC: ',mcc)\n\nLogisticRegression = pd.DataFrame(output)\nLogisticRegression.columns = ['CLASS']\nLogisticRegression.index.name = 'Index'\nLogisticRegression['CLASS'] = LogisticRegression['CLASS'].map({0.0:False, 1.0:True})\n\nLogisticRegression.to_csv('LogisticRegression.csv')\n\nprint(LogisticRegression['CLASS'].unique())\nprint('False: ',LogisticRegression.groupby('CLASS').size()[0].sum())\nprint('True: ',LogisticRegression.groupby('CLASS').size()[1].sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Linear Discriminant Analysis\nLinear Discriminant Analysis or LDA is a statistical technique for binary and multiclass classiffication. It too assumes a Gaussian distribution for the numerical input variables. You can construct an LDA model using the LinearDiscriminantAnalysis class"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.metrics import matthews_corrcoef\n\narray = Train.values\nX = array[:,0:11]\nY = array[:,11]\nnum_folds = 10\nkfold = KFold(n_splits=10, random_state=7)\nmodel = LinearDiscriminantAnalysis()\nresults = cross_val_score(model, X, Y, cv=kfold)\nprint(results.mean())\n\ntest_set = Test.values\nmodel.fit(X, Y)\noutput = model.predict(test_set)\n\nmcc = matthews_corrcoef(model.predict(X), Y)\nprint('MCC: ',mcc)\n\nLinearDiscriminantAnalysis = pd.DataFrame(output)\nLinearDiscriminantAnalysis.columns = ['CLASS']\nLinearDiscriminantAnalysis.index.name = 'Index'\nLinearDiscriminantAnalysis['CLASS'] = LinearDiscriminantAnalysis['CLASS'].map({0.0:False, 1.0:True})\n\nLinearDiscriminantAnalysis.to_csv('LinearDiscriminantAnalysis.csv')\n\nprint(LinearDiscriminantAnalysis['CLASS'].unique())\nprint('False: ',LinearDiscriminantAnalysis.groupby('CLASS').size()[0].sum())\nprint('True: ',LinearDiscriminantAnalysis.groupby('CLASS').size()[1].sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Nonlinear Machine Learning Algorithms\nThis section demonstrates minimal recipes for how to use 4 nonlinear machine learning algorithms.\n\n## k-Nearest Neighbors\nThe k-Nearest Neighbors algorithm (or KNN) uses a distance metric to find the k most similar instances in the training data for a new instance and takes the mean outcome of the neighbors as the prediction. You can construct a KNN model using the KNeighborsClassifier class."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import matthews_corrcoef\n\narray = Train.values\nX = array[:,0:11]\nY = array[:,11]\nnum_folds = 10\nkfold = KFold(n_splits=10, random_state=7)\nmodel = KNeighborsClassifier()\nresults = cross_val_score(model, X, Y, cv=kfold)\nprint(results.mean())\n\ntest_set = Test.values\nmodel.fit(X, Y)\noutput = model.predict(test_set)\n\nmcc = matthews_corrcoef(model.predict(X), Y)\nprint('MCC: ',mcc)\n\nk_nearest = pd.DataFrame(output)\nk_nearest.columns = ['CLASS']\nk_nearest.index.name = 'Index'\nk_nearest['CLASS'] = k_nearest['CLASS'].map({0.0:False, 1.0:True})\n\nk_nearest.to_csv('k_nearest.csv')\n\nprint(k_nearest['CLASS'].unique())\nprint('False: ',k_nearest.groupby('CLASS').size()[0].sum())\nprint('True: ',k_nearest.groupby('CLASS').size()[1].sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Naive Bayes\nNaive Bayes calculates the probability of each class and the conditional probability of each class given each input value. These probabilities are estimated for new data and multiplied together, assuming that they are all independent (a simple or naive assumption). When working with real-valued data, a Gaussian distribution is assumed to easily estimate the probabilities for input variables using the Gaussian Probability Density Function. You can construct a Naive Bayes model using the GaussianNB class4."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import matthews_corrcoef\n\narray = Train.values\nX = array[:,0:11]\nY = array[:,11]\nkfold = KFold(n_splits=10, random_state=7)\nmodel = GaussianNB()\nresults = cross_val_score(model, X, Y, cv=kfold)\nprint(results.mean())\n\ntest_set = Test.values\nmodel.fit(X, Y)\noutput = model.predict(test_set)\n\nmcc = matthews_corrcoef(model.predict(X), Y)\nprint('MCC: ',mcc)\n\nnaive_bayes = pd.DataFrame(output)\nnaive_bayes.columns = ['CLASS']\nnaive_bayes.index.name = 'Index'\nnaive_bayes['CLASS'] = naive_bayes['CLASS'].map({0.0:False, 1.0:True})\n\nnaive_bayes.to_csv('naive_bayes_nb.csv')\n\nprint(naive_bayes['CLASS'].unique())\nprint('False: ',naive_bayes.groupby('CLASS').size()[0].sum())\nprint('True: ',naive_bayes.groupby('CLASS').size()[1].sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Classiffication and Regression Trees\nClassiffication and Regression Trees (CART or just decision trees) construct a binary tree from the training data. Split points are chosen greedily by evaluating each attribute and each value of each attribute in the training data in order to minimize a cost function (like the Gini index). You can construct a CART model using the DecisionTreeClassifier class"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import matthews_corrcoef\n\narray = Train.values\nX = array[:,0:11]\nY = array[:,11]\nkfold = KFold(n_splits=10, random_state=7)\nmodel = DecisionTreeClassifier()\nresults = cross_val_score(model, X, Y, cv=kfold)\nprint(results.mean())\n\ntest_set = Test.values\nmodel.fit(X, Y)\noutput = model.predict(test_set)\n\nmcc = matthews_corrcoef(model.predict(X), Y)\nprint('MCC: ',mcc)\n\nDecisionTree_report = pd.DataFrame(output)\nDecisionTree_report.columns = ['CLASS']\nDecisionTree_report.index.name = 'Index'\nDecisionTree_report['CLASS'] = DecisionTree_report['CLASS'].map({0.0:False, 1.0:True})\n\nDecisionTree_report.to_csv('DecisionTree_report_dt.csv')\n\nprint(DecisionTree_report['CLASS'].unique())\nprint('False: ',DecisionTree_report.groupby('CLASS').size()[0].sum())\nprint('True: ',DecisionTree_report.groupby('CLASS').size()[1].sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Support Vector Machines\nSupport Vector Machines (or SVM) seek a line that best separates two classes. Those data instances that are closest to the line that best separates the classes are called support vectors and in uence where the line is placed. SVM has been extended to support multiple classes. Of particular importance is the use of di\u000berent kernel functions via the kernel parameter."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\nfrom sklearn.metrics import matthews_corrcoef\n\n\n\narray = Train.values\nX = array[:,0:11]\nY = array[:,11]\nkfold = KFold(n_splits=10)\nmodel = SVC()\nscoring = 'acuracy'\nresults = cross_val_score(model, X, Y, cv=kfold)\nprint(results.mean())\n\ntest_set = Test.values\nmodel.fit(X, Y)\noutput = model.predict(test_set)\n\nmcc = matthews_corrcoef(model.predict(X), Y)\nprint('MCC: ',mcc)\n\nfinalreport1 = pd.DataFrame(output)\nfinalreport1.columns = ['CLASS']\nfinalreport1.index.name = 'Index'\nfinalreport1['CLASS'] = finalreport1['CLASS'].map({0.0:False, 1.0:True})\n\nfinalreport1.to_csv('finalreport1_sv.csv')\n\nprint(finalreport1['CLASS'].unique())\nprint('False: ',finalreport1.groupby('CLASS').size()[0].sum())\nprint('True: ',finalreport1.groupby('CLASS').size()[1].sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Regression Metrics\nIn this section will review 3 of the most common metrics for evaluating predictions on regression\nmachine learning problems:\n- Mean Absolute Error.\n- Mean Squared Error.\n- R2.\n\n## Mean Absolute Error\nThe Mean Absolute Error (or MAE) is the sum of the absolute di\u000berences between predictions\nand actual values. It gives an idea of how wrong the predictions were. The measure gives an\nidea of the magnitude of the error, but no idea of the direction (e.g. over or under predicting).\nThe example below demonstrates calculating mean absolute error on the Boston house price\ndataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"from pandas import read_csv\nfrom sklearn.linear_model import LinearRegression\n\narray = Train.values\nX = array[:,0:11]\nY = array[:,11]\nkfold = KFold(n_splits=10, random_state=7)\nmodel = LinearRegression()\nscoring = 'neg_mean_absolute_error'\nresults = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\nprint(\"MAE:\",(results.mean(), results.std()))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Mean Squared Error\nThe Mean Squared Error (or MSE) is much like the mean absolute error in that it provides a gross idea of the magnitude of error. Taking the square root of the mean squared error converts the units back to the original units of the output variable and can be meaningful for description and presentation. This is called the Root Mean Squared Error (or RMSE). The example below provides a demonstration of calculating mean squared error."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Cross Validation Regression MSE\nfrom pandas import read_csv\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LinearRegression\n\narray = Train.values\nX = array[:,0:11]\nY = array[:,11]\nnum_folds = 10\nkfold = KFold(n_splits=10, random_state=7)\nmodel = LinearRegression()\nscoring = 'neg_mean_squared_error'\nresults = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\nprint(\"MSE:\",(results.mean(), results.std()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This metric too is inverted so that the results are increasing. Remember to take the absolute value before taking the square root if you are interested in calculating the RMSE.\n\n## R2 Metric\nThe R2 (or R Squared) metric provides an indication of the goodness of fit of a set of predictions to the actual values. In statistical literature this measure is called the coefficient of determination. This is a value between 0 and 1 for no-fit and perfect fit respectively. The example below provides a demonstration of calculating the mean R2 for a set of predictions."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Cross Validation Regression R^2\nfrom pandas import read_csv\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LinearRegression\n\narray = Train.values\nX = array[:,0:11]\nY = array[:,11]\n\nkfold = KFold(n_splits=10, random_state=7)\nmodel = LinearRegression()\nscoring = 'r2'\nresults = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\nprint(\"R^2:\",(results.mean(), results.std()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Classiffication and Regression Trees\nClassiffication and Regression Trees (CART or just decision trees) construct a binary tree from the training data. Split points are chosen greedily by evaluating each attribute and each value of each attribute in the training data in order to minimize a cost function (like the Gini index). You can construct a CART model using the DecisionTreeClassifier class"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\narray = Train.values\nX = array[:,0:11]\nY = array[:,11]\nkfold = KFold(n_splits=10, random_state=7)\nmodel = DecisionTreeClassifier()\nresults = cross_val_score(model, X, Y, cv=kfold)\nprint(results.mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# d) Compare Algorithms\n## Compare Machine Learning Algorithms\nIt is important to compare the performance of multiple di\u000berent machine learning algorithms\nconsistently. In this chapter you will discover how you can create a test harness to compare\nmultiple different machine learning algorithms in Python with scikit-learn. You can use this\ntest harness as a template on your own machine learning problems and add more and different\nalgorithms to compare. After completing this lesson you will know:\n\n1. How to formulate an experiment to directly compare machine learning algorithms.\n2. A reusable template for evaluating the performance of multiple algorithms on one dataset.\n3. How to report and visualize the results when comparing algorithm performance.\n\n\n### Choose The Best Machine Learning Model\nWhen you work on a machine learning project, you often end up with multiple good models\nto choose from. Each model will have different performance characteristics. Using resampling\nmethods like cross validation, you can get an estimate for how accurate each model may be on\nunseen data. You need to be able to use these estimates to choose one or two best models from\nthe suite of models that you have created.\nWhen you have a new dataset, it is a good idea to visualize the data using different techniques\nin order to look at the data from di\u000berent perspectives. The same idea applies to model selection.\nYou should use a number of di\u000berent ways of looking at the estimated accuracy of your machine\nlearning algorithms in order to choose the one or two algorithm to finalize. A way to do this is\nto use visualization methods to show the average accuracy, variance and other properties of the\ndistribution of model accuracies. In the next section you will discover exactly how you can do\nthat in Python with scikit-learn.\n\n\n### Compare Machine Learning Algorithms Consistently\nThe key to a fair comparison of machine learning algorithms is ensuring that each algorithm is\nevaluated in the same way on the same data. You can achieve this by forcing each algorithm to be evaluated on a consistent test harness. In the example below six different classiffication\nalgorithms are compared on a single dataset:\n\n- Logistic Regression.\n- Linear Discriminant Analysis.\n- k-Nearest Neighbors.\n- Classiffication and Regression Trees.\n- Naive Bayes.\n- Support Vector Machines.\n\nThe dataset is the Pima Indians onset of diabetes problem. The problem has two classes and\neight numeric input variables of varying scales. The 10-fold cross validation procedure is used to\nevaluate each algorithm, importantly con\fgured with the same random seed to ensure that the\nsame splits to the training data are performed and that each algorithm is evaluated in precisely\nthe same way. Each algorithm is given a short name, useful for summarizing results afterward."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compare Algorithms\nfrom pandas import read_csv\nfrom matplotlib import pyplot\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import matthews_corrcoef\n\n# load dataset\narray = Train.values\n\n#split the dataset \nX = array[:,0:11]\nY = array[:,11]\n\n# prepare models and add them to a list\nmodels = []\nmodels.append(('LR', LogisticRegression()))\nmodels.append(('LDA', LinearDiscriminantAnalysis()))\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('CART', DecisionTreeClassifier()))\nmodels.append(('NB', GaussianNB()))\nmodels.append(('SVM', SVC()))\n\n# evaluate each model in turn\nresults = []\nnames = []\nscoring = 'accuracy'\n\nfor name, model in models:\n    kfold = KFold(n_splits=10, random_state=7)\n    cv_results = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n    results.append(cv_results)\n    names.append(name)\n    msg = (name, cv_results.mean(), cv_results.std())\n    print(msg)\n\n# boxplot algorithm comparison\nfig = pyplot.figure()\nfig.suptitle('Algorithm Comparison')\nax = fig.add_subplot(111)\npyplot.boxplot(results)\nax.set_xticklabels(names)\npyplot.show()\n\ntest_set = Test.values\nmodel.fit(X, Y)\noutput = model.predict(test_set)\n\nmcc = matthews_corrcoef(model.predict(X), Y)\nprint('MCC: ',mcc)\n\ncompare_testmethods_report = pd.DataFrame(output)\ncompare_testmethods_report.columns = ['CLASS']\ncompare_testmethods_report.index.name = 'Index'\ncompare_testmethods_report['CLASS'] = compare_testmethods_report['CLASS'].map({0.0:False, 1.0:True})\n\ncompare_testmethods_report.to_csv('compare_testmethods_report.csv')\n\nprint(compare_testmethods_report['CLASS'].unique())\nprint('False: ',compare_testmethods_report.groupby('CLASS').size()[0].sum())\nprint('True: ',compare_testmethods_report.groupby('CLASS').size()[1].sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Automate Machine Learning Workflows with Pipelines\n\nThere are standard work\nows in a machine learning project that can be automated. In Python\nscikit-learn, Pipelines help to clearly define and automate these work\nows. In this chapter you\nwill discover Pipelines in scikit-learn and how you can automate common machine learning\nwork\nows. After completing this lesson you will know:\n\n1. How to use pipelines to minimize data leakage.\n2. How to construct a data preparation and modeling pipeline.\n3. How to construct a feature extraction and modeling pipeline.\n\n### Automating Machine Learning Workflows\nThere are standard work\nows in applied machine learning. Standard because they overcome\ncommon problems like data leakage in your test harness. Python scikit-learn provides a Pipeline\nutility to help automate machine learning work\nows. Pipelines work by allowing for a linear\nsequence of data transforms to be chained together culminating in a modeling process that can\nbe evaluated.\n\nThe goal is to ensure that all of the steps in the pipeline are constrained to the data available\nfor the evaluation, such as the training dataset or each fold of the cross validation procedure.\nYou can learn more about Pipelines in scikit-learn by reading the Pipeline section1 of the user\nguide. You can also review the API documentation for the Pipeline and FeatureUnion classes\nand the pipeline module2.\n\n### Data Preparation and Modeling Pipeline\nAn easy trap to fall into in applied machine learning is leaking data from your training dataset\nto your test dataset. To avoid this trap you need a robust test harness with strong separation of training and testing. This includes data preparation. Data preparation is one easy way to leak\nknowledge of the whole training dataset to the algorithm. For example, preparing your data\nusing normalization or standardization on the entire training dataset before learning would not\nbe a valid test because the training dataset would have been in\nuenced by the scale of the data\nin the test set.\n\n\nPipelines help you prevent data leakage in your test harness by ensuring that data preparation\nlike standardization is constrained to each fold of your cross validation procedure. The example\nbelow demonstrates this important data preparation and model evaluation work\now on the\nPima Indians onset of diabetes dataset. The pipeline is de\fned with two steps:\n\n1. Standardize the data.\n2. Learn a Linear Discriminant Analysis model.\n\nThe pipeline is then evaluated using 10-fold cross validation."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a pipeline that standardizes the data then creates a model\nfrom pandas import read_csv\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.metrics import matthews_corrcoef\n\n# load data\n\n#dataframe = read_csv('diabetes.csv')\narray = Train.values\nX = array[:,0:11]\nY = array[:,11]\n\n# create pipeline\nestimators = []\nestimators.append(('standardize', StandardScaler()))\nestimators.append(('lda', LinearDiscriminantAnalysis()))\nmodel = Pipeline(estimators)\n\n\n# evaluate pipeline\nkfold = KFold(n_splits=10, random_state=7)\nresults = cross_val_score(model, X, Y, cv=kfold)\nprint(results.mean())\n\n\ntest_set = Test.values\nmodel.fit(X, Y)\noutput = model.predict(test_set)\n\nmcc = matthews_corrcoef(model.predict(X), Y)\nprint('MCC: ',mcc)\n\nstandarzied_pipeline_report = pd.DataFrame(output)\nstandarzied_pipeline_report.columns = ['CLASS']\nstandarzied_pipeline_report.index.name = 'Index'\nstandarzied_pipeline_report['CLASS'] = standarzied_pipeline_report['CLASS'].map({0.0:False, 1.0:True})\n\nstandarzied_pipeline_report.to_csv('standarzied_pipeline_report.csv')\n\nprint(standarzied_pipeline_report['CLASS'].unique())\nprint('False: ',standarzied_pipeline_report.groupby('CLASS').size()[0].sum())\nprint('True: ',standarzied_pipeline_report.groupby('CLASS').size()[1].sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5. Improve Accuracy\n# a) Algorithm Tuning\n# b) Ensembles\n\n# Improve Performance with Ensembles\n\nEnsembles can give you a boost in accuracy on your dataset. In this chapter you will discover\nhow you can create some of the most powerful types of ensembles in Python using scikit-learn.\nThis lesson will step you through Boosting, Bagging and Majority Voting and show you how you\ncan continue to ratchet up the accuracy of the models on your own datasets. After completing\nthis lesson you will know:\n\n1. How to use bagging ensemble methods such as bagged decision trees, random forest and extra trees.\n2. How to use boosting ensemble methods such as AdaBoost and stochastic gradient boosting.\n3. How to use voting ensemble methods to combine the predictions from multiple algorithms.\n\n### Combine Models Into Ensemble Predictions\nThe three most popular methods for combining the predictions from different models are:\n\n- Bagging. Building multiple models (typically of the same type) from different subsamples of the training dataset.\n- Boosting. Building multiple models (typically of the same type) each of which learns to fix the prediction errors of a prior model in the sequence of models.\n- Voting. Building multiple models (typically of di\u000bering types) and simple statistics (like calculating the mean) are used to combine predictions.\n\nThis assumes you are generally familiar with machine learning algorithms and ensemble\nmethods and will not go into the details of how the algorithms work or their parameters.\nThe Pima Indians onset of Diabetes dataset is used to demonstrate each algorithm. Each\nensemble algorithm is demonstrated using 10-fold cross validation and the classiffication accuracy\nperformance metric.\n"},{"metadata":{},"cell_type":"markdown","source":"# Boosting Algorithms\nBoosting ensemble algorithms creates a sequence of models that attempt to correct the mistakes\nof the models before them in the sequence. Once created, the models make predictions which\nmay be weighted by their demonstrated accuracy and the results are combined to create a final\noutput prediction. The two most common boosting ensemble machine learning algorithms are:\n\n- AdaBoost.\n- Stochastic Gradient Boosting.\n\n\n### AdaBoost\nAdaBoost was perhaps the \frst successful boosting ensemble algorithm. It generally works\nby weighting instances in the dataset by how easy or di\u000ecult they are to classify, allowing\nthe algorithm to pay or less attention to them in the construction of subsequent models. You\ncan construct an AdaBoost model for classi\fcation using the AdaBoostClassifier class4. The\nexample below demonstrates the construction of 30 decision trees in sequence using the AdaBoost\nalgorithm."},{"metadata":{"trusted":true},"cell_type":"code","source":"# AdaBoost Classification\nfrom pandas import read_csv\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.metrics import matthews_corrcoef\n\n\narray = Train.values\n\nX = array[:,0:11]\nY = array[:,11]\n\nnum_trees = 30\nseed=7\n\nkfold = KFold(n_splits=10, random_state=seed)\n\nmodel = AdaBoostClassifier(n_estimators=num_trees, random_state=seed)\nresults = cross_val_score(model, X, Y, cv=kfold)\n\nprint(results.mean())\n\n\ntest_set = Test.values\nmodel.fit(X, Y)\noutput = model.predict(test_set)\n\nmcc = matthews_corrcoef(model.predict(X), Y)\nprint('MCC: ',mcc)\n\nAdaBoost_report = pd.DataFrame(output)\nAdaBoost_report.columns = ['CLASS']\nAdaBoost_report.index.name = 'Index'\nAdaBoost_report['CLASS'] = AdaBoost_report['CLASS'].map({0.0:False, 1.0:True})\n\nAdaBoost_report.to_csv('AdaBoost_report.csv')\n\nprint(AdaBoost_report['CLASS'].unique())\nprint('False: ',AdaBoost_report.groupby('CLASS').size()[0].sum())\nprint('True: ',AdaBoost_report.groupby('CLASS').size()[1].sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Stochastic Gradient Boosting\nStochastic Gradient Boosting (also called Gradient Boosting Machines) are one of the most\nsophisticated ensemble techniques. It is also a technique that is proving to be perhaps one of\nthe best techniques available for improving performance via ensembles. You can construct a\nGradient Boosting model for classiffication using the GradientBoostingClassifier class5. The\nexample below demonstrates Stochastic Gradient Boosting for classification with 100 trees"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Stochastic Gradient Boosting Classification\nfrom pandas import read_csv\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import GradientBoostingClassifier\n\narray = Train.values\n\nX = array[:,0:11]\nY = array[:,11]\n\nseed = 7\nnum_trees = 100\n\nkfold = KFold(n_splits=10, random_state=seed)\nmodel = GradientBoostingClassifier(n_estimators=num_trees, random_state=seed)\nresults = cross_val_score(model, X, Y, cv=kfold)\nprint(results.mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## XGB"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Stochastic X Gradient Boosting Classification\nfrom pandas import read_csv\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom xgboost import XGBClassifier\n\narray = Train.values\n\nX = array[:,0:11]\nY = array[:,11]\n\nseed = 7\nnum_trees = 100\n\nkfold = KFold(n_splits=10, random_state=seed)\nmodel = XGBClassifier(n_estimators=num_trees, random_state=seed)\nresults = cross_val_score(model, X, Y, cv=kfold)\nprint(results.mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Voting Ensemble\nVoting is one of the simplest ways of combining the predictions from multiple machine learning\nalgorithms. It works by first creating two or more standalone models from your training dataset.\nA Voting Classiffier can then be used to wrap your models and average the predictions of the\nsub-models when asked to make predictions for new data. The predictions of the sub-models can\nbe weighted, but specifying the weights for classiffiers manually or even heuristically is difficult.\nMore advanced methods can learn how to best weight the predictions from sub-models, but this\nis called stacking (stacked aggregation) and is currently not provided in scikit-learn.\nYou can create a voting ensemble model for classiffication using the VotingClassifier\nclass6. The code below provides an example of combining the predictions of logistic regression,\nclassiffication and regression trees and support vector machines together for a classiffication\nproblem."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Voting Ensemble for Classification\nfrom pandas import read_csv\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.metrics import matthews_corrcoef\n\n\n\narray = Train.values\n\nX = array[:,0:11]\nY = array[:,11]\nkfold = KFold(n_splits=10, random_state=7)\n\n# create the sub models\nestimators = []\nmodel1 = LogisticRegression()\nestimators.append(('logistic', model1))\n\nmodel2 = DecisionTreeClassifier()\nestimators.append(('cart', model2))\n\nmodel3 = SVC()\nestimators.append(('svm', model3))\n\nmodel4 = XGBClassifier()\nestimators.append(('xgb', model4))\n\nmodel5 = RandomForestClassifier()\nestimators.append(('rfc', model5))\n\n# create the ensemble model\nensemble = VotingClassifier(estimators)\nresults = cross_val_score(ensemble, X, Y, cv=kfold)\nprint(results.mean())\n\n\ntest_set = Test.values\nmodel.fit(X, Y)\noutput = model.predict(test_set)\n\nmcc = matthews_corrcoef(model.predict(X), Y)\nprint('MCC: ',mcc)\n\nVoting_Ensemble_report = pd.DataFrame(output)\nVoting_Ensemble_report.columns = ['CLASS']\nVoting_Ensemble_report.index.name = 'Index'\nVoting_Ensemble_report['CLASS'] = Voting_Ensemble_report['CLASS'].map({0.0:False, 1.0:True})\n\nVoting_Ensemble_report.to_csv('Voting_Ensemble_report.csv')\n\nprint(Voting_Ensemble_report['CLASS'].unique())\nprint('False: ',Voting_Ensemble_report.groupby('CLASS').size()[0].sum())\nprint('True: ',Voting_Ensemble_report.groupby('CLASS').size()[1].sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Extra Trees\nExtra Trees are another modi\fcation of bagging where random trees are constructed from\nsamples of the training dataset. You can construct an Extra Trees model for classiffication using\nthe ExtraTreesClassifier class3. The example below provides a demonstration of extra trees\nwith the number of trees set to 100 and splits chosen from 7 random features."},{"metadata":{"trusted":true},"cell_type":"code","source":"from pandas import read_csv\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.metrics import matthews_corrcoef\n\n#let's read the data\n\narray = Train.values\n\nX = array[:,0:11]\nY = array[:,11]\n\nnum_trees = 100\nmax_features = 7\n\nkfold = KFold(n_splits=10, random_state=7)\n\nmodel = ExtraTreesClassifier(n_estimators=num_trees, max_features=max_features)\n\nresults = cross_val_score(model, X, Y, cv=kfold)\n\nprint(results.mean())\n\ntest_set = Test.values\nmodel.fit(X, Y)\noutput = model.predict(test_set)\n\nmcc = matthews_corrcoef(model.predict(X), Y)\nprint('MCC: ',mcc)\n\nExtra_tree_report = pd.DataFrame(output)\nExtra_tree_report.columns = ['CLASS']\nExtra_tree_report.index.name = 'Index'\nExtra_tree_report['CLASS'] = Extra_tree_report['CLASS'].map({0.0:False, 1.0:True})\n\nExtra_tree_report.to_csv('Extra_tree_report.csv')\n\nprint(Extra_tree_report.groupby('CLASS').size()[1].sum())\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Stochastic Gradient Boosting\nStochastic Gradient Boosting (also called Gradient Boosting Machines) are one of the most\nsophisticated ensemble techniques. It is also a technique that is proving to be perhaps one of\nthe best techniques available for improving performance via ensembles."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Stochastic Gradient Boosting Classification\nfrom pandas import read_csv\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.metrics import matthews_corrcoef\n\n\narray = Train.values\n\nX = array[:,0:11]\nY = array[:,11]\n\nseed = 7\nnum_trees = 100\n\nkfold = KFold(n_splits=10, random_state=seed)\nmodel = GradientBoostingClassifier(n_estimators=num_trees, random_state=seed)\nresults = cross_val_score(model, X, Y, cv=kfold)\nprint(results.mean())\n\n\n\ntest_set = Test.values\nmodel.fit(X, Y)\noutput = model.predict(test_set)\n\nmcc = matthews_corrcoef(model.predict(X), Y)\nprint('MCC: ',mcc)\n\nStochastic_report = pd.DataFrame(output)\nStochastic_report.columns = ['CLASS']\nStochastic_report.index.name = 'Index'\nStochastic_report['CLASS'] = Stochastic_report['CLASS'].map({0.0:False, 1.0:True})\n\nStochastic_report.to_csv('Stochastic_report.csv')\n\nprint(Stochastic_report.groupby('CLASS').size()[1].sum())\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Finalize Your Model with pickle\nPickle is the standard way of serializing objects in Python. You can use the pickle1 operation\nto serialize your machine learning algorithms and save the serialized format to a file. Later you\ncan load this file to deserialize your model and use it to make new predictions. The example\nbelow demonstrates how you can train a logistic regression model on data set and save the model to file and load it to make predictions on the unseen test set."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Save Model Using Pickle\nfrom pandas import read_csv\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom pickle import dump\nfrom pickle import load\nfrom sklearn.metrics import matthews_corrcoef\n\narray = Train.values\nX = array[:,0:11]\nY = array[:,11]\n\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.30, random_state=7)\n# Fit the model on 30%\nmodel = LogisticRegression()\nmodel.fit(X_train, Y_train)\n\n# save the model to disk\nfilename = 'finalized_model2.sav'\ndump(model, open(filename, 'wb'))\n\n# some time later...\n# load the model from disk\nloaded_model = load(open(filename, 'rb'))\nresult = loaded_model.score(X_test, Y_test)\nprint(result)\n\n\ntest_set = Test.values\nmodel.fit(X, Y)\noutput = model.predict(test_set)\n\nmcc = matthews_corrcoef(model.predict(X), Y)\nprint('MCC: ',mcc)\n\npickle_report = pd.DataFrame(output)\npickle_report.columns = ['CLASS']\npickle_report.index.name = 'Index'\npickle_report['CLASS'] = pickle_report['CLASS'].map({0.0:False, 1.0:True})\n\npickle_report.to_csv('pickle.csv')\n\nprint(pickle_report['CLASS'].unique())\nprint('False: ',pickle_report.groupby('CLASS').size()[0].sum())\nprint('True: ',pickle_report.groupby('CLASS').size()[1].sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Finalize Your Model with Joblib\nThe Joblib2 library is part of the SciPy ecosystem and provides utilities for pipelining Python\njobs. It provides utilities for saving and loading Python objects that make use of NumPy data\nstructures, efficiently3. This can be useful for some machine learning algorithms that require a\nlot of parameters or store the entire dataset (e.g. k-Nearest Neighbors). The example below\ndemonstrates how you can train a logistic regression model on the dataset, save the model to file using Joblib and load it to make predictions on the unseen test\nset."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Save Model Using joblib\nfrom pandas import read_csv\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.externals.joblib import dump\nfrom sklearn.externals.joblib import load\nfrom sklearn.metrics import matthews_corrcoef\n\n\narray = Train.values\nX = array[:,0:11]\nY = array[:,11]\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.30, random_state=7)\n\n# Fit the model on 30%\nmodel = LogisticRegression()\nmodel.fit(X_train, Y_train)\n\n# save the model to disk\n\ndump(model, filename)\n\n# some time later...\n# load the model from disk\nloaded_model = load(filename)\nresult = loaded_model.score(X_test, Y_test)\nprint(result)\n\n\ntest_set = Test.values\nmodel.fit(X, Y)\noutput = model.predict(test_set)\n\nmcc = matthews_corrcoef(model.predict(X), Y)\nprint('MCC: ',mcc)\n\njoblib_report = pd.DataFrame(output)\njoblib_report.columns = ['CLASS']\njoblib_report.index.name = 'Index'\npicklej_report['CLASS'] = pickle_report['CLASS'].map({0.0:False, 1.0:True})\n\npickle_report.to_csv('pickle.csv')\n\nprint(pickle_report['CLASS'].unique())\nprint('False: ',pickle_report.groupby('CLASS').size()[0].sum())\nprint('True: ',pickle_report.groupby('CLASS').size()[1].sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}