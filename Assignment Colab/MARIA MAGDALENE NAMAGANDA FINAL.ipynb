{"cells":[{"metadata":{},"cell_type":"markdown","source":"# MARIA MAGDALENE NAMAGANDA\n# 2019/HD07/24853U\n# Ace_class Kaggle assignment_1"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Importing some of the needed packages and libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib import pyplot\nimport seaborn as sns\n\nfrom sklearn.model_selection import KFold\nfrom pandas import read_csv\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> *Now i need to get rid of some unnecessary warnings by ignoring them*"},{"metadata":{"trusted":true},"cell_type":"code","source":"#To avoid unnecessary warnings, I will ignore them in the code below\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Loading the datasets \n### There are two datasets given; \n1. AMP_TrainSet.csv and \n2. Test.csv  "},{"metadata":{"_cell_guid":"","_uuid":"","trusted":true},"cell_type":"code","source":"#Loading the datasets, \n\nTrain = pd.read_csv(\"../input/amp-data-set/AMP_TrainSet.csv\")\nTest = pd.read_csv(\"../input/amp-data-set/Test.csv\")\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### For training the model, I will first use the training set which I will further split into the train and validation set. \n### Then I will test the model on the Test set provided to gauge its performance."},{"metadata":{},"cell_type":"markdown","source":"# Exploring my data\n## Exploring data helps one understand what kind of data is given. That is to say knowing how much data it is, the shape, type, dimensions, missing values, data summary, correlation of attributes among others as am going to do in the following steps."},{"metadata":{"trusted":true},"cell_type":"code","source":"#here, am trying to find the type of data\ntype(Train)\ntype(Test)\nTrain.dtypes, Test.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# checking the dimensions of the data\n# this retuns the number of rows and columns in the data\n\nTrain.shape, Test.shape\n\n#this helps to know how big the data is in terms of rows and columns.\n#also from here I can tell which data is labeled","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data description\n>For now, I will focus more on the train dataset because its what I will use to train the model "},{"metadata":{"trusted":true},"cell_type":"code","source":"#getting a description of the train dataset\n#description gives a summary of the data.\n\nTrain.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#looking at the first 5 entries of my data\nTrain.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Class Distribution\n> From the Train dataset, i see there is an extra 'CLASS' column which will be my validation set.\n### I need to know how this class is distributed to guide me on what to do with it."},{"metadata":{"trusted":true},"cell_type":"code","source":"#to see the class distribution, I will plot a bar graph\nTrain.groupby('CLASS').size().plot(kind='bar')\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> I would like to know how many instaces i have for each class"},{"metadata":{"trusted":true},"cell_type":"code","source":"#getting the number of instances in each class\nTrain.groupby('CLASS').size()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### There are 1519 intances for each class which is proof for even distribution.\n### From the above barplot and class instances, I can see that the classes are evenly distributed so no need to use smote."},{"metadata":{},"cell_type":"markdown","source":"# Data Visualisation\n### Data visualization is the technique to present the data in a pictorial or graphical format. It enables one to analyze data visually. The data in a graphical format allows identification of new trends and patterns easily.\n### Visualisation identifies the relationship between data points and variables."},{"metadata":{},"cell_type":"markdown","source":"## Density plots"},{"metadata":{},"cell_type":"markdown","source":"> I chose to use density plots because they are better at determing the distribution shape as they are not affected by the number of bins as is the case with Histograms."},{"metadata":{"trusted":true},"cell_type":"code","source":"#now plotting density subplots\n#setting the figsize to 12 so that my graphs are not congested\nTrain.plot(kind='density', subplots = True, layout=(3,4), sharex= False, sharey= False, figsize=(12,12))\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> From the above density subpots, I can see that most of the data follows a Gaussian distribution given some of the characteristics such as bell shaped curves and graphs being symmetrical about the mean."},{"metadata":{},"cell_type":"markdown","source":"## Box and whisker plots\n> A box plot is a type of graph that displays a summary of a large amount of data in terms of the median, upper quartile, lower quartile, minimum and maximum data values.\n\n> It handles large data easily, gives a clear summary and displays outliers."},{"metadata":{"trusted":true},"cell_type":"code","source":"#plotting a box and whisker graph\n#setting the figsize to 10 so that the plots are not congested.\nTrain.plot(kind='box', subplots = True, layout=(3,4), sharex= False, sharey= False, figsize=(10,10))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Looking at the correlation of the data"},{"metadata":{},"cell_type":"markdown","source":"> Correlation is the relationship between two variables. The commonly used method is Pearson's correlation coefficient. It assumes a normal distribution of the attributes involved.\n\n> -1 shows full negative correlation, \n\n> +1 shows full negative correlation and \n\n> 0 shows no correlation at all."},{"metadata":{"trusted":true},"cell_type":"code","source":"#first I will checkfor the pairwise correlation of the attributes.\nTrain.corr(method='pearson')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Then now am reviewing the inter-correlation of attributes using heatmap\n#graphical representation\nplt.figure(figsize=(6,6))\nsns.heatmap(Train.corr(method='pearson'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Ill also check the correlation in regards to the 'CLASS' attribute\nTrain.corr(method='pearson')['CLASS']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Skewnwess of the data\n> knowing data skewness allows one to perform data preparation and improve a model\n\n> If Skewness value lies above +1 or below -1 then the data is highly skewed. \n\n> If skewness value lies between +0.5 and -0.5 then the data ids moderately skewed.\n\n> If skewness is 0 then data is symmetrical"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking out skewness of data\nTrain.skew().plot(kind='bar')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Comparing Machine Learning Algorithms\n>  This helps to choose the best model for the problem at hand.\n\n>  Using resampling methods like cross validation, gives an estimate for how accurate each model may be on unseen data.\n\n>  It is important to ensure that each algorithm is evaluated in the same way on the same data to avoid bias."},{"metadata":{"trusted":true},"cell_type":"code","source":"#comparing different models to from which ill choose.\n\n\n# load dataset\n\narray = Train.values\n\n#split the dataset \nX = array[:,0:11]  #X = Train.drop(columns=['CLASS'])\nY = array[:,11]   #Y = Train['CLASS']\n\n# preparing models and adding them to a list\nmodels = []\nmodels.append(('LR', LogisticRegression()))\nmodels.append(('LDA', LinearDiscriminantAnalysis()))\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('CART', DecisionTreeClassifier()))\nmodels.append(('NB', GaussianNB()))\nmodels.append(('SVM', SVC()))\nmodels.append(('AB', AdaBoostClassifier()))\nmodels.append(('GBC', GradientBoostingClassifier()))\nmodels.append(('EXT', ExtraTreesClassifier()))\nmodels.append(('RTC', RandomForestClassifier()))\n#models.append(('XGB', XGBClassifier)) \n#am commenting out XGB it does not seem to be a scikit-learn estimator as it does not implement a 'get_params' methods.\n\n# evaluating each model in turn\nresults = []\nnames = []\n\nfor name, model in models:\n    kfold = KFold(n_splits=50, random_state=42)\n    cv_results = cross_val_score(model, X, Y, cv=kfold, scoring='accuracy')\n    results.append(cv_results)\n    names.append(name)\n    msg = (name, cv_results.mean(), cv_results.std())\n    print(msg)\n\n# boxplot algorithm comparison\nfig = pyplot.figure()\nfig.suptitle('Algorithm Comparison')\nax = fig.add_subplot(111)\npyplot.boxplot(results)\nax.set_xticklabels(names)\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ## From the above algorithm comparison, I can see that Naive Bayes(NB), ExtraTreesClassifiers(EXT) and RandomForestClassifiers(RTC) are some of the best performing algorithms. \n> ## Am yet to find out the overall best valgorithm after prediction on the Test set"},{"metadata":{},"cell_type":"markdown","source":"# Feature selection \n## Using recursive feature elimination\n> Sometimes, you may asses a dataset and find out that you do not need to use all the given features. This can be because some of them are highly correlates or they are not in any way helpful in developing the model.\n* It is therefore important to get rid of some features where applicable.\n\n> For this data, I will use Recursive Feature Elimination(RFE)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#feature selection using RFE\n#first i will start with choosing 4 features\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LogisticRegression\n\narray = Train.values\nX = array[:,0:11]\nY = array[:,11]\n# feature extraction\nmodel = LogisticRegression()\nrfe = RFE(model, 4)\nfit = rfe.fit(X, Y)\nprint(\"Num Features: \", fit.n_features_)\nprint(\"Selected Features:\", fit.support_)\nprint(\"Feature Ranking: \", fit.ranking_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Calling out the column names so I can know which features am going to drop from RFE\nTrain.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Using Feature importance"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import ExtraTreesClassifier\n\narray = Train.values\nX = array[:,0:11]\nY = array[:,11]\n# feature extraction\nmodel = ExtraTreesClassifier()\nmodel.fit(X, Y)\nprint(model.feature_importances_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## If am to use 4 features and drop the rest\n> I will assign the new dataset a variable 'New_Train4' after choosing the 4 features and dropping the rest."},{"metadata":{"trusted":true},"cell_type":"code","source":"# I will call the new Train data with selected features New_Train4.\nTrain\nNew_Train4 = Train.drop(['FULL_Charge', 'FULL_AcidicMolPerc', 'FULL_DAYM780201', 'FULL_GEOR030101', 'AS_MeanAmphiMoment', 'AS_DAYM780201', 'AS_FUKS010112'], axis =1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#dropping the same features in the test dataset\nNew_Test4 = Test.drop(['FULL_Charge', 'FULL_AcidicMolPerc', 'FULL_DAYM780201', 'FULL_GEOR030101', 'AS_MeanAmphiMoment', 'AS_DAYM780201', 'AS_FUKS010112'], axis =1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#viewing the selected features\nNew_Train4.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#now splitting data\n#array = New_Train.values\nX = New_Train4.values[:,0:4]\nY = New_Train4.values[:,4]\nfrom sklearn.model_selection import train_test_split\nX_Train, X_Test, Y_Train, Y_Test = train_test_split(X, Y, test_size=0.2, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# also train and test the model on Matthews correlation coefficient.\nfrom sklearn.metrics import matthews_corrcoef\n\nGS = GaussianNB()\nGS.fit(X_Train,Y_Train)\npred = GS.predict(X_Test)\n\nprint(\"The result is: \",np.round(matthews_corrcoef(Y_Test,pred) *100,2),\" Mathew's Coef\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Using four features gives me a very low MCC and low overall score.\n## I'll therefore consider using 8 features and see what score  I get."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LogisticRegression\n\narray = Train.values\nX = array[:,0:11]\nY = array[:,11]\n# feature extraction\nmodel = LogisticRegression()\nrfe = RFE(model, 8)\nfit = rfe.fit(X, Y)\nprint(\"Num Features: \", fit.n_features_)\nprint(\"Selected Features:\", fit.support_)\nprint(\"Feature Ranking: \", fit.ranking_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# I will call the new Train data with selected features New_Train8.\nTrain\nNew_Train8 = Train.drop(['FULL_AcidicMolPerc', 'FULL_DAYM780201', 'AS_DAYM780201'], axis =1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#dropping the same features in the test dataset\nNew_Test8 = Test.drop(['FULL_AcidicMolPerc', 'FULL_DAYM780201', 'AS_DAYM780201'], axis =1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#now splitting data\n#array = New_Train.values\n\n\nX = New_Train8.values[:,0:8]\nY = New_Train8.values[:,8]\nfrom sklearn.model_selection import train_test_split\nX_Train, X_Test, Y_Train, Y_Test = train_test_split(X, Y, test_size=0.2, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#now creating a model and training it on 8 features\nmodel = LogisticRegression(solver='liblinear', C=0.05, multi_class='ovr', random_state=30)\nmodel.fit(X_Train, Y_Train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_Train, X_Test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2,\nrandom_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import matthews_corrcoef\n\nnv = GaussianNB()\nnv.fit(X_Train,Y_Train)\npred = nv.predict(X_Test)\n\nprint(\"The result is: \",np.round(matthews_corrcoef(Y_Test,pred) *100,2),\" Mathew's Coef\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Standardising data\n> This is useful to transform attributes with a Gaussian distribution and it workd better with rescaled data(also known as normalistaion where attributes are scaled into a range between 0 and 1)"},{"metadata":{},"cell_type":"markdown","source":"# Below are some of the algorithms I will be using;"},{"metadata":{},"cell_type":"markdown","source":"# LINEAR ALGORITHMS\n1. Linear Regression\n2. Logistic Regression\n3. Linear Discriminant Analysis"},{"metadata":{},"cell_type":"markdown","source":"# Logistic Regression using 8 features"},{"metadata":{"trusted":true},"cell_type":"code","source":"# here am using a dataset 'New_Train8' with 8 selected features\narray = New_Train8.values\nX = array[:,0:8]\nY = array[:,8]\n\nkfold = KFold(n_splits=10, random_state=42) #spliiting my data into 10 folds and random_state of 42 for reproducibility\nmodel = LogisticRegression() #calling out the prediction algorithm\nresults = cross_val_score(model, X, Y, cv=kfold) #estimating the model on new data and assigning it to results variable\nprint(results.mean())  #getting the mean of the accuracy scores from cross validation scores\n\nmodel.fit(X,Y)\noutput = model.predict(New_Test8.values) #testing the model on the test_set (Test.values)\n\nfrom sklearn.metrics import matthews_corrcoef\nmcc = matthews_corrcoef(model.predict(X),Y)\nprint('MCC:',mcc)\n                       \nmaria_logistic = pd.DataFrame(output)  #here we are converting the array into a pandas dataframe which will give a single column\nmaria_logistic.columns = ['CLASS'] #renaming the output column to 'CLASS'\nmaria_logistic.index.name = \"Index\"  #naming the index column as 'Index'\nmaria_logistic['CLASS']=maria_logistic['CLASS'].map({0.0:False, 1.0:True}) #converting '0.0' to' False' and '1.0' to 'True'\nmaria_logistic.to_csv(\"maria_logistic.csv\") #changing my output file as a 'csv' file\n\nprint(maria_logistic['CLASS'].unique()) #checking out the unique instances in the 'CLASS' column\nprint('False: ',maria_logistic.groupby('CLASS').size()[0].sum()) #summing up the '0' instances in the 'CLASS' column\nprint('True: ',maria_logistic.groupby('CLASS').size()[1].sum()) #summing up the '1' instances in the 'CLASS' column","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Applying Linear Dicriminant Analysis (LDA) using all features "},{"metadata":{"trusted":true},"cell_type":"code","source":"array = Train.values\nX = array[:,0:11]\nY = array[:,11]\nnum_folds = 10\nkfold = KFold(n_splits=10, random_state=42) #spliiting my data into 10 folds and random_state of 42 for reproducibility\nmodel = LinearDiscriminantAnalysis() #calling out the prediction algorithm\nresults = cross_val_score(model, X, Y, cv=kfold) #estimating the model on new data and assigning it to results variable\nprint(results.mean())  #getting the mean of the accuracy scores from cross validation scores\n\nmodel.fit(X,Y)\noutput = model.predict(Test.values) #testing the model on the test_set (Test.values)\n\nfrom sklearn.metrics import matthews_corrcoef\nmcc = matthews_corrcoef(model.predict(X),Y)\nprint('MCC:',mcc)\n                       \nmaria_LDA = pd.DataFrame(output)  #here we are converting the array into a pandas dataframe which will give a single column\nmaria_LDA.columns = ['CLASS'] #renaming the output column to 'CLASS'\nmaria_LDA.index.name = \"Index\"  #naming the index column as 'Index'\nmaria_LDA['CLASS']=maria_LDA['CLASS'].map({0.0:False, 1.0:True}) #converting '0.0' to' False' and '1.0' to 'True'\nmaria_LDA.to_csv(\"maria_LDA.csv\") #changing my output file as a 'csv' file\n\nprint(maria_LDA['CLASS'].unique())  #checking out the unique instances in the 'CLASS' column\nprint('False: ',maria_LDA.groupby('CLASS').size()[0].sum()) #summing up the '0' instances in the 'CLASS' column\nprint('True: ',maria_LDA.groupby('CLASS').size()[1].sum()) #summing up the '1' instances in the 'CLASS' column","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# NON LINEAR ALGORITHMS\n1. Naive Bayes\n2. Support Vector Machines\n3. K-Nearest Neighbours\n4. Classification and Regression Trees\n5. Learning Vector Quantization"},{"metadata":{},"cell_type":"markdown","source":"# Using Naive Bayes and all the features"},{"metadata":{"trusted":true},"cell_type":"code","source":"array = Train.values\nX = array[:,0:11]\nY = array[:,11]\ntest_size = 0.35 \n\nkfold = KFold(n_splits=10) #spliiting my data into 10 folds \n\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size, random_state=42) #random_state of 42 for reproducibility\n\nmodel = GaussianNB() #calling out the prediction algorithm\nresults = cross_val_score(model, X, Y, cv=kfold) #estimating the model on new data and assigning it to results variable\nprint(results.mean())  #getting the mean of the accuracy scores from cross validation scores\n\n\nmodel.fit(X,Y)\noutput = model.predict(Test.values) #testing the model on the test_set (Test.values)\n\nfrom sklearn.metrics import matthews_corrcoef\nmcc = matthews_corrcoef(model.predict(X),Y)\nprint('MCC:',mcc)\n                       \nmaria2_bayes = pd.DataFrame(output)  #here we are converting the array into a pandas dataframe which will give a single column\nmaria2_bayes.columns = ['CLASS'] #renaming the output column to 'CLASS'\nmaria2_bayes.index.name = \"Index\"  #naming the index column as 'Index'\nmaria2_bayes['CLASS']=maria2_bayes['CLASS'].map({0.0:False, 1.0:True}) #converting '0.0' to' False' and '1.0' to 'True'\nmaria2_bayes.to_csv(\"maria2_bayes.csv\") #changing my output file as a 'csv' file\n\n\nprint(maria2_bayes['CLASS'].unique()) #checking out the unique instances in the 'CLASS' column\nprint('False: ',maria2_bayes.groupby('CLASS').size()[0].sum()) #summing up the '0' instances in the 'CLASS' column\nprint('True: ',maria2_bayes.groupby('CLASS').size()[1].sum()) #summing up the '1' instances in the 'CLASS' column","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Support Vector Machines algorithm using all features"},{"metadata":{"trusted":true},"cell_type":"code","source":"array = Train.values\nX = array[:,0:11]\nY = array[:,11]\nkfold = KFold(n_splits=10) #spliiting my data into 10 folds\n\nmodel = SVC() #calling out the prediction algorithm\nscoring = 'acuracy'\nresults = cross_val_score(model, X, Y, cv=kfold) #estimating the model on new data and assigning it to results variable\nprint(results.mean())  #getting the mean of the accuracy scores from cross validation scores\n\nmodel.fit(X, Y)\noutput = model.predict(Test.values) #testing the model on the test_set (Test.values)\n\nmcc = matthews_corrcoef(model.predict(X), Y)\nprint('MCC: ',mcc)\n\nmaria_svc = pd.DataFrame(output)  #here we are converting the array into a pandas dataframe which will give a single column\nmaria_svc.columns = ['CLASS'] #renaming the output column to 'CLASS'\nmaria_svc.index.name = 'Index'  #naming the index column as 'Index'\nmaria_svc['CLASS'] = maria_svc['CLASS'].map({0.0:False, 1.0:True}) #converting '0.0' to' False' and '1.0' to 'True'\n\nmaria_svc.to_csv('maria_svc.csv') #changing my output file as a 'csv' file\n\nprint(maria_svc['CLASS'].unique()) #checking out the unique instances in the 'CLASS' column\nprint('False: ',maria_svc.groupby('CLASS').size()[0].sum()) #summing up the '0' instances in the 'CLASS' column\nprint('True: ',maria_svc.groupby('CLASS').size()[1].sum()) #summing up the '1' instances in the 'CLASS' column","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Applying Classification and Regression Trees"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\narray = Train.values\nX = array[:,0:11]\nY = array[:,11]\nkfold = KFold(n_splits=10, random_state=42) #spliiting my data into 10 folds and random_state of 42 for reproducibility\nmodel = DecisionTreeClassifier() #calling out the prediction algorithm\nresults = cross_val_score(model, X, Y, cv=kfold) #estimating the model on new data and assigning it to results variable\nprint(results.mean())  #getting the mean of the accuracy scores from cross validation scores\n\n\nmodel.fit(X,Y)\noutput = model.predict(Test.values) #testing the model on the test_set (Test.values)\n\nfrom sklearn.metrics import matthews_corrcoef\nmcc = matthews_corrcoef(model.predict(X),Y)\nprint('MCC:',mcc)\n                       \nmaria_tree = pd.DataFrame(output)  #here we are converting the array into a pandas dataframe which will give a single column\nmaria_tree.columns = ['CLASS'] #renaming the output column to 'CLASS'\nmaria_tree.index.name = \"Index\"  #naming the index column as 'Index'\nmaria_tree['CLASS']=maria_tree['CLASS'].map({0.0:False, 1.0:True}) #converting '0.0' to' False' and '1.0' to 'True'\nmaria_tree.to_csv(\"maria_tree.csv\") #changing my output file as a 'csv' file\n\n\nprint(maria_tree['CLASS'].unique()) #checking out the unique instances in the 'CLASS' column\nprint('False: ',maria_tree.groupby('CLASS').size()[0].sum()) #summing up the '0' instances in the 'CLASS' column\nprint('True: ',maria_tree.groupby('CLASS').size()[1].sum()) #summing up the '1' instances in the 'CLASS' column","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# K-Nearest Neighbours"},{"metadata":{"trusted":true},"cell_type":"code","source":"array = Train.values\nX = array[:,0:11]\nY = array[:,11]\n\nkfold = KFold(n_splits=10, random_state=42) #spliiting my data into 10 folds and random_state of 42 for reproducibility\nmodel = KNeighborsClassifier() #calling out the prediction algorithm\nresults = cross_val_score(model, X, Y, cv=kfold) #estimating the model on new data and assigning it to results variable\nprint(results.mean()) #getting the mean of the accuracy scores from cross validation scores\n\nmodel.fit(X,Y) \noutput = model.predict(Test.values) #testing the model on the test_set (Test.values)\n\nfrom sklearn.metrics import matthews_corrcoef\nmcc = matthews_corrcoef(model.predict(X),Y)\nprint('MCC:',mcc)\n                       \nmaria_knn = pd.DataFrame(output) #here we are converting the array into a pandas dataframe which will give a single column\nmaria_knn.columns = ['CLASS'] #renaming the output column to 'CLASS' \nmaria_knn.index.name = \"Index\" #naming the index column as 'Index'\nmaria_knn['CLASS']=maria_knn['CLASS'].map({0.0:False, 1.0:True}) #converting '0.0' to' False' and '1.0' to 'True'\nmaria_knn.to_csv(\"maria_knn.csv\") #changing my output file as a 'csv' file\n\n\nprint(maria_knn['CLASS'].unique()) #checking out the unique instances in the 'CLASS' column\nprint('False: ',maria_knn.groupby('CLASS').size()[0].sum()) #summing up the '0' instances in the 'CLASS' column\nprint('True: ',maria_knn.groupby('CLASS').size()[1].sum()) #summing up the '1' instances in the 'CLASS' column","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Esemble Algorithms\n1. Boosting and Adaboost\n2. Bagging and Random forest"},{"metadata":{},"cell_type":"markdown","source":"# Applying Adaboost "},{"metadata":{"trusted":true},"cell_type":"code","source":"array = Train.values\n\nX = array[:,0:11]\nY = array[:,11]\n\nkfold = KFold(n_splits=10, random_state=42)\n\nmodel = AdaBoostClassifier(n_estimators=200, random_state=42) \nresults = cross_val_score(model, X, Y, cv=kfold)\n\nprint(results.mean())\n\n\ntest_set = Test.values\nmodel.fit(X, Y)\noutput = model.predict(test_set)\n\nmcc = matthews_corrcoef(model.predict(X), Y)\nprint('MCC: ',mcc)\n\nmaria_AB= pd.DataFrame(output)\nmaria_AB.columns = ['CLASS']\nmaria_AB.index.name = 'Index'\nmaria_AB['CLASS'] = maria_AB['CLASS'].map({0.0:False, 1.0:True})\n\nmaria_AB.to_csv('maria_AB.csv')\n\nprint(maria_AB['CLASS'].unique())\nprint('False: ',maria_AB.groupby('CLASS').size()[0].sum())\nprint('True: ',maria_AB.groupby('CLASS').size()[1].sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Applying Gradient Boosting Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"array = Train.values\n\nX = array[:,0:11]\nY = array[:,11]\n\n#num_trees = 200\n#seed =42\n\nkfold = KFold(n_splits=10, random_state=42)\nmodel = GradientBoostingClassifier(n_estimators=200, random_state=42)\nresults = cross_val_score(model, X, Y, cv=kfold)\nprint(results.mean())\n\nmodel.fit(X,Y) \noutput = model.predict(Test.values) #testing the model on the test_set (Test.values)\n\nfrom sklearn.metrics import matthews_corrcoef\nmcc = matthews_corrcoef(model.predict(X),Y)\nprint('MCC:',mcc)\n                       \nmaria_GBC = pd.DataFrame(output) #here we are converting the array into a pandas dataframe which will give a single column\nmaria_GBC.columns = ['CLASS'] #renaming the output column to 'CLASS' \nmaria_GBC.index.name = \"Index\" #naming the index column as 'Index'\nmaria_GBC['CLASS']=maria_GBC['CLASS'].map({0.0:False, 1.0:True}) #converting '0.0' to' False' and '1.0' to 'True'\nmaria_GBC.to_csv(\"maria_GBC.csv\") #changing my output file as a 'csv' file\n\n\nprint(maria_GBC['CLASS'].unique()) #checking out the unique instances in the \nprint('False: ',maria_GBC.groupby('CLASS').size()[0].sum()) #summing up the '0' instances in the 'CLASS' column\nprint('True: ',maria_GBC.groupby('CLASS').size()[1].sum()) #summing up the '1' instances in the 'CLASS' column","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Using Stochastic Gradient Boosting Classification"},{"metadata":{"trusted":true},"cell_type":"code","source":"array = Train.values\nX = array[:,0:11]\nY = array[:,11]\n\nkfold = KFold(n_splits=10, random_state=42)\nmodel = XGBClassifier(n_estimators=200, random_state=42)\nresults = cross_val_score(model, X, Y, cv=kfold)\nprint(results.mean())\n\nmodel.fit(X,Y) \noutput = model.predict(Test.values) #testing the model on the test_set (Test.values)\n\nfrom sklearn.metrics import matthews_corrcoef\nmcc = matthews_corrcoef(model.predict(X),Y)\nprint('MCC:',mcc)\n                       \nmaria_XGB = pd.DataFrame(output) #here we are converting the array into a pandas dataframe which will give a single column\nmaria_XGB.columns = ['CLASS'] #renaming the output column to 'CLASS' \nmaria_XGB.index.name = \"Index\" #naming the index column as 'Index'\nmaria_XGB['CLASS']=maria_XGB['CLASS'].map({0.0:False, 1.0:True}) #converting '0.0' to' False' and '1.0' to 'True'\nmaria_XGB.to_csv(\"maria_XGB.csv\") #changing my output file as a 'csv' file\n\n\nprint(maria_GBC['CLASS'].unique()) #checking out the unique instances in the \nprint('False: ',maria_XGB.groupby('CLASS').size()[0].sum()) #summing up the '0' instances in the 'CLASS' column\nprint('True: ',maria_XGB.groupby('CLASS').size()[1].sum()) #summing up the '1' instances in the 'CLASS' column","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Using Extra Trees Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"array = Train.values\n\nX = array[:,0:11]\nY = array[:,11]\n\nkfold = KFold(n_splits=10, random_state=42)\nmodel = ExtraTreesClassifier(n_estimators=200) # (max_features=11)\nresults = cross_val_score(model, X, Y, cv=kfold)\nprint(results.mean())\n                             \nmodel.fit(X,Y) \noutput = model.predict(Test.values) #testing the model on the test_set (Test.values)\n\nfrom sklearn.metrics import matthews_corrcoef\nmcc = matthews_corrcoef(model.predict(X),Y)\nprint('MCC:',mcc)\n                       \nmaria_ETX = pd.DataFrame(output) #here we are converting the array into a pandas dataframe which will give a single column\nmaria_ETX.columns = ['CLASS'] #renaming the output column to 'CLASS' \nmaria_ETX.index.name = \"Index\" #naming the index column as 'Index'\nmaria_ETX['CLASS']=maria_ETX['CLASS'].map({0.0:False, 1.0:True}) #converting '0.0' to' False' and '1.0' to 'True'\nmaria_ETX.to_csv(\"maria_ETX.csv\") #changing my output file as a 'csv' file\n\n\nprint(maria_ETX['CLASS'].unique()) #checking out the unique instances in the \nprint('False: ',maria_ETX.groupby('CLASS').size()[0].sum()) #summing up the '0' instances in the 'CLASS' column\nprint('True: ',maria_ETX.groupby('CLASS').size()[1].sum()) #summing up the '1' instances in the 'CLASS' column                           ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# CONCLUSION\n## From the algorithm comparisons and my prediction scores, Naive Bayes algorithm performed the best.\n## I think this is because the data was following a Gaussian distribution(normally distributed) as seen earlier from the density subplots."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}