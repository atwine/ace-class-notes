{"cells":[{"metadata":{},"cell_type":"markdown","source":"# **BIG BIO-DATA ANALYSIS - PRACTICE : 1**\n### **SABAKAKI PETER ZIRIBAGWA** (MSC. BIOINFORMATICS)"},{"metadata":{},"cell_type":"markdown","source":"## 1. Importing the neccessary libraries"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"## 2. Loading the datasets"},{"metadata":{"_cell_guid":"","_uuid":"","trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"../input/amp-data-set/AMP_TrainSet.csv\") # this is the trainning dataset\ntest = pd.read_csv(\"../input/amp-data-set/Test.csv\") # this is the testing dataset","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 3. Checking dataset content\n- This helps to give a quick insight about the content of the datasete. i.e it answers the question, \"What is within the datasets?\""},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head() # to see what is in the first 5 rows of the train dataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head() # to see what is in the first 5 rows of the test data set","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **4.Checking the dimensions of the dataset**\n- it is important to know the number of columns and rows with in the dataset, because to big data set may take so long to train whereas too small dataset may reduce the performance of the model.\n\n- to know which program to use when loading the data,forexample, pandas can't be used to load data with a big dimension"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape # to cheak the dimension i.e the number of rows and columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"`The above results show that there are 3038 rows and 12 columns.`"},{"metadata":{},"cell_type":"markdown","source":"## **5. Checking column names of the train dataset**"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.columns  #checking the column names with in the train dataset \n#This gives an insight on the features available in the dataset\n ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **6.Checking the data type of each feature / column**\n- This is important because some algorithms prefer certain data types to others. so it is good for me to first ensure the data type of my features is appropriate for the algorithm am intending to use, and if not, i should then convert the data types to the suitable ones."},{"metadata":{"trusted":true},"cell_type":"code","source":"train.dtypes # cheaking datatypes in the train dataset","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 7. Descriptive Statistics \n- This is important to understand and draw conclusions about the data in each column of the dataset.\n- It enable one to describe the size, center and spread of the data in each column; i.e the count, mean, max, min, standard deviations and the percentage quartiles."},{"metadata":{"trusted":true},"cell_type":"code","source":"train.describe() # to output the statistics of each attribute in the entire dataset","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 8. Checking the distribution of the class\n- It is important to know how balanced the frequency of the outcome categories is !. This is because imbalanced frequncies of the outcome categories may cause bias in the performance of the final model."},{"metadata":{"trusted":true},"cell_type":"code","source":"train.groupby('CLASS').size().plot(kind='bar') #a bar-graph is used to visualize the distribution of the values in the class atribute","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" `From the above bar graph, i deduce that the \"class attribute\" is evenly distributed;` \n `hence, there is no need to use methods such as smote to balance the class attribute`"},{"metadata":{},"cell_type":"markdown","source":"## **9. Determining the corrlation between the all the atributes across the entire dataset**"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.corr(method='pearson') # do correlation using pearson correlation coeficient","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(6,6))\nsns.heatmap(train.corr(method='pearson')) # constract a heatmap to visually display the pearson correlation between attributes across the entire train databse","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## **10.Checking the skewness of the data**\n- it is important to know the skewness of the data, because some algorithm perform better with gaussian data"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.skew() # compute the level of skewness of each attribute in the train dataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.skew().plot(kind='bar') # visualise the level of skewness of each attribute in the train dataset","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"`The graph above shows that the \" NT_EFC195\" attribute is highly posiively skewed. i may consider transforming\nit in future`"},{"metadata":{},"cell_type":"markdown","source":"## 11. Univariate data visualisation \n- it is importart to observe the distribution of each attribute independently\n- it also shades a picture of the data skewness. i.e how are values spread in the entire attribute and how many are distributed around the center."},{"metadata":{"trusted":true},"cell_type":"code","source":"### Generate Histograms;\nplt.figure(figsize=(15,15)) # sets the dimmesions of the plots.\ntrain.hist() # histogram of each attribute in the train dataset\nplt.show() # it outputs the plots","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Density plots\ntrain.plot(kind='density', subplots=True, layout=(4,3), sharex=False)\nplt.show","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Box and whisker plots\n# This gives a visual of the median, the range between 25% and 75% quartiles, and the outliers on the whiskers \ntrain.plot(kind='box', subplots=True, layout=(4,3), sharex=False, sharey=False)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **12. Multivariate data visualization**\n- Differing from univariate,this shows how each attribute is correlated with other attributes in the dataset"},{"metadata":{},"cell_type":"markdown","source":"### Correlation matrix "},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot a correlation matrix across the train dataset.\n# this displays a heatmap that shows how each attribute is correlated with the other in the dataset\ncorrelations = train.corr()\nfig = plt.figure()\nax = fig.add_subplot(111)\ncax = ax.matshow(correlations, vmin=-1, vmax=1)\nfig.colorbar(cax)\nticks = np.arange(0,9,1)\nax.set_xticks(ticks)\nax.set_yticks(ticks)\nax.set_xticklabels(train.columns)\nax.set_yticklabels(train.columns)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Scatter plot matrix\n- This shows a relationship between two attributes in the dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"## sns.pairplot(train,) # draw scatterplot using pair plot\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **13. Data preparation**"},{"metadata":{},"cell_type":"markdown","source":"### Standardizing the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Standidizing the data.\nfrom sklearn.preprocessing import StandardScaler\narray = train.values\n# Separate the array into input and output componets\nx = array[:,0:11]\ny = array[:,11]\nscaler = StandardScaler().fit(x)\nrescalledx = scaler.transform(x)\n# summurized transformed data\nnp.set_printoptions(precision=3)\nprint(rescalledx[0:5,:])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Rescaling the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Rescaling the data.\nfrom numpy import set_printoptions\nfrom sklearn.preprocessing import MinMaxScaler\n\narray = train.values\n# separate array into input and output components\nX = array[:,0:11]\nY = array[:,11]\nscaler = MinMaxScaler(feature_range=(0, 1))\nrescalledX = scaler.fit_transform(X)\n# summarize transformed data\nset_printoptions(precision=3)\nprint(rescalledX[0:5,:])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature selection"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Recursive feature elimination\n\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LogisticRegression\n\narray = train.values\nX = rescalledx[:,0:11]\nY = array[:,11]\n# feature extraction\nmodel = LogisticRegression()\nrfe = RFE(model, 5)\nfit = rfe.fit(X, Y)\nprint(\"Num Features: \",  fit.n_features_)\nprint(\"Selected Features:\",  fit.support_)\nprint(\"Feature Ranking: \",  fit.ranking_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## Comparing mutiple classification algorithms"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compare Algorithms\nfrom pandas import read_csv\nfrom matplotlib import pyplot\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\n# load dataset\n\n#split the dataset \nX = rescalledX[:,0:11]\nY = array[:,11]\n\n# prepare models and add them to a list\nmodels = []\nmodels.append(('LR', LogisticRegression()))\nmodels.append(('LDA', LinearDiscriminantAnalysis()))\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('CART', DecisionTreeClassifier()))\nmodels.append(('NB', GaussianNB()))\nmodels.append(('SVM', SVC()))\n\n# evaluate each model in turn\nresults = []\nnames = []\nscoring = 'accuracy'\n\nfor name, model in models:\n    kfold = KFold(n_splits=10)\n    cv_results = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n    results.append(cv_results)\n    names.append(name)\n    msg = (name, cv_results.mean(), cv_results.std())\n    print(msg)\n\n# boxplot algorithm comparison\nfig = pyplot.figure()\nfig.suptitle('Algorithm Comparison')\nax = fig.add_subplot(111)\npyplot.boxplot(results)\nax.set_xticklabels(names)\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"`the above comparision shows that naive bayes gives the best performance,therefore,  i am going to continue with naive bayes`"},{"metadata":{},"cell_type":"markdown","source":"## Rescaling the test_dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Rescaling the test_data.\nfrom numpy import set_printoptions\nfrom sklearn.preprocessing import MinMaxScaler\n\narray = test.values\n# separate array into input and output components\ny = array[:,0:11]\nscaler = MinMaxScaler(feature_range=(0, 1))\nrescalled_test = scaler.fit_transform(y)\n# summarize transformed data\nset_printoptions(precision=3)\nprint(rescalled_test[0:5,:])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Genenerating the model using Naive_Bayes"},{"metadata":{"trusted":true},"cell_type":"code","source":"# using naive_bayes\n\nfrom sklearn.naive_bayes import GaussianNB\narray = train.values\nX = rescalledX[:,0:11]\nY = array[:,11]\nkfold = KFold(n_splits=10)\nmodel = GaussianNB()  # Using naive bayes\n\n# fitting the model\nmodel.fit(X,Y)\n\n# predicting the test_dataset using the model\nClass= model.predict(rescalled_test)\n\n#Returning the Naive Bayes output in a dataframe\nreport = pd.DataFrame(Class)\n\nreport.columns=['CLASS'] # Creating a class column\nreport.index.name='Index' #Creating a culumn index\n\n# Map function to change the 0.0 and 1.0 into False and True repectively\nreport['CLASS']= report['CLASS'].map({0.0:False, 1.0:True})\nreport\n\n#Storing the dataframe output in a csv file.\nreport.to_csv(\"ziribagwa.csv\")\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}