{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "        \n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "#### https://machinelearningmastery.com/evaluate-performance-machine-learning-algorithms-python-using-resampling/\n",
    "#### https://www.dataquest.io/blog/top-10-machine-learning-algorithms-for-beginners/\n",
    "#### https://monkeylearn.com/blog/introduction-to-support-vector-machines-svm/\n",
    "#### https://towardsdatascience.com/understanding-random-forest-58381e0602d2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"../input/ace-class-assignment/Test.csv\")\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in the data\n",
    "data = pd.read_csv(\"../input/ace-class-assignment/AMP_TrainSet.csv\")\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze data by describing\n",
    "\n",
    "#### This step helped me know which features are in my dataset, are they categorical or numerical.\n",
    "#### How many rows and columns does the dataset have\n",
    "#### The data types for the various features\n",
    "#### Checked whether the dataset has null or missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the dimensions to the number of rows and columns\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate descriptive statistics that summarize the central tendency, dispersion, and shape of a dataset’s distribution, excluding NaN values\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of null values in each column\n",
    "data.isnull().sum()\n",
    "#since my data has no null values then its good to go"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### needed to know how balanced the class values are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data.groupby('CLASS').size().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Its a good idea to review all the pairwise correlations of the attributes in the dataset because some machine learning algorithm like linear and logistic regression can suffer poor performance if there are highly correlated attributes in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.corr(method='pearson')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  heat map to show the correlation of the data; plots that show the interactions between multiple variables in the dataset\n",
    "#### Correlation gives an indication of how related the changes are between two variables. If two variables change in the same direction they are positively correlated. If they change in opposite directions together (one goes up, one goes down), then they are negatively correlated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "sns.heatmap(data.corr(method='pearson'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### also checked the corelation in regards to the class since am trying to build a ML agorithm for that class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data.corr(method='pearson')['CLASS']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Most of my variables are positively skewed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " data.skew().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## understanding data with visualization\n",
    "#### Data can be visualised in many ways that is univariate plots and multivariate plots             #### Used the Histogram for univariate plot as shown below and the correlation matrix plot as the multivariate plot as shown above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Histogram\n",
    "#### This helps to understand each attribute of my dataset independently"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,18))\n",
    "data.hist()\n",
    "plt.subplots_adjust(bottom=3, right=2, top=5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardize data\n",
    "#### Standardization is a useful technique to transform attributes with a Gaussian distribution and differing means and standard deviations to a standard Gaussian distribution with a mean of 0 and a standard deviation of 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "array = data.values\n",
    "#separate array into input and output components\n",
    "X = array[:,0:11]\n",
    "Y = array[:,11]\n",
    "scaler = StandardScaler().fit(X)\n",
    "rescaledX = scaler.transform(X)\n",
    "# summarize transformed data\n",
    "#set_printoptions(precision=3)\n",
    "print(rescaledX[0:5,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array = test.values\n",
    "scaler = StandardScaler().fit(array)\n",
    "rescaledt = scaler.transform(array)\n",
    "# summarize transformed data\n",
    "#set_printoptions(precision=3)\n",
    "print(rescaledt[0:5,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Feature selection\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  it's the process of selecting a subset of relevant features for use in model construction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chose Recursive Feature Elimination\n",
    "#### This is an automatic feature selection technique\n",
    "#### Used logistic regression it is a good baseline as it is fast to train and predict and scales well.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "array = data.values\n",
    "X = array[:,0:11]\n",
    "Y = array[:,11]\n",
    "# feature extraction\n",
    "model = LogisticRegression()\n",
    "rfe = RFE(model,8)\n",
    "fit = rfe.fit(X,Y)\n",
    "print(\"Num Features:\", fit.n_features_)\n",
    "print(\"Selected Features:\", fit.support_)\n",
    "print(\"Feature Ranking:\", fit.ranking_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[:,fit.support_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop=data.drop(['FULL_AcidicMolPerc', 'FULL_DAYM780201', 'AS_DAYM780201'],axis=1)\n",
    "drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_test = test.drop(['FULL_AcidicMolPerc', 'FULL_DAYM780201', 'AS_DAYM780201'],axis=1)\n",
    "drop_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. #### Decided to first use all the  first\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the Performance of Machine Learning Algorithms with Resampling¶\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The best way to evaluate the performance of an algorithm would be to make predictions for new data to which you already know the answers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into Train and Test Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This algorithm evaluation technique is very fast. It is ideal for large datasets where there is strong evidence that both splits of the data are representative of the underlying problem. Because of the speed, it is useful to use this approach when the algorithm you are investigating is slow to train.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "array = data.values\n",
    "X = array[:,0:11]\n",
    "Y = array[:,11]\n",
    "test_size = 0.30\n",
    "seed = 7\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size,\n",
    "random_state=seed)\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, Y_train)\n",
    "result = model.score(X_test, Y_test)\n",
    "print(\"Accuracy: \",  (result*100.0))\n",
    "\n",
    "\n",
    "model.fit(X,Y)\n",
    "output = model.predict(test.values)\n",
    "\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "mcc = matthews_corrcoef(model.predict(X),Y)\n",
    "print('MCC:',mcc)\n",
    "                       \n",
    "report = pd.DataFrame(output)\n",
    "report.columns = ['CLASS']\n",
    "report.index.name = \"Index\"\n",
    "report['CLASS']=report['CLASS'].map({0.0:False, 1.0:True})\n",
    "report.to_csv(\"report.csv\")\n",
    "\n",
    "print(report['CLASS'].unique())\n",
    "print('False: ',report.groupby('CLASS').size()[0].sum())\n",
    "print('True: ',report.groupby('CLASS').size()[1].sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-fold Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### It is more accurate because the algorithm is trained and evaluated multiple times on different data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "num_folds = 10 #number of folds to use\n",
    "seed = 7 #reproducibility\n",
    "\n",
    "kfold = KFold(n_splits=num_folds, random_state=seed)\n",
    "model = LogisticRegression()\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "\n",
    "print(f\"Accuracy:\", (results.mean()*100.0, results.std()*100.0))\n",
    "\n",
    "\n",
    "model.fit(X,Y)\n",
    "output = model.predict(test.values)\n",
    "\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "mcc = matthews_corrcoef(model.predict(X),Y)\n",
    "print('MCC:',mcc)\n",
    "                       \n",
    "report_kf = pd.DataFrame(output)\n",
    "report_kf.columns = ['CLASS']\n",
    "report_kf.index.name = \"Index\"\n",
    "report_kf['CLASS']=report_kf['CLASS'].map({0.0:False, 1.0:True})\n",
    "report_kf.to_csv(\"report_kf.csv\")\n",
    "\n",
    "print(report_kf['CLASS'].unique())\n",
    "print('False: ',report_kf.groupby('CLASS').size()[0].sum())\n",
    "print('True: ',report_kf.groupby('CLASS').size()[1].sum())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leave One Out Cross Validation\n",
    "#### Its a special case of cross validation where the number of folds equals the number of  instances in the  data set thus the learning algorithm is applied once for each instance, using all other instances as a  training set and using the selected instance as a single-item  test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "num_folds = 10\n",
    "loocv = LeaveOneOut()\n",
    "model = LogisticRegression()\n",
    "results = cross_val_score(model, X, Y, cv=loocv)\n",
    "print(\"Accuracy:\",  (results.mean()*100.0, results.std()*100.0))\n",
    "\n",
    "\n",
    "model.fit(X,Y)\n",
    "output = model.predict(test.values)\n",
    "\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "mcc = matthews_corrcoef(model.predict(X),Y)\n",
    "print('MCC:',mcc)\n",
    "                       \n",
    "report_l = pd.DataFrame(output)\n",
    "report_l.columns = ['CLASS']\n",
    "report_l.index.name = \"Index\"\n",
    "report_l['CLASS']=report_l['CLASS'].map({0.0:False, 1.0:True})\n",
    "report_l.to_csv(\"report_l.csv\")\n",
    "\n",
    "print(report_l['CLASS'].unique())\n",
    "print('False: ',report_l.groupby('CLASS').size()[0].sum())\n",
    "print('True: ',report_l.groupby('CLASS').size()[1].sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repeated Random Test-Train Splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creates a random split of the data like the train/test split , but repeats the process of splitting and evaluation of the algorithm multiple times, like cross validation. Repeated random splits can be useful intermediates when trying to balance variance in the estimated performance, model training speed and dataset size\n",
    "#### In this I prefered using Repeated Random Test_Train Splits because when you look at the dataset the zeros are one side and the ones on the otherside in the 'class' column. So I would prefer to first shuffle the data and then split it to reduce on the bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "n_splits = 10\n",
    "test_size = 0.30\n",
    "seed = 7\n",
    "kfold = ShuffleSplit(n_splits=n_splits, test_size=test_size, random_state=seed)\n",
    "model = LogisticRegression()\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(\"Accuracy: \" , (results.mean()*100.0, results.std()*100.0))\n",
    "\n",
    "\n",
    "model.fit(X,Y)\n",
    "output = model.predict(test.values)\n",
    "\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "mcc = matthews_corrcoef(model.predict(X),Y)\n",
    "print('MCC:',mcc)\n",
    "                       \n",
    "report_rrt = pd.DataFrame(output)\n",
    "report_rrt.columns = ['CLASS']\n",
    "report_rrt.index.name = \"Index\"\n",
    "report_rrt['CLASS']=report_rrt['CLASS'].map({0.0:False, 1.0:True})\n",
    "report_rrt.to_csv(\"report_rrt.csv\")\n",
    "\n",
    "print(report_rrt['CLASS'].unique())\n",
    "print('False: ',report_rrt.groupby('CLASS').size()[0].sum())\n",
    "print('True: ',report_rrt.groupby('CLASS').size()[1].sum())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Algorithm Performance Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithms Overview\n",
    "### linear machine learning algorithms:\n",
    "\n",
    "    Logistic Regression.\n",
    "    Linear Discriminant Analysis.\n",
    "### onlinear machine learning algorithms\n",
    "\n",
    "    k-Nearest Neighbors.\n",
    "    Naive Bayes.\n",
    "    Classication and Regression Trees.\n",
    "    Support Vector Machines.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Machine Learning Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "#### It's the appropriate regression analysis to conduct when the dependent variable is binary. So tried to use it on my data since it is binary and has no outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using standardized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression on standardized data\n",
    "num_folds = 10\n",
    "kfold = KFold(n_splits=10, random_state=7)\n",
    "model = LogisticRegression()\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())\n",
    "\n",
    "model.fit(rescaledX,Y)\n",
    "output = model.predict(rescaledt)\n",
    "\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "mcc = matthews_corrcoef(model.predict(X),Y)\n",
    "print('MCC:',mcc)\n",
    "                       \n",
    "report_scaled = pd.DataFrame(output)\n",
    "report_scaled.columns = ['CLASS']\n",
    "report_scaled.index.name = \"Index\"\n",
    "report_scaled['CLASS']=report_scaled['CLASS'].map({0.0:False, 1.0:True})\n",
    "report_scaled.to_csv(\"report_scaled.csv\")\n",
    "\n",
    "print(report_scaled['CLASS'].unique())\n",
    "print('False: ',report_scaled.groupby('CLASS').size()[0].sum())\n",
    "print('True: ',report_scaled.groupby('CLASS').size()[1].sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression Classification on untuched data\n",
    "\n",
    "num_folds = 10\n",
    "kfold = KFold(n_splits=10, random_state=7)\n",
    "model = LogisticRegression()\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())\n",
    "\n",
    "model.fit(X,Y)\n",
    "output = model.predict(test.values)\n",
    "\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "mcc = matthews_corrcoef(model.predict(X),Y)\n",
    "print('MCC:',mcc)\n",
    "                       \n",
    "my_report = pd.DataFrame(output)\n",
    "my_report.columns = ['CLASS']\n",
    "my_report.index.name = \"Index\"\n",
    "my_report['CLASS']=my_report['CLASS'].map({0.0:False, 1.0:True})\n",
    "my_report.to_csv(\"report_XGB.csv\")\n",
    "\n",
    "print(my_report['CLASS'].unique())\n",
    "print('False: ',my_report.groupby('CLASS').size()[0].sum())\n",
    "print('True: ',my_report.groupby('CLASS').size()[1].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Discriminant Analysis¶\n",
    "#### Linear Discriminant Analysis(LDA) is a very common technique used for supervised classification problems. It reduces the dimensions by removing the reduntant and dependent features by transforming the features from higher dimensional space to a space with lower dimensions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "num_folds = 10\n",
    "kfold = KFold(n_splits=10, random_state=7)\n",
    "model = LinearDiscriminantAnalysis()\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())\n",
    "\n",
    "\n",
    "model.fit(X,Y)\n",
    "output = model.predict(test.values)\n",
    "\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "mcc = matthews_corrcoef(model.predict(X),Y)\n",
    "print('MCC:',mcc)\n",
    "                       \n",
    "lda_report = pd.DataFrame(output)\n",
    "lda_report.columns = ['CLASS']\n",
    "lda_report.index.name = \"Index\"\n",
    "lda_report['CLASS']=lda_report['CLASS'].map({0.0:False, 1.0:True})\n",
    "lda_report.to_csv(\"ldareport.csv\")\n",
    "\n",
    "print(lda_report['CLASS'].unique())\n",
    "print('False: ',lda_report.groupby('CLASS').size()[0].sum())\n",
    "print('True: ',lda_report.groupby('CLASS').size()[1].sum())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nonlinear Machine Learning Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-Nearest Neighbors\n",
    "#### Can solve both classification and regression problems. However, it is more widely used in classification problems so decided to try it and it gave me a very low score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "num_folds = 10\n",
    "kfold = KFold(n_splits=10, random_state=7)\n",
    "model = KNeighborsClassifier()\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())\n",
    "\n",
    "\n",
    "\n",
    "model.fit(X,Y)\n",
    "output = model.predict(test.values)\n",
    "\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "mcc = matthews_corrcoef(model.predict(X),Y)\n",
    "print('MCC:',mcc)\n",
    "                       \n",
    "report_k = pd.DataFrame(output)\n",
    "report_k.columns = ['CLASS']\n",
    "report_k.index.name = \"Index\"\n",
    "report_k['CLASS']=report_k['CLASS'].map({0.0:False, 1.0:True})\n",
    "report_k.to_csv(\"report_k.csv\")\n",
    "\n",
    "\n",
    "print(report_k['CLASS'].unique())\n",
    "print('False: ',report_k.groupby('CLASS').size()[0].sum())\n",
    "print('True: ',report_k.groupby('CLASS').size()[1].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tried using Standardised data on Naive Bayes\n",
    "\n",
    "### When I predicted Naive Bayes on Standardised data gave me a score of 0.98235, after feature selection it gave 0.90 and on unstandardised data it gave a score of 0.9959"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes on standardised data\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "kfold = KFold(n_splits=10, random_state=7)\n",
    "model = GaussianNB()\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())\n",
    "\n",
    "\n",
    "model.fit(rescaledX,Y)\n",
    "output = model.predict(rescaledt)\n",
    "\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "mcc = matthews_corrcoef(model.predict(X),Y)\n",
    "print('MCC:',mcc)\n",
    "                       \n",
    "report_rebayes = pd.DataFrame(output)\n",
    "report_rebayes.columns = ['CLASS']\n",
    "report_rebayes.index.name = \"Index\"\n",
    "report_rebayes['CLASS']=report_rebayes['CLASS'].map({0.0:False, 1.0:True})\n",
    "report_rebayes.to_csv(\"report_rebayes.csv\")\n",
    "\n",
    "\n",
    "print(report_rebayes['CLASS'].unique())\n",
    "print('False: ',report_rebayes.groupby('CLASS').size()[0].sum())\n",
    "print('True: ',report_rebayes.groupby('CLASS').size()[1].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes on selected features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes on selected features\n",
    "\n",
    "array = data.values\n",
    "X = array[:,0:11]\n",
    "Y = array[:,11]\n",
    "\n",
    "selectedX = X[:,fit.support_]\n",
    "\n",
    "array2 =test.values\n",
    "selectedT = array2[:,fit.support_]\n",
    "\n",
    "kfold = KFold(n_splits=10, random_state=7)\n",
    "model = GaussianNB()\n",
    "results = cross_val_score(model, selectedX, Y, cv=kfold)\n",
    "print(results.mean())\n",
    "\n",
    "\n",
    "model.fit(selectedX,Y)\n",
    "output = model.predict(selectedT)\n",
    "\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "mcc = matthews_corrcoef(model.predict(selectedX),Y)\n",
    "print('MCC:',mcc)\n",
    "                       \n",
    "report_sel = pd.DataFrame(output)\n",
    "report_sel.columns = ['CLASS']\n",
    "report_sel.index.name = \"Index\"\n",
    "report_sel['CLASS']=report_sel['CLASS'].map({0.0:False, 1.0:True})\n",
    "report_sel.to_csv(\"report_sel.csv\")\n",
    "\n",
    "\n",
    "print(report_sel['CLASS'].unique())\n",
    "print('False: ',report_sel.groupby('CLASS').size()[0].sum())\n",
    "print('True: ',report_sel.groupby('CLASS').size()[1].sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "kfold = KFold(n_splits=10, random_state=7)\n",
    "model = GaussianNB()\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())\n",
    "\n",
    "\n",
    "model.fit(X,Y)\n",
    "output = model.predict(test.values)\n",
    "\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "mcc = matthews_corrcoef(model.predict(X),Y)\n",
    "print('MCC:',mcc)\n",
    "                       \n",
    "report_bayes = pd.DataFrame(output)\n",
    "report_bayes.columns = ['CLASS']\n",
    "report_bayes.index.name = \"Index\"\n",
    "report_bayes['CLASS']=report_bayes['CLASS'].map({0.0:False, 1.0:True})\n",
    "report_bayes.to_csv(\"report_bayes.csv\")\n",
    "\n",
    "\n",
    "print(report_bayes['CLASS'].unique())\n",
    "print('False: ',report_bayes.groupby('CLASS').size()[0].sum())\n",
    "print('True: ',report_bayes.groupby('CLASS').size()[1].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classiffication and Regression Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### used for classification or regression predictive modeling problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "kfold = KFold(n_splits=10, random_state=7)\n",
    "model = DecisionTreeClassifier()\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())\n",
    "\n",
    "\n",
    "model.fit(X,Y)\n",
    "output = model.predict(test.values)\n",
    "\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "mcc = matthews_corrcoef(model.predict(X),Y)\n",
    "print('MCC:',mcc)\n",
    "                       \n",
    "report_tree = pd.DataFrame(output)\n",
    "report_tree.columns = ['CLASS']\n",
    "report_tree.index.name = \"Index\"\n",
    "report_tree['CLASS']=report_tree['CLASS'].map({0.0:False, 1.0:True})\n",
    "report_tree.to_csv(\"report_tree.csv\")\n",
    "\n",
    "\n",
    "print(report_tree['CLASS'].unique())\n",
    "print('False: ',report_tree.groupby('CLASS').size()[0].sum())\n",
    "print('True: ',report_tree.groupby('CLASS').size()[1].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machines "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A support vector machine (SVM) is a supervised machine learning model that uses classification algorithms for two-group classification problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "kfold = KFold(n_splits=10, random_state=7)\n",
    "model = SVC()\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())\n",
    "\n",
    "model.fit(X,Y)\n",
    "output = model.predict(test.values)\n",
    "\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "mcc = matthews_corrcoef(model.predict(X),Y)\n",
    "print('MCC:',mcc)\n",
    "                       \n",
    "report_svm = pd.DataFrame(output)\n",
    "report_svm.columns = ['CLASS']\n",
    "report_svm.index.name = \"Index\"\n",
    "report_svm['CLASS']=report_svm['CLASS'].map({0.0:False, 1.0:True})\n",
    "report_svm.to_csv(\"report_svm.csv\")\n",
    "\n",
    "\n",
    "print(report_svm['CLASS'].unique())\n",
    "print('False: ',report_svm.groupby('CLASS').size()[0].sum())\n",
    "print('True: ',report_svm.groupby('CLASS').size()[1].sum())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine Models Into Ensemble Predictions\n",
    "\n",
    "The three most popular methods for combining the predictions from different models are:\n",
    "   \n",
    "   Bagging\n",
    "   Boosting\n",
    "   Voting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BoostingAlgorithms\n",
    "####  These seek to improve the prediction power by training a sequence of weak models, each compensating the weaknesses of its predecessors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost\n",
    "#### This is specifically designed for classification problems\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AdaBoost Classification\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "\n",
    "X = array[:,0:11]\n",
    "Y = array[:,11]\n",
    "\n",
    "num_trees = 39\n",
    "seed=10\n",
    "\n",
    "kfold = KFold(n_splits=10, random_state=seed)\n",
    "\n",
    "model = AdaBoostClassifier(n_estimators=num_trees, random_state=seed)\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "\n",
    "print(results.mean())\n",
    "\n",
    "model.fit(X,Y)\n",
    "output = model.predict(test.values)\n",
    "\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "mcc = matthews_corrcoef(model.predict(X),Y)\n",
    "print('MCC:',mcc)\n",
    "                       \n",
    "report_ada = pd.DataFrame(output)\n",
    "report_ada.columns = ['CLASS']\n",
    "report_ada.index.name = \"Index\"\n",
    "report_ada['CLASS']=report_ada['CLASS'].map({0.0:False, 1.0:True})\n",
    "report_ada.to_csv(\"report_ada.csv\")\n",
    "\n",
    "\n",
    "print(report_ada['CLASS'].unique())\n",
    "print('False: ',report_ada.groupby('CLASS').size()[0].sum())\n",
    "print('True: ',report_ada.groupby('CLASS').size()[1].sum())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bagging is used with decision trees where it significantly raises the stability of models in the reduction of variance and improving accuracy, which eliminates the challenge of overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagged Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bagged Decision Trees for Classification\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "#split the data in portions\n",
    "X = array[:,0:11]\n",
    "Y = array[:,11]\n",
    "seed = 7 #duplication\n",
    "\n",
    "#split according to cross validation\n",
    "kfold = KFold(n_splits=10, random_state=seed)\n",
    "\n",
    "#initialize the model\n",
    "cart = DecisionTreeClassifier()\n",
    "\n",
    "#bagging\n",
    "num_trees = 250\n",
    "\n",
    "#model\n",
    "model = BaggingClassifier(base_estimator=cart, n_estimators=num_trees, random_state=seed)\n",
    "\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())\n",
    "\n",
    "model.fit(X,Y)\n",
    "output = model.predict(test.values)\n",
    "\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "mcc = matthews_corrcoef(model.predict(X),Y)\n",
    "print('MCC:',mcc)\n",
    "                       \n",
    "report_bag = pd.DataFrame(output)\n",
    "report_bag.columns = ['CLASS']\n",
    "report_bag.index.name = \"Index\"\n",
    "report_bag['CLASS']=report_bag['CLASS'].map({0.0:False, 1.0:True})\n",
    "report_bag.to_csv(\"report_bag.csv\")\n",
    "\n",
    "\n",
    "print(report_bag['CLASS'].unique())\n",
    "print('False: ',report_bag.groupby('CLASS').size()[0].sum())\n",
    "print('True: ',report_bag.groupby('CLASS').size()[1].sum())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "#### The random forest combines hundreds or thousands of decision trees, trains each one on a slightly different set of the observations, splitting nodes in each tree considering a limited number of the features. The final predictions of the random forest are made by averaging the predictions of each individual tree.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Classification\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "X = array[:,0:11]\n",
    "Y = array[:,11]\n",
    "\n",
    "num_trees = 1000\n",
    "\n",
    "max_features = 3\n",
    "\n",
    "kfold = KFold(n_splits=10, random_state=7)\n",
    "model = RandomForestClassifier(n_estimators=num_trees, max_features=max_features)\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())\n",
    "\n",
    "model.fit(X,Y)\n",
    "output = model.predict(test.values)\n",
    "\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "mcc = matthews_corrcoef(model.predict(X),Y)\n",
    "print('MCC:',mcc)\n",
    "                       \n",
    "report_rf = pd.DataFrame(output)\n",
    "report_rf.columns = ['CLASS']\n",
    "report_rf.index.name = \"Index\"\n",
    "report_rf['CLASS']=report_rf['CLASS'].map({0.0:False, 1.0:True})\n",
    "report_rf.to_csv(\"report_rf.csv\")\n",
    "\n",
    "\n",
    "print(report_rf['CLASS'].unique())\n",
    "print('False: ',report_rf.groupby('CLASS').size()[0].sum())\n",
    "print('True: ',report_rf.groupby('CLASS').size()[1].sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "X = array[:,0:11]\n",
    "Y = array[:,11]\n",
    "\n",
    "num_trees = 100\n",
    "max_features = 7\n",
    "\n",
    "kfold = KFold(n_splits=10, random_state=7)\n",
    "\n",
    "model = ExtraTreesClassifier(n_estimators=num_trees, max_features=max_features)\n",
    "\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "\n",
    "print(results.mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voting Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Voting Ensemble for Classification\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "\n",
    "X = array[:,0:11]\n",
    "Y = array[:,11]\n",
    "kfold = KFold(n_splits=10, random_state=7)\n",
    "\n",
    "# create the sub models\n",
    "estimators = []\n",
    "model1 = LogisticRegression()\n",
    "estimators.append(('logistic', model1))\n",
    "\n",
    "model2 = DecisionTreeClassifier()\n",
    "estimators.append(('cart', model2))\n",
    "\n",
    "model3 = SVC()\n",
    "estimators.append(('svm', model3))\n",
    "\n",
    "model4 = XGBClassifier()\n",
    "estimators.append(('xgb', model4))\n",
    "\n",
    "model5 = RandomForestClassifier()\n",
    "estimators.append(('rfc', model5))\n",
    "\n",
    "# create the ensemble model\n",
    "ensemble = VotingClassifier(estimators)\n",
    "results = cross_val_score(ensemble, X, Y, cv=kfold)\n",
    "print(results.mean())\n",
    "\n",
    "\n",
    "model.fit(X,Y)\n",
    "output = model.predict(test.values)\n",
    "\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "mcc = matthews_corrcoef(model.predict(X),Y)\n",
    "print('MCC:',mcc)\n",
    "                       \n",
    "report_v = pd.DataFrame(output)\n",
    "report_v.columns = ['CLASS']\n",
    "report_v.index.name = \"Index\"\n",
    "report_v['CLASS']=report_v['CLASS'].map({0.0:False, 1.0:True})\n",
    "report_v.to_csv(\"report_v.csv\")\n",
    "\n",
    "\n",
    "print(report_v['CLASS'].unique())\n",
    "print('False: ',report_v.groupby('CLASS').size()[0].sum())\n",
    "print('True: ',report_v.groupby('CLASS').size()[1].sum())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## comparing the algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# prepare models and add them to a list\n",
    "from matplotlib import pyplot\n",
    "\n",
    "models = []\n",
    "models.append(('LR', LogisticRegression(solver='liblinear', multi_class='ovr')))\n",
    "models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "models.append(('KNN', KNeighborsClassifier()))\n",
    "models.append(('CART', DecisionTreeClassifier()))\n",
    "models.append(('NB', GaussianNB()))\n",
    "models.append(('SVM', SVC(gamma='auto')))\n",
    "models.append(('ETC', ExtraTreesClassifier()))\n",
    "models.append(('RFC', RandomForestClassifier()))\n",
    "\n",
    "# evaluate each model in turn\n",
    "results = []\n",
    "names = []\n",
    "scoring = 'accuracy'\n",
    "\n",
    "for name, model in models:\n",
    "    kfold = KFold(n_splits=10, random_state=7)\n",
    "    cv_results = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = (name, cv_results.mean(), cv_results.std())\n",
    "    print(msg)\n",
    "\n",
    "# boxplot algorithm comparison\n",
    "fig = pyplot.figure()\n",
    "fig.suptitle('Algorithm Comparison')\n",
    "ax = fig.add_subplot(111)\n",
    "pyplot.boxplot(results)\n",
    "ax.set_xticklabels(names)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# '''''''''''''''''''''''''''''''END''''''''''''''''''''''''''''''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
