{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: \n",
    "# https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # used in linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) \n",
    "# will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Machine Learning pipeline\n",
    "\n",
    "## by Tumuhimbise Peninah\n",
    "## This involves;\n",
    "### 1). import necessary libraries and the data. 2). look at your raw data and review the dimensions. 3). review the data types of attributes in your data. 4). summarize your data using descriptive statistics. 5). understand the relationships in your data using correlations. 6). review the skew of the distributions of each attribute and visualise your data. 7). prepare your data for machine learning. 8). feature selection for training your ML models. 9). evaluate the performance of ML algorithms. 10). machine learning algorithm performance metrics. 11). create model based on training dataset. 12). use model on test dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary libraries\n",
    "import numpy as np # used in linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt #interactive plots and plot generation\n",
    "import seaborn as sns #provides interface for data visualization\n",
    "\n",
    "# remove all warnings from the cells\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import datasets; training dataset;AMP_TrainSet.csv and test dataset; Test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import your data\n",
    "AMP_TrainSet = pd.read_csv(\"../input/AMP_TrainSet.csv\")\n",
    "Test = pd.read_csv(\"../input/Test.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the head() command, we can see how our data looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this displays the first five lines of the train dataset\n",
    "AMP_TrainSet.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this displays the first five lines of the test dataset\n",
    "Test.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensions of our dataset\n",
    "\n",
    "## This will tell us how many rows and columns our datasets contain. The first figure represents the rows and the second figure represents the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape command tells us how many rows and columns our dataset contain. \n",
    "AMP_TrainSet.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns command tells us how many columns our dataset contain.\n",
    "AMP_TrainSet.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data type for attributes of the dataset\n",
    "## Each sample in a dataset has an arbitrary number of attributes. They are stored as vectors of the same length as the number of samples in a collection, and are accessible via the attribute. Attribute data can be stored as one of five different field types in a dataset: character, integer, floating, date, and BLOB.\n",
    "## Data types are the classification or categorization of data items. Data types represent a kind of value which determines what operations can be performed on that data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "",
    "_uuid": ""
   },
   "outputs": [],
   "source": [
    "#dtypes command will show us the datatypes for each attribute\n",
    "AMP_TrainSet.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descriptive Statistics\n",
    "## When we have a set of observations, it is useful to summarize features of our data into a single statement called a descriptive statistic. As their name suggests, descriptive statistics describe a particular quality of the data they summarize. These statistics fall into two general categories: the measures of central tendency such as mean, median and mode and the measures of spread such as standard deviation, range and interquatrtile range and variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# describe() gives us the descriptive statistics as you can see below\n",
    "AMP_TrainSet.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Distribution\n",
    "## To know how balanced the class values are, we need to classify the data. You can quickly get an idea of the distribution of the class attributes in Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are classifying the data based on CLASS a column in our train dataset\n",
    "AMP_TrainSet.groupby('CLASS').size().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlations Between Attributes\n",
    "## Correlation is any statistical association, though it commonly refers to the degree to which a pair of variables are linearly related. Correlations are useful because they can indicate a predictive relationship that can be exploited.\n",
    "## A correlation of -1 or 1 shows a full negative or positive correlation respectively. Whereas a value of 0 shows no correlation at all. Some machine learning algorithms like linear and logistic regression can suffer poor performance if there are highly correlated attributes in your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us use pearson correlation method\n",
    "AMP_TrainSet.corr(method='pearson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we are going to plot a heat map to see the correlation\n",
    "# the seaborn package is usually loaded at this point\n",
    "# but since we already imported no need to do it now\n",
    "plt.figure(figsize=(6,6))\n",
    "sns.heatmap(AMP_TrainSet.corr(method='pearson'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's check how these variables correlate to CLASS \n",
    "# because we are trying to build an algorithm to predict the CLASS variable\n",
    "\n",
    "AMP_TrainSet.corr(method='pearson')['CLASS']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The following have correlation coefficients closest to 1 and therefore are better picks to use to build the algorithms\n",
    "### AS_MeanAmphiMoment    0.693552\n",
    "### FULL_Charge           0.534602"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skew of the distributions of each attribute\n",
    "## A data is called as skewed when curve appears distorted or skewed either to the left or to the right, in a statistical distribution. In a normal distribution, the graph appears symmetry meaning that there are about as many data values on the left side of the median as on the right side. Skew refers to a distribution that is assumed Gaussian (normal or bell curve) that is shifted or squashed in one direction or another.\n",
    "## You can calculate the skew of each attribute using the skew() function. If skewness value lies above +1 or below -1, data is highly skewed. If it lies between +0.5 to -0.5, it is moderately skewed. If the value is 0, then the data is symmetric\n",
    "## When data is positively skewed, the data can be transformed using some common transformations like square root, cube root and logarithm\n",
    "## When data is negatively skewed, transformations that can be used to reduce the skewedness are square and logarithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skew command is used to show the skew distributions \n",
    "# plot visualises the skew distributions for all the attributes\n",
    "AMP_TrainSet.skew().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualise your data\n",
    "## Visualisation of the data can provide some really important information about your dataset. Plots are used for visualisation; univariate plots that can be used to understand each attribute of the data such as histograms, density plots, box and whisker plots and multivariate plots that show interactions between multiple variables in the data such as correlation matrix plot and scatter plot matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Univariate plots;\n",
    "\n",
    "## 1. Histograms\n",
    "## A histogram is one of the most frequently used data visualization techniques in machine learning. It represents the distribution of a continuous variable over a given interval or period of time. Histograms plot the data by dividing it into intervals called ‘bins’. It is used to inspect the underlying frequency distribution (eg. Normal distribution), outliers, skewness, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,15))\n",
    "AMP_TrainSet.hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Density plots\n",
    "## This are similar to histograms except that we have a smooth curve instead of the bars. Density plots a way to estimate the probability density function of a continuous random variable. It is used when you need to know the distribution of the variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have to indicate what kind of plot we want in the plot() function\n",
    "AMP_TrainSet.plot(kind='density', subplots=True, layout=(3,4), sharex=False)\n",
    "# this displays the plot\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Box and Whisker Plots\n",
    "## Boxplots summarize the distribution of each attribute, drawing a line for the median (middle value) and a box around the 25th and 75th percentiles (the middle 50% of the data). The whiskers give an idea of the spread of the data and dots outside of the whiskers show candidate outlier values (values that are 1.5 times greater than the size of spread of the middle 50% of the data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have to indicate what kind of plot we want in the plot() function\n",
    "AMP_TrainSet.plot(kind='box', subplots=True, layout=(3,4), sharex=False, sharey=False)\n",
    "# this displays the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate Plots\n",
    "\n",
    "## 1. Correlation Matrix Plot\n",
    "## Correlogram is a graph of correlation matrix. It is very useful to highlight the most correlated variables in a data table. In this plot, correlation coefficients is colored according to the value. Correlation matrix can be also reordered according to the degree of association between variables.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's remind ourseves of the columns in our data\n",
    "AMP_TrainSet.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corr() is used to get the correlation\n",
    "# we are redirecting the correlations to correlations\n",
    "correlations = AMP_TrainSet.corr()\n",
    "\n",
    "# now let's plot the correlation matriix\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(correlations, vmin=-1, vmax=1)\n",
    "fig.colorbar(cax)\n",
    "ticks = np.arange(0,9,1)\n",
    "ax.set_xticks(ticks)\n",
    "ax.set_yticks(ticks)\n",
    "ax.set_xticklabels(AMP_TrainSet.columns)\n",
    "ax.set_yticklabels(AMP_TrainSet.columns)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Scatter Plot Matrix\n",
    "## A scatter plot shows the relationship between two variables as dots in two dimensions, one axis for each attribute. You can create a scatter plot for each pair of attributes in your data.\n",
    "\n",
    "## Scatter plots are useful for spotting structured relationships between variables, like whether you could summarize the relationship between two variables with a line. Attributes with structured relationships may also be correlated and good candidates for removal from your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are using pairplot to get the scatter plot matrix\n",
    "# sns to visualisation the matrix\n",
    "sns.pairplot(AMP_TrainSet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Your Data For Machine Learning\n",
    "\n",
    "## Data pre-processing\n",
    "## Data preprocessing is an integral step in Machine Learning as the quality of data and the useful information that can be derived from it directly affects the ability of our model to learn; therefore, it is extremely important that we preprocess our data before feeding it into our model.\n",
    "## The steps involved in data preprocessing include; splitting the dataset into the input and output variables for machine learning, apply a pre-processing transform to the input variables and summarize the data to show the change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Rescaling your data\n",
    "## This involves rescaling the attributes so all vallues are in the range between 0 and 1. This is useful for optimisation algorithms, you can rescale your data using scikit-learn using the MinMaxScaler class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set_printoptions determines the way floating point numbers, arrays and other NumPy objects are displayed.\n",
    "from numpy import set_printoptions\n",
    "\n",
    "# MinMaxScaler estimator transforms features by scaling each feature to a given range.\n",
    "from sklearn.preprocessing import MinMaxScaler \n",
    "\n",
    "# convert our train dataset in to a dataset\n",
    "AMP_array = AMP_TrainSet.values\n",
    "\n",
    "# separate array into input and output components\n",
    "# our data has 11 variables and that's what we are using to convert to arrays\n",
    "T = AMP_array[:,0:11]\n",
    "P = AMP_array[:,11]\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "rescaledT = scaler.fit_transform(T)\n",
    "\n",
    "# summarize transformed data\n",
    "set_printoptions(precision=3)\n",
    "print(rescaledT[0:5,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Standardize Data\n",
    "## In Standardization we transform our values such that the mean of the values is 0 and the standard deviation is 1.\n",
    "## Consider a data frame with 2 numerical values Age and Salary . They are not on the same scale as Age is in years and Salary is in dollars and since Salary will always be greater than Age, the model will give more weight to salary which is not the ideal scenario as age is also an integral factor. In order to avoid this issue we perform standardization. So we just calculate the mean and standard deviation of the values and then for each data point we just subtract the mean and divide it by standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# StandardScaler standardizes features by removing the mean and scaling to unit variance\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "AMP_array2 = AMP_TrainSet.values\n",
    "\n",
    "# separate array into input and output components\n",
    "T = AMP_array2[:,0:11]\n",
    "P = AMP_array2[:,11]\n",
    "scaler = StandardScaler().fit(T)\n",
    "standardizedT = scaler.transform(T)\n",
    "\n",
    "# summarize transformed data\n",
    "set_printoptions(precision=3)\n",
    "print(standardizedT[0:5,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Normalize Data\n",
    "## Normalizing in scikit-learn refers to rescaling each observation (row) to have a length of 1 (called a unit norm or a vector with the length of 1 in linear algebra)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "AMP_array3 = AMP_TrainSet.values\n",
    "\n",
    "# separate array into input and output components\n",
    "T = AMP_array3[:,0:11]\n",
    "P = AMP_array3[:,11]\n",
    "scaler = Normalizer().fit(T)\n",
    "normalizedT = scaler.transform(T)\n",
    "\n",
    "# summarize transformed data\n",
    "set_printoptions(precision=3)\n",
    "\n",
    "#print the data type so we can know what we are working with in the dataset\n",
    "print(normalizedT[0:5,:])\n",
    "print(type(normalizedT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Binarize Data\n",
    "## You can transform your data using a binary threshold. All values above the threshold are marked 1 and all equal to or below are marked as 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Binarizer\n",
    "\n",
    "AMP_array4 = AMP_TrainSet.values\n",
    "\n",
    "# separate array into input and output components\n",
    "T = AMP_array4[:,0:11]\n",
    "P = AMP_array4[:,11]\n",
    "binarizer = Binarizer(threshold=0.0).fit(T)\n",
    "binaryT = binarizer.transform(T)\n",
    "\n",
    "# summarize transformed data\n",
    "set_printoptions(precision=3)\n",
    "print(binaryT[0:5,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection\n",
    "\n",
    "## Feature selection is a process where you automatically or manually select those features in your data which contribute most to the prediction variable or output in which you are interested. Having irrelevant features in your data can decrease the accuracy of many models, especially linear algorithms like linear and logistic regression. Feature selection can reduce overfitting, improve accuracy and also reduce training time for the model.\n",
    "\n",
    "## Feature selection techniques that are easy to use include; univariate selection, feature importance and correlation matrix with heatmap\n",
    "\n",
    "## 1. Univariate Selecion\n",
    "## Statistical tests can be used to select those features that have the strongest relationship with the output variable. The scikit-learn library provides the SelectKBest class that can be used with a suite of different statistical tests to select a specific number of features.\n",
    "## Chi-Squared test (chi2) is a statistical hypothesis test that assumes (the null hypothesis) that the observed frequencies for a categorical variable match the expected frequencies for the categorical variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to select a specific number of features\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "# feature selector chi2 is imported\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "# separate array into input and output components\n",
    "AMP_array_ = AMP_TrainSet.values\n",
    "T = AMP_array_[:,0:11]\n",
    "P = AMP_array_[:,11]\n",
    "\n",
    "# feature extraction\n",
    "# rescaledT data is used because it has all positive values\n",
    "# this feature selector requires X values for test.fit to be positive \n",
    "test = SelectKBest(score_func=chi2, k=4)\n",
    "fit = test.fit(rescaledT, P)\n",
    "\n",
    "# summarize scores\n",
    "set_printoptions(precision=3)\n",
    "print(fit.scores_)\n",
    "features = fit.transform(rescaledT)\n",
    "\n",
    "# summarize selected features\n",
    "print(features[0:5,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recursive Feature Elimination (RFE)\n",
    "## Recursive means doing or saying the same thing several times in order to produce a particular result or effect and a feature is an individual measurable property or characteristic of a phenomenon being observed attribute in your dataset\n",
    "\n",
    "## Recursive Feature Elimination works by recursively removing attributes and building a model on those attributes that remain. It uses the model accuracy to identify which attributes or combination of attributes contribute the most to predicting the target attribute (output)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing required libraries\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# separate array into input and output components\n",
    "AMP_array_1 = AMP_TrainSet.values\n",
    "T = AMP_array_1[:,0:11]\n",
    "P = AMP_array_1[:,11]\n",
    "\n",
    "# feature extraction\n",
    "model = LogisticRegression()\n",
    "\n",
    "# now we decide how many features we want to be selected for our model\n",
    "rfe = RFE(model, 3)\n",
    "fit = rfe.fit(T, P)\n",
    "print(\"Num Features: \",  fit.n_features_)\n",
    "print(\"Selected Features:\",  fit.support_)\n",
    "print(\"Feature Ranking: \",  fit.ranking_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis\n",
    "## This is a technique used to reduce the dimension of the feature space by feature extraction. For example, if we have 10 variables, in feature extraction, we create new independent variables by combining the old ten variables. By creating new variables it might seem as if more dimensions are introduced, but we select only a few variables from the newly created variables in the order of importance. Then the number of those selected variables is less than what we started with and that’s how we reduce the dimensionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# separate array into input and output components\n",
    "AMP_array = AMP_TrainSet.values\n",
    "T = AMP_array[:,0:11]\n",
    "P = AMP_array[:,11]\n",
    "\n",
    "# feature extraction\n",
    "pca = PCA(n_components=3)\n",
    "fit = pca.fit(T)\n",
    "\n",
    "# summarize components\n",
    "print(\"Explained Variance: \" , fit.explained_variance_ratio_)\n",
    "print(fit.components_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance\n",
    "## Knowing feature importance indicated by machine learning models can benefit you in multiple ways, for example: by getting a better understanding of the model’s logic you can not only verify it being correct but also work on improving the model by focusing only on the important variables, variable selection (you can remove a certain number of variables that are not that significant) and have similar or better performance in much shorter training time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Random Forest method is used for determining feature importance\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# separate array into input and output components\n",
    "AMP_array = AMP_TrainSet.values\n",
    "T = AMP_array[:,0:11]\n",
    "P = AMP_array[:,11]\n",
    "model = RandomForestRegressor(n_estimators = 100,\n",
    "                           n_jobs = 1,\n",
    "                           oob_score = True,\n",
    "                           bootstrap = True,\n",
    "                           random_state = 42)\n",
    "model.fit(T, P)\n",
    "\n",
    "print('R^2 Training Score: {:.2f} \\nOOB Score: {:.2f} \\nR^2 Validation Score: {:.2f}'.format(model.score(T, P), \n",
    "                                                                                             model.oob_score_,\n",
    "                                                                                             model.score(T, P)))\n",
    "# display the feature importance values\n",
    "print(model.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we can visualise the importance of features\n",
    "(pd.Series(model.feature_importances_, index=AMP_TrainSet.iloc[:,0:11].columns).nlargest(11).plot(kind='barh')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can clearly see that AS_MeanAmphiMoment is most important of all the features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Machine Learning Algorithms\n",
    "## We must evaluate our machine learning algorithms on data that is not used to train the algorithm. Once we estimate the performance of our algorithm, we can then re-train the final algorithm on the entire training dataset and get it ready for operational use.\n",
    "## Next we shall look at four different techniques that we can use to split up our training dataset and create useful estimates of performance for our machine learning algorithms: Train and Test Sets, K-fold Cross Validation, Leave One Out Cross Validation and Repeated Random Test-Train Splits.\n",
    "## You need some kind of assurance that your model has got most of the patterns from the data correct, and its not picking up too much on the noise, or in other words its low on bias and variance.\n",
    "## 1. Split into Train and Test Sets\n",
    "## The simplest method that we can use to evaluate the performance of a machine learning algorithm is to use different training and testing datasets.\n",
    "## We can take our original dataset, split it into two parts. Train the algorithm on the first part, make predictions on the second part and evaluate the predictions against the expected results.The size of the split can depend on the size and specifics of your dataset, although it is common to use 67% of the data for training and the remaining 33% for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate using a train and a test set\n",
    "# LogisticRegression as the algorithm in this case\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# separate array into input and output components\n",
    "AMP_array = AMP_TrainSet.values\n",
    "T = AMP_array[:,0:11]\n",
    "P = AMP_array[:,11]\n",
    "\n",
    "test_size = 0.33\n",
    "seed = 7\n",
    "T_train, T_test, P_train, P_test = model_selection.train_test_split(T, P, test_size=test_size, random_state=seed)\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(T_train, P_train)\n",
    "result = model.score(T_test, P_test)\n",
    "print(\"Accuracy: %.3f%%\" % (result*100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-fold Cross Validation\n",
    "## Cross validation is an approach that you can use to estimate the performance of a machine learning algorithm with less variance than a single train-test set split.\n",
    "\n",
    "## It works by splitting the dataset into k-parts (e.g. k=5 or k=10). Each split of the data is called a fold. The algorithm is trained on k-1 folds with one held back and tested on the held back fold. This is repeated so that each fold of the dataset is given a chance to be the held back test set. After running cross validation you end up with k different performance scores that you can summarize using a mean and a standard deviation. The result is a more reliable estimate of the performance of the algorithm on new data given your test data. It is more accurate because the algorithm is trained and evaluated multiple times on different data.\n",
    "## For modest sized datasets in the thousands or tens of thousands of records, k values of 3, 5 and 10 are common."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import model_selection\n",
    "\n",
    "# separate array into input and output components\n",
    "AMP_array = AMP_TrainSet.values\n",
    "T = AMP_array[:,0:11]\n",
    "P = AMP_array[:,11]\n",
    "\n",
    "num_instances = len(T)\n",
    "seed = 7\n",
    "kfold = model_selection.KFold(n_splits=15, random_state=seed)\n",
    "\n",
    "model = LogisticRegression()\n",
    "results = model_selection.cross_val_score(model, T, P, cv=kfold)\n",
    "print(\"Accuracy: %.3f%% (%.3f%%)\" % (results.mean()*100.0, results.std()*100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leave One Out Cross Validation\n",
    "## This approach leaves p data points out of training data, i.e. if there are n data points in the original sample then, n-p samples are used to train the model and p points are used as the validation set. This is repeated for all combinations in which original sample can be separated this way, and then the error is averaged for all trials, to give overall effectiveness. This method is exhaustive in the sense that it needs to train and validate the model for all possible combinations, and for moderately large p, it can become computationally infeasible.\n",
    "\n",
    "## A particular case of this method is when p = 1. This is known as Leave one out cross validation. This method is generally preferred over the previous one because it does not suffer from the intensive computation, as number of possible combinations is equal to number of data points in original sample or n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# separate array into input and output components\n",
    "AMP_array = AMP_TrainSet.values\n",
    "T = AMP_array[:,0:11]\n",
    "P = AMP_array[:,11]\n",
    "\n",
    "num_folds = 10\n",
    "loocv = model_selection.LeaveOneOut()\n",
    "\n",
    "model = LogisticRegression()\n",
    "results = model_selection.cross_val_score(model, T, P, cv=loocv)\n",
    "print(\"Accuracy: %.3f%%\" % (results.mean()*100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repeated Random Test-Train Splits\n",
    "## This technique is a hybrid of traditional train-test splitting and the k-fold cross-validation method. In this technique, we create random splits of the data in the training-test set manner and then repeat the process of splitting and evaluating the algorithm multiple times, just like the cross-validation method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# separate array into input and output components\n",
    "AMP_array = AMP_TrainSet.values\n",
    "T = AMP_array[:,0:11]\n",
    "P = AMP_array[:,11]\n",
    "\n",
    "kfold = ShuffleSplit(n_splits=10, test_size=0.33, random_state=7)\n",
    "\n",
    "model = LogisticRegression()\n",
    "results = cross_val_score(model, T, P, cv=kfold)\n",
    "print(\"Accuracy: %.3f%% (%.3f%%)\" % (results.mean()*100.0, results.std()*100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Algorithm Performance Metrics;\n",
    "## Classiffication Accuracy\n",
    "## Classification Accuracy is what we usually mean, when we use the term accuracy. It is the ratio of number of correct predictions to the total number of input samples. It works well only if there are equal number of samples belonging to each class. For example, consider that there are 98% samples of class A and 2% samples of class B in our training set. Then our model can easily get 98% training accuracy by simply predicting every training sample belonging to class A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "AMP_array = AMP_TrainSet.values\n",
    "T = AMP_array[:,0:11]\n",
    "P = AMP_array[:,11]\n",
    "\n",
    "kfold = KFold(n_splits=10, random_state=7)\n",
    "\n",
    "model = LogisticRegression()\n",
    "\n",
    "scoring = 'accuracy'\n",
    "\n",
    "results = cross_val_score(model, T, P, cv=kfold, scoring=scoring)\n",
    "print(\"Accuracy: %.3f%% (%.3f%%)\" % (results.mean(), results.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Area Under ROC Curve\n",
    "## Area Under Curve(AUC) is one of the most widely used metrics for evaluation. It is used for binary classification problem. AUC of a classifier is equal to the probability that the classifier will rank a randomly chosen positive example higher than a randomly chosen negative example. Before defining AUC, let us understand two basic terms:\n",
    "## True Positive Rate (Sensitivity) : True Positive Rate is defined as TP/ (FN+TP). True Positive Rate corresponds to the proportion of positive data points that are correctly considered as positive, with respect to all positive data points. False Positive Rate (Specificity) : False Positive Rate is defined as FP / (FP+TN). False Positive Rate corresponds to the proportion of negative data points that are mistakenly considered as positive, with respect to all negative data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "AMP_array = AMP_TrainSet.values\n",
    "T = AMP_array[:,0:11]\n",
    "P = AMP_array[:,11]\n",
    "\n",
    "kfold = KFold(n_splits=2, random_state=7)\n",
    "model = LogisticRegression()\n",
    "scoring = 'roc_auc'\n",
    "results = cross_val_score(model, T, P, cv=kfold, scoring=scoring)\n",
    "print(\"AUC:\", (results.mean(), results.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix\n",
    "## The confusion matrix is a handy presentation of the accuracy of a model with two or more classes. The table presents predictions on the x-axis and accuracy outcomes on the y-axis. The cells of the table are the number of predictions made by a machine learning algorithm. For example, a machine learning algorithm can predict 0 or 1 and each prediction may actually have been a 0 or 1. Predictions for 0 that were actually 0 appear in the cell for prediction = 0 and actual = 0, whereas predictions for 0 that were actually 1 appear in the cell for prediction = 0 and actual = 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# separate array into input and output components\n",
    "AMP_array = AMP_TrainSet.values\n",
    "T = AMP_array[:,0:11]\n",
    "P = AMP_array[:,11]\n",
    "\n",
    "test_size = 0.33\n",
    "seed = 7\n",
    "T_train, T_test, P_train, P_test = train_test_split(T, P, test_size=test_size,\n",
    "random_state=seed)\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(T_train, P_train)\n",
    "\n",
    "predicted = model.predict(T_test)\n",
    "matrix = confusion_matrix(P_test, predicted)\n",
    "print(matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Classiffication Report\n",
    "\n",
    "## The scikit-learn library provides a convenience report when working on classiffication problems to give you a quick idea of the accuracy of a model using a number of measures. The classification report() function displays the precision, recall, F1-score and support for each class. The example below demonstrates the report on the binary classication problem.\n",
    "## F1 Score is the Harmonic Mean between precision and recall. The range for F1 Score is [0, 1]. It tells you how precise your classifier is (how many instances it classifies correctly), as well as how robust it is (it does not miss a significant number of instances). High precision but lower recall, gives you an extremely accurate, but it then misses a large number of instances that are difficult to classify. The greater the F1 Score, the better is the performance of our model.\n",
    "## Precision: It is the number of correct positive results divided by the number of positive results predicted by the classifier.\n",
    "## Recall: It is the number of correct positive results divided by the number of all relevant samples (all samples that should have been identified as positive).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "AMP_array = AMP_TrainSet.values\n",
    "T = AMP_array[:,0:11]\n",
    "P = AMP_array[:,11]\n",
    "test_size = 0.33\n",
    "seed = 7\n",
    "T_train, T_test, P_train, P_test = train_test_split(T, P, test_size=test_size,\n",
    "random_state=seed)\n",
    "model = LogisticRegression()\n",
    "model.fit(T_train, P_train)\n",
    "predicted = model.predict(T_test)\n",
    "report = classification_report(P_test, predicted)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Metrics\n",
    "## Mean Absolute Error\n",
    "## Mean Absolute Error is the average of the difference between the Original Values and the Predicted Values. It gives us the measure of how far the predictions were from the actual output. However, they don’t gives us any idea of the direction of the error i.e. whether we are under predicting the data or over predicting the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "AMP_array = AMP_TrainSet.values\n",
    "T = AMP_array[:,0:11]\n",
    "P = AMP_array[:,11]\n",
    "\n",
    "kfold = KFold(n_splits=15, random_state=7)\n",
    "model = LinearRegression()\n",
    "scoring = 'neg_mean_absolute_error'\n",
    "results = cross_val_score(model, T, P, cv=kfold, scoring=scoring)\n",
    "print(\"MAE:\",(results.mean(), results.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Squared Error\n",
    "## Mean Squared Error(MSE) is quite similar to Mean Absolute Error, the only difference being that MSE takes the average of the square of the difference between the original values and the predicted values. The advantage of MSE being that it is easier to compute the gradient, whereas Mean Absolute Error requires complicated linear programming tools to compute the gradient. As, we take square of the error, the effect of larger errors become more pronounced then smaller error, hence the model can now focus more on the larger errors.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross Validation Regression MSE\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "AMP_array = AMP_TrainSet.values\n",
    "T = AMP_array[:,0:11]\n",
    "P = AMP_array[:,11]\n",
    "num_folds = 10\n",
    "kfold = KFold(n_splits=15, random_state=7)\n",
    "model = LinearRegression()\n",
    "scoring = 'neg_mean_squared_error'\n",
    "results = cross_val_score(model, T, P, cv=kfold, scoring=scoring)\n",
    "print(\"MSE:\",(results.mean(), results.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## R2 Metric\n",
    "\n",
    "## R-squared is a statistical measure of how close the data are to the fitted regression line. It is also known as the coefficient of determination, or the coefficient of multiple determination for multiple regression. The definition of R-squared is fairly straight-forward; it is the percentage of the response variable variation that is explained by a linear model. Or:\n",
    "\n",
    "## R-squared = Explained variation / Total variation\n",
    "\n",
    "## R-squared is always between 0 and 1: 0 indicates that the model explains none of the variability of the response data around its mean. 1 indicates that the model explains all the variability of the response data around its mean. In general, the higher the R-squared, the better the model fits your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross Validation Regression R^2\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "AMP_array = AMP_TrainSet.values\n",
    "T = AMP_array[:,0:11]\n",
    "P = AMP_array[:,11]\n",
    "\n",
    "kfold = KFold(n_splits=15, random_state=7)\n",
    "model = LinearRegression()\n",
    "scoring = 'r2'\n",
    "results = cross_val_score(model, T, P, cv=kfold, scoring=scoring)\n",
    "print(\"R^2:\",(results.mean(), results.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spot-Check Classification Machine Learning Algorithms\n",
    "## Spot-checking is a way of discovering which algorithms perform well on your machine learning problem. You cannot know which algorithms are best suited to your problem beforehand. You must trial a number of methods and focus attention on those that prove themselves the most promising.\n",
    "\n",
    "## You cannot know which algorithm will work best on your dataset before hand. You must use trial and error to discover a short list of algorithms that do well on your problem that you can then double down on and tune further. I call this process spot checking.\n",
    "### The question is not:\n",
    "\n",
    "   * What algorithm should I use on my dataset?\n",
    "\n",
    "## Instead it is:\n",
    "\n",
    "   * What algorithms should I spot check on my dataset?\n",
    "\n",
    "## Algorithms Overview\n",
    "\n",
    "## We are going to take a look at 10 classification algorithms that you can spot check on your dataset.\n",
    "## Linear Machine Learning Algorithms:\n",
    "   * Lasso\n",
    "   * Bayesian ridge regression\n",
    "   * Logistic Regression\n",
    "   * Linear Discriminant Analysis  \n",
    "\n",
    "## Nonlinear Machine Learning Algorithms:\n",
    "   * \n",
    "   * Naive Bayes\n",
    "   * Nearest centroid\n",
    "   * Stochastic gradient descent\n",
    "   * Classification and Regression Trees\n",
    "   * Support Vector Machines\n",
    "   \n",
    "\n",
    "## Ensemble machine learning algorithm:\n",
    "   * Bagging  \n",
    "   * Random Forest\n",
    "   * Boosting \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# boosting algorithm requires an experimental feature before importing for the classifier to work\n",
    "# importing classifiers from sklearn\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# linear algorithms\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "#non linear algorithms\n",
    "\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "#ensemble algorithms\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "\n",
    "\n",
    "# prepare all the models to be used and add them to a list also called appending\n",
    "# implies that they can all be run at once\n",
    "models = []\n",
    "# models.append(('Las', Lasso()))\n",
    "# Error:Classification metrics can't handle a mix of binary and continuous targets\n",
    "# this particular model is not good for our data\n",
    "\n",
    "#models.append(('BR', BayesianRidge()))\n",
    "models.append(('LR', LogisticRegression()))\n",
    "models.append(('LA', LinearDiscriminantAnalysis()))\n",
    "\n",
    "models.append(('DTC', DecisionTreeClassifier()))\n",
    "models.append(('NC', NearestCentroid()))\n",
    "models.append(('NB', GaussianNB()))\n",
    "models.append(('SVM', SVC()))\n",
    "models.append(('SGD',SGDClassifier()))\n",
    "\n",
    "models.append(('XGB',XGBClassifier()))\n",
    "models.append(('BAG',BaggingClassifier()))\n",
    "models.append(('RF',RandomForestClassifier()))\n",
    "models.append(('HGB',HistGradientBoostingClassifier()))\n",
    "\n",
    "# evaluate each model in turn using accuracy\n",
    "results = []\n",
    "names = []\n",
    "scoring = 'accuracy'\n",
    "\n",
    "# separate array into input and output components\n",
    "AMP_array = AMP_TrainSet.values\n",
    "T = AMP_array[:,0:11]\n",
    "P = AMP_array[:,11]\n",
    "\n",
    "for name, model in models:\n",
    "    kfold = KFold(n_splits=10, random_state=7)\n",
    "    cv_results = cross_val_score(model, T, P, cv=kfold, scoring=scoring)\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = (name, cv_results.mean(), cv_results.std())\n",
    "    print(msg)\n",
    "    \n",
    "# boxplot algorithm comparison\n",
    "fig = plt.figure()\n",
    "fig.suptitle('Algorithm Comparison')\n",
    "ax = fig.add_subplot(111)\n",
    "plt.boxplot(results)\n",
    "ax.set_xticklabels(names)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From the result, these two models have an accuracy closest to 1;Gaussian Naive Bayes has an accuracy of 88.08% and Nearest Centroid Classifier has 86.76%. Now we are going to test these algorithms using our test dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction metrics\n",
    "\n",
    "## Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Trial with naive bayes\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "array_1 = AMP_TrainSet.values\n",
    "X = array_1[:,0:11]\n",
    "Y = array_1[:,11]\n",
    "test_size = 0.33\n",
    "seed = 7\n",
    " \n",
    "array_2 = Test.values\n",
    "Test_set = array_2[:,0:11]\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size,\n",
    "random_state=seed)\n",
    "\n",
    "#model =GaussianNB ()\n",
    "NBmodel = GaussianNB()\n",
    "NBmodel.fit(X, Y)\n",
    "\n",
    "predicted = NBmodel.predict(Test_set)\n",
    "#matrix = confusion_matrix(Y_test, predicted)\n",
    "print(predicted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Trial with nearest centroid classifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "array_2 = AMP_TrainSet.values\n",
    "X = array_2[:,0:11]\n",
    "Y = array_2[:,11]\n",
    "test_size = 0.33\n",
    "seed = 7\n",
    " \n",
    "array_3 = Test.values\n",
    "Test_set = array_3[:,0:11]\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size,\n",
    "random_state=seed)\n",
    "\n",
    "# our model is nearest centroid\n",
    "NCmodel = NearestCentroid()\n",
    "NCmodel.fit(X, Y)\n",
    "\n",
    "predicted_2 = NCmodel.predict(Test_set)\n",
    "#matrix = confusion_matrix(Y_test, predicted_2)\n",
    "print(predicted)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating .csv for Naive bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indexing the predicted variable\n",
    "df1=pd.DataFrame(predicted)\n",
    "df1.columns=['CLASS']\n",
    "df1.index.names=[\"Index\"]\n",
    "df1['CLASS']=df1['CLASS'].map({0.0:False, 1.0:True})\n",
    "df1\n",
    "\n",
    "#converting to csv\n",
    "df1.to_csv('Naive_Bayes.csv')\n",
    "print(df1['CLASS'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating a .csv for Nearest centroid classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indexing the predicted variable\n",
    "df2=pd.DataFrame(predicted_2)\n",
    "df2.columns=['CLASS']\n",
    "df2.index.names=[\"Index\"]\n",
    "df2['CLASS']=df2['CLASS'].map({0.0:False, 1.0:True})\n",
    "df2\n",
    "\n",
    "#converting to csv\n",
    "df2.to_csv('NC.csv')\n",
    "print(df2['CLASS'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matthews correlation coefficient (MCC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import matthews_corrcoef\n",
    "\n",
    "array= AMP_TrainSet.values\n",
    "X = array[:,0:11]\n",
    "Y = array[:,11]\n",
    "\n",
    "test_size = 0.33\n",
    "seed = 7\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size,\n",
    "random_state=seed)\n",
    "\n",
    "model = GaussianNB()\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "predicted = model.predict(X_test)\n",
    "matrix = matthews_corrcoef(Y_test, predicted)\n",
    "print(matrix)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
