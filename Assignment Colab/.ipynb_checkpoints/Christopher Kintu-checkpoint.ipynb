{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "#First importing the necessary libraries helpful in building and/or testing the model\n",
    "#Three main libraries for a start: numpy, pandas, matplotlib.pyplot\n",
    "#numpy is for linear algebra, pandas for data processing\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kintu Christopher\n",
    "## This is my assignment.\n",
    "## I am in to win the competition\n",
    "\n",
    "```import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))```\n",
    "        \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Input data files necessary for this assignment were obtained from \"../input/\" directory.\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ignoring warnings: It is necessary to import while ignoring warnings\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data sets are loaded and read as below in form of csv files\n",
    "Train = pd.read_csv(\"../input/ace-class-assignment/AMP_TrainSet.csv\")\n",
    "Test = pd.read_csv(\"../input/ace-class-assignment/Test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data visualization\n",
    "## The data is loaded for viewing directly from the ACE_class_Assignment folder. This displays, in tabular format, the real values we are to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#There is need to know how many dimensions our data has\n",
    "# This helps me to know how many rows or columns are in each data set and also how large the data set is.\n",
    "#Too many rows and columns require a longer time to train the model.\n",
    "\n",
    "Train.shape, Test.shape\n",
    "\n",
    "# The Train data set has 12 columns and 3038 rows. \n",
    "# The Test data set has 758 rows and 11 columns\n",
    "#This means our Train data is large enough to train the algorithm without over training or under training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the column variables for the Train data set\n",
    "#This gives me an idea of the variables I am dealing with and how they are labelled.\n",
    "\n",
    "Train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Similarly, checking for column variables for the Test data set\n",
    "\n",
    "Test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is good to also take a look at they data types for each data set. \n",
    "#Just incase there are strings that need to be converted into floats \n",
    "\n",
    "#First, the train data set\n",
    "Train.dtypes\n",
    "\n",
    "#The data type of the train set is an object\n",
    "#The column variables are all floats and integers as shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Similarly, for the Test data set\n",
    "\n",
    "Test.dtypes\n",
    "\n",
    "#It is an object, with floats and integers only. \n",
    "#This is good, we dont need to change anything.\n",
    "#We are good to go."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking for quick statistical comparisons\n",
    "## This will quickly give me a view of the relationship between variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We check out quick summary statistics for both data sets\n",
    "\n",
    "Train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#summary statistics for the Test data set too\n",
    "\n",
    "Test.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attribute Correlations\n",
    "## It is important to check whether different attributes have a positive or negative correlation or none."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assuming a Gaussian distribution for both data stes; we shall use the pearson correlation\n",
    "\n",
    "Train.corr(method = 'pearson')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correlation on the Test data set\n",
    "\n",
    "Test.corr(method = 'pearson')\n",
    "\n",
    "#A quci eyeball shows all variables have some form of correlation to each other. \n",
    "#There is no variable with zero correlation to another"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting the data\n",
    "## Upon noticing some correlation, it is good to cisualize it in a heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing correlations between attributes using heatmaps\n",
    "#import the necessary library; seaborn\n",
    "\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot a heat map for the Test data set\n",
    "plt.figure(figsize=(6,6))\n",
    "sns.heatmap(Test.corr(method='pearson'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Heat map for the Train data set\n",
    "plt.figure(figsize=(6,6))\n",
    "sns.heatmap(Train.corr(method='pearson'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing for skewness of variables\n",
    "# This will be important incase some variables are skewed either positively or negatively since we are assuming a Gaussian distribution. Correcting for this skewness improves on the accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Skewness of distributions on the Train data set\n",
    "Train.skew().plot(kind= 'bar')\n",
    "\n",
    "#The NT_EFC195 show a high right skew (above 1).\n",
    "#We shall have to correct for this to improve the model accuracy\n",
    "#This we shall do by finding the square root, cube root or a logarithmic transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing for skewness on the Train Data set\n",
    "\n",
    "Test.skew().plot(kind= 'bar')\n",
    "\n",
    "# This data set also has a high positive skew for the NT_EFC195 variable as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding my data\n",
    "# Different plots will give me a good insight into what to do next with the data I have.Plots like histograms, scatter plots are good to give a genearl view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting a histogram for the Train Data\n",
    "\n",
    "plt.figure(figsize=(15,15))\n",
    "Train.hist()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting a histogram for the Test Data\n",
    "\n",
    "plt.figure(figsize=(15,15))\n",
    "Test.hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rug plots\n",
    "#The rugplots are like histograms except they show every single data point on the x-axis, allowing us to visualize all of the actual values\n",
    "#Both the Train and Test rugplots indicate some kind of left skew. This we need to take into account when designing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(Train, hist = False, kde = True, rug = True, color = 'darkblue', kde_kws={'linewidth': 3}, rug_kws={'color': 'black'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(Test, hist = False, kde = True, rug = True, color = 'darkblue', kde_kws={'linewidth': 3}, rug_kws={'color': 'black'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Box plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train\n",
    "Train.plot(kind='box', subplots=True, layout=(12,12), sharex=False, sharey=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test\n",
    "Test.plot(kind='box', subplots=True, layout=(12,12), sharex=False, sharey=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scatter Plot matrix\n",
    "#scatter plot matrix shows the relationship between two variables as dots in two dimensions all at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(Train)\n",
    "#The scatter plot for the train dataset indicates a mixture of correlations for different variables. \n",
    "#Some have a positive strong correlation, others a negative correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test data scatter plot\n",
    "sns.pairplot(Test)\n",
    "#Similarly, some have aa positive correlation and negative correlation between different paired variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing Data for machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rescaling Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rescaling\n",
    "#Since I might use features like regression and neural networks and algorithms that use distance measures like k-Nearest Neighbors. \n",
    "#I need to rescale my data using scikit-learn using the MinMaxScaler class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We rescale to make sure all our data values lie in a scale from 0 to 1\n",
    "from numpy import set_printoptions\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "array = Train.values\n",
    "# separate array into input and output components\n",
    "X = array[:,0:11]\n",
    "Y = array[:,11]\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "rescaledX = scaler.fit_transform(X)\n",
    "# summarize transformed data\n",
    "set_printoptions(precision=3)\n",
    "print(rescaledX[0:5,:])\n",
    "\n",
    "#Indeed all our data values have been rescaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# standardization of data\n",
    "```from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "array2 = data.values\n",
    "#separate array into input and output components\n",
    "X = array2[:,0:8]\n",
    "Y = array2[:,8]\n",
    "scaler = StandardScaler().fit(X)\n",
    "rescaledX = scaler.transform(X)\n",
    "# summarize transformed data\n",
    "set_printoptions(precision=3)\n",
    "print(rescaledX[0:5,:])\n",
    "```\n",
    "\n",
    "# Normalization of data\n",
    "``from sklearn.preprocessing import Normalizer\n",
    "\n",
    "array3 = data.values\n",
    "#separate array into input and output components\n",
    "X = array3[:,0:8]\n",
    "Y = array3[:,8]\n",
    "scaler = Normalizer().fit(X)\n",
    "normalizedX = scaler.transform(X)\n",
    "#summarize transformed data\n",
    "set_printoptions(precision=3)\n",
    "print(normalizedX[0:5,:])\n",
    "print(type(normalizedX))#print the data type so we can know what we are \n",
    "#working with in the dataset.\n",
    "```\n",
    "\n",
    "```# Binarizing data\n",
    "from sklearn.preprocessing import Binarizer\n",
    "\n",
    "array4 = data.values\n",
    "#separate array into input and output components\n",
    "X = array4[:,0:11]\n",
    "Y = array4[:,11]\n",
    "binarizer = Binarizer(threshold=0.0).fit(X)\n",
    "binaryX = binarizer.transform(X)\n",
    "#summarize transformed data\n",
    "set_printoptions(precision=3)\n",
    "print(binaryX[0:5,:])\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using the Recursive elimination\n",
    "#from sklearn.feature_selection import RFE\n",
    "#from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#array_1 = Train.values\n",
    "#X = array_1[:,0:11]\n",
    "#Y = array_1[:,11]\n",
    "\n",
    "# Extracting features\n",
    "#model = LogisticRegression()\n",
    "#rfe = RFE(model, 3)\n",
    "#fit = rfe.fit(X, Y)\n",
    "#print(\"Num Features: \",  fit.n_features_)\n",
    "#print(\"Selected Features:\",  fit.support_)\n",
    "#print(\"Feature Ranking: \",  fit.ranking_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluating perfomance\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "array = Train.values\n",
    "X = array[:,0:11]\n",
    "Y = array[:,11]\n",
    "test_size = 0.33\n",
    "seed = 10\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size, random_state=seed)\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, Y_train)\n",
    "result = model.score(X_test, Y_test)\n",
    "print(\"Accuracy: \",  (result*100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#out = model.predict(Test.values)\n",
    "\n",
    "#KC = pd.DataFrame(out) #Converting to data frame\n",
    "#KC.columns=[\"CLASS\"] #Naming the column\n",
    "#KC.index.name=\"Index\" #Creating a column index\n",
    "#KC[\"CLASS\"]=KC[\"CLASS\"].map({0.0:False,1.0:True}) # Chaninging 0 to \"False\" 1 to \"True\"\n",
    "\n",
    "#KC.to_csv(\"KC_csv\") ## Writing a csv file\n",
    "#print(KC['CLASS'].unique())\n",
    "#print(KC['CLASS'].nunique())\n",
    "\n",
    "#printing the numbers of False and True\n",
    "#print(KC.groupby('CLASS').size()[0].sum()) #\n",
    "#print(KC.groupby('CLASS').size()[1].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "array2 = Train.values\n",
    "# separating array into input and output components\n",
    "X = array2[:,0:11]\n",
    "Y = array2[:,11]\n",
    "scaler = StandardScaler().fit(X)\n",
    "rescaledX = scaler.transform(X)\n",
    "# transformed data should be summarized using the code below\n",
    "set_printoptions(precision=3)\n",
    "print(rescaledX[0:5,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component analysis feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training the model using PCA as the feature of choice\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "array = Train.values\n",
    "X = array[:,0:11]\n",
    "Y = array[:,11]\n",
    "\n",
    "# feature extraction\n",
    "pca = PCA(n_components=3)\n",
    "fit = pca.fit(X)\n",
    "# summarize components\n",
    "print(\"Explained Variance: \" , fit.explained_variance_ratio_)\n",
    "print(fit.components_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output model using PCA as the feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model.predict(Test.values)\n",
    "\n",
    "KC2 = pd.DataFrame(out) #Converting the data to data frame\n",
    "KC2.columns=[\"CLASS\"] #Naming the column\n",
    "KC2.index.name=\"Index\" #Creating a column index\n",
    "KC2[\"CLASS\"]=KC2[\"CLASS\"].map({0.0:False,1.0:True}) # Chaninging 0 to \"False\" 1 to \"True\"\n",
    "\n",
    "KC2.to_csv(\"KC2_csv\") # obtaininng a csv file\n",
    "print(KC2['CLASS'].unique())\n",
    "print(KC2['CLASS'].nunique())\n",
    "\n",
    "#printing the numbers of False and True\n",
    "print(KC2.groupby('CLASS').size()[0].sum()) #\n",
    "print(KC2.groupby('CLASS').size()[1].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
