{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MARIA MAGDALENE NAMAGANDA\n",
    "# 2019/HD07/24853U\n",
    "# Ace_class Kaggle assignment_1"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing some of the needed packages and libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import pyplot\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from pandas import read_csv\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description of Libraries used.\n",
    "* Pandas is a library in Python for data manipulation and analysis. It offers data structures and operations for manipulating numerical tables and time series.\n",
    "* Matplotlib is a plotting library for the Python.\n",
    "* Seaborn is a Python data visualization library based on matplotlib. It provides a high-level interface for drawing attractive and informative statistical graphics.\n",
    "* K-Fold cross validation is where a given data set is split into a K number of sections/folds where each fold is used as a testing set at some point.\n",
    "* A confusion matrix is a table that is often used to describe the performance of a classification model on a set of test data for which the true values are known.\n",
    "* In Train/Test Split the data we use is usually split into training data and test data. The training set contains a known output and the model learns on this data in order to be generalized to other data later on\n",
    "* Logistic Regression is used for classification problems, it is a predictive analysis algorithm and based on the concept of probability.\n",
    "* A decision tree is a flowchart-like tree structure where an internal node represents feature(attribute), the branch represents a decision rule, and each leaf node represents the outcome.\n",
    "* KNeighbours Classifier is a non-parametric algorithm whose purpose is to use a database in which the data points are separated into several classes to predict the classification of a new sample point.\n",
    "* The Naive Bayes is a classification algorithm that is suitable for binary and multiclass classification. It performs well in cases of categorical input variables compared to numerical variables. \n",
    "* The linear Discriminant analysis estimates the probability that a new set of inputs belongs to every class.\n",
    "* SVC (Support Vector Classifier) is to fit the data you provide, returning a \"best fit\" hyperplane that divides, or categorizes your data.\n",
    "* The basic concept behind Adaboost is to set the weights of classifiers and training the data sample in each iteration such that it ensures the accurate predictions of unusual observations.\n",
    "* AdaBoost works by weighting the observations, putting more weight on difficult to classify instances and less on those already handled well.\n",
    "* ExtraTreesClassifier, like RandomForest, randomizes certain decisions and subsets of data to minimize over-learning from the data and overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Now i need to get rid of some unnecessary warnings by ignoring them*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To avoid unnecessary warnings, I will ignore them in the code below\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the datasets \n",
    "### There are two datasets given; \n",
    "1. AMP_TrainSet.csv and \n",
    "2. Test.csv  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "",
    "_uuid": ""
   },
   "outputs": [],
   "source": [
    "#Loading the datasets, \n",
    "\n",
    "Train = pd.read_csv(\"../Data/AMP_TrainSet.csv\")\n",
    "Test = pd.read_csv(\"../Data/Test.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For training the model, I will first use the training set which I will further split into the train and validation set. \n",
    "\n",
    "<div class = 'alert alert-info'>\n",
    "   @atwine: Did not explain why they are splitting, I need to see why they are doing this.\n",
    "   </div>\n",
    "### Then I will test the model on the Test set provided to gauge its performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring my data\n",
    "## Exploring data helps one understand what kind of data is given. That is to say knowing how much data it is, the shape, type, dimensions, missing values, data summary, correlation of attributes among others as am going to do in the following steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here, am trying to find the type of data\n",
    "type(Train)\n",
    "type(Test)\n",
    "Train.dtypes, Test.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the dimensions of the data\n",
    "# this retuns the number of rows and columns in the data\n",
    "\n",
    "Train.shape, Test.shape\n",
    "\n",
    "#this helps to know how big the data is in terms of rows and columns.\n",
    "#also from here I can tell which data is labeled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data description\n",
    ">For now, I will focus more on the train dataset because its what I will use to train the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting a description of the train dataset\n",
    "#description gives a summary of the data.\n",
    "\n",
    "Train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#looking at the first 5 entries of my data\n",
    "Train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Distribution\n",
    "> From the Train dataset, i see there is an extra 'CLASS' column which will be my validation set.\n",
    "### I need to know how this class is distributed to guide me on what to do with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to see the class distribution, I will plot a bar graph\n",
    "Train.groupby('CLASS').size().plot(kind='bar')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> I would like to know how many instaces i have for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the number of instances in each class\n",
    "Train.groupby('CLASS').size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There are 1519 intances for each class which is proof for even distribution.\n",
    "### From the above barplot and class instances, I can see that the classes are evenly distributed so no need to use smote."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Visualisation\n",
    "### Data visualization is the technique to present the data in a pictorial or graphical format. It enables one to analyze data visually. The data in a graphical format allows identification of new trends and patterns easily.\n",
    "### Visualisation identifies the relationship between data points and variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Density plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> I chose to use density plots because they are better at determing the distribution shape as they are not affected by the number of bins as is the case with Histograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now plotting density subplots\n",
    "#setting the figsize to 12 so that my graphs are not congested\n",
    "Train.plot(kind='density', subplots = True, layout=(3,4), sharex= False, sharey= False, figsize=(12,12))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> From the above density subpots, I can see that most of the data follows a Gaussian distribution given some of the characteristics such as bell shaped curves and graphs being symmetrical about the mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Box and whisker plots\n",
    "> A box plot is a type of graph that displays a summary of a large amount of data in terms of the median, upper quartile, lower quartile, minimum and maximum data values.\n",
    "\n",
    "> It handles large data easily, gives a clear summary and displays outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting a box and whisker graph\n",
    "#setting the figsize to 10 so that the plots are not congested.\n",
    "Train.plot(kind='box', subplots = True, layout=(3,4), sharex= False, sharey= False, figsize=(10,10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at the correlation of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Correlation is the relationship between two variables. The commonly used method is Pearson's correlation coefficient. It assumes a normal distribution of the attributes involved.\n",
    "\n",
    "> -1 shows full negative correlation, \n",
    "\n",
    "> +1 shows full negative correlation and \n",
    "\n",
    "> 0 shows no correlation at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first I will checkfor the pairwise correlation of the attributes.\n",
    "Train.corr(method='pearson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Then now am reviewing the inter-correlation of attributes using heatmap\n",
    "#graphical representation\n",
    "plt.figure(figsize=(6,6))\n",
    "sns.heatmap(Train.corr(method='pearson'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ill also check the correlation in regards to the 'CLASS' attribute\n",
    "Train.corr(method='pearson')['CLASS']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skewnwess of the data\n",
    "> knowing data skewness allows one to perform data preparation and improve a model\n",
    "\n",
    "> If Skewness value lies above +1 or below -1 then the data is highly skewed. \n",
    "\n",
    "> If skewness value lies between +0.5 and -0.5 then the data ids moderately skewed.\n",
    "\n",
    "> If skewness is 0 then data is symmetrical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking out skewness of data\n",
    "Train.skew().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing Machine Learning Algorithms\n",
    ">  This helps to choose the best model for the problem at hand.\n",
    "\n",
    ">  Using resampling methods like cross validation, gives an estimate for how accurate each model may be on unseen data.\n",
    "\n",
    ">  It is important to ensure that each algorithm is evaluated in the same way on the same data to avoid bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#comparing different models to from which ill choose.\n",
    "\n",
    "\n",
    "# load dataset\n",
    "\n",
    "array = Train.values\n",
    "\n",
    "#split the dataset \n",
    "X = array[:,0:11]  #X = Train.drop(columns=['CLASS'])\n",
    "Y = array[:,11]   #Y = Train['CLASS']\n",
    "\n",
    "# preparing models and adding them to a list\n",
    "models = []\n",
    "models.append(('LR', LogisticRegression()))\n",
    "models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "models.append(('KNN', KNeighborsClassifier()))\n",
    "models.append(('CART', DecisionTreeClassifier()))\n",
    "models.append(('NB', GaussianNB()))\n",
    "models.append(('SVM', SVC()))\n",
    "models.append(('AB', AdaBoostClassifier()))\n",
    "models.append(('GBC', GradientBoostingClassifier()))\n",
    "models.append(('EXT', ExtraTreesClassifier()))\n",
    "models.append(('RTC', RandomForestClassifier()))\n",
    "#models.append(('XGB', XGBClassifier)) \n",
    "#am commenting out XGB it does not seem to be a scikit-learn estimator as it does not implement a 'get_params' methods.\n",
    "\n",
    "# evaluating each model in turn\n",
    "results = []\n",
    "names = []\n",
    "\n",
    "for name, model in models:\n",
    "    kfold = KFold(n_splits=50, random_state=42)\n",
    "    cv_results = cross_val_score(model, X, Y, cv=kfold, scoring='accuracy')\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = (name, cv_results.mean(), cv_results.std())\n",
    "    print(msg)\n",
    "\n",
    "# boxplot algorithm comparison\n",
    "fig = pyplot.figure()\n",
    "fig.suptitle('Algorithm Comparison')\n",
    "ax = fig.add_subplot(111)\n",
    "pyplot.boxplot(results)\n",
    "ax.set_xticklabels(names)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## From the above algorithm comparison, I can see that Naive Bayes(NB), ExtraTreesClassifiers(EXT) and RandomForestClassifiers(RTC) are some of the best performing algorithms. \n",
    "> ## Am yet to find out the overall best valgorithm after prediction on the Test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection \n",
    "## Using recursive feature elimination\n",
    "> Sometimes, you may asses a dataset and find out that you do not need to use all the given features. This can be because some of them are highly correlates or they are not in any way helpful in developing the model.\n",
    "* It is therefore important to get rid of some features where applicable.\n",
    "\n",
    "> For this data, I will use Recursive Feature Elimination(RFE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature selection using RFE\n",
    "#first i will start with choosing 4 features\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "array = Train.values\n",
    "X = array[:,0:11]\n",
    "Y = array[:,11]\n",
    "# feature extraction\n",
    "model = LogisticRegression()\n",
    "rfe = RFE(model, 4)\n",
    "fit = rfe.fit(X, Y)\n",
    "print(\"Num Features: \", fit.n_features_)\n",
    "print(\"Selected Features:\", fit.support_)\n",
    "print(\"Feature Ranking: \", fit.ranking_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calling out the column names so I can know which features am going to drop from RFE\n",
    "Train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "array = Train.values\n",
    "X = array[:,0:11]\n",
    "Y = array[:,11]\n",
    "# feature extraction\n",
    "model = ExtraTreesClassifier()\n",
    "model.fit(X, Y)\n",
    "print(model.feature_importances_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If am to use 4 features and drop the rest\n",
    "> I will assign the new dataset a variable 'New_Train4' after choosing the 4 features and dropping the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I will call the new Train data with selected features New_Train4.\n",
    "Train\n",
    "New_Train4 = Train.drop(['FULL_Charge', 'FULL_AcidicMolPerc', 'FULL_DAYM780201', 'FULL_GEOR030101', 'AS_MeanAmphiMoment', 'AS_DAYM780201', 'AS_FUKS010112'], axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping the same features in the test dataset\n",
    "New_Test4 = Test.drop(['FULL_Charge', 'FULL_AcidicMolPerc', 'FULL_DAYM780201', 'FULL_GEOR030101', 'AS_MeanAmphiMoment', 'AS_DAYM780201', 'AS_FUKS010112'], axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#viewing the selected features\n",
    "New_Train4.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now splitting data\n",
    "#array = New_Train.values\n",
    "X = New_Train4.values[:,0:4]\n",
    "Y = New_Train4.values[:,4]\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_Train, X_Test, Y_Train, Y_Test = train_test_split(X, Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# also train and test the model on Matthews correlation coefficient.\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "\n",
    "GS = GaussianNB()\n",
    "GS.fit(X_Train,Y_Train)\n",
    "pred = GS.predict(X_Test)\n",
    "\n",
    "print(\"The result is: \",np.round(matthews_corrcoef(Y_Test,pred) *100,2),\" Mathew's Coef\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using four features gives me a very low MCC and low overall score.\n",
    "## I'll therefore consider using 8 features and see what score  I get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "array = Train.values\n",
    "X = array[:,0:11]\n",
    "Y = array[:,11]\n",
    "# feature extraction\n",
    "model = LogisticRegression()\n",
    "rfe = RFE(model, 8)\n",
    "fit = rfe.fit(X, Y)\n",
    "print(\"Num Features: \", fit.n_features_)\n",
    "print(\"Selected Features:\", fit.support_)\n",
    "print(\"Feature Ranking: \", fit.ranking_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I will call the new Train data with selected features New_Train8.\n",
    "Train\n",
    "New_Train8 = Train.drop(['FULL_AcidicMolPerc', 'FULL_DAYM780201', 'AS_DAYM780201'], axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping the same features in the test dataset\n",
    "New_Test8 = Test.drop(['FULL_AcidicMolPerc', 'FULL_DAYM780201', 'AS_DAYM780201'], axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now splitting data\n",
    "#array = New_Train.values\n",
    "\n",
    "\n",
    "X = New_Train8.values[:,0:8]\n",
    "Y = New_Train8.values[:,8]\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_Train, X_Test, Y_Train, Y_Test = train_test_split(X, Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now creating a model and training it on 8 features\n",
    "model = LogisticRegression(solver='liblinear', C=0.05, multi_class='ovr', random_state=30)\n",
    "model.fit(X_Train, Y_Train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_Train, X_Test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2,\n",
    "random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import matthews_corrcoef\n",
    "\n",
    "nv = GaussianNB()\n",
    "nv.fit(X_Train,Y_Train)\n",
    "pred = nv.predict(X_Test)\n",
    "\n",
    "print(\"The result is: \",np.round(matthews_corrcoef(Y_Test,pred) *100,2),\" Mathew's Coef\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardising data\n",
    "> This is useful to transform attributes with a Gaussian distribution and it workd better with rescaled data(also known as normalistaion where attributes are scaled into a range between 0 and 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Below are some of the algorithms I will be using;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LINEAR ALGORITHMS\n",
    "1. Linear Regression\n",
    "2. Logistic Regression\n",
    "3. Linear Discriminant Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression using 8 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here am using a dataset 'New_Train8' with 8 selected features\n",
    "array = New_Train8.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "\n",
    "kfold = KFold(n_splits=10, random_state=42) #spliiting my data into 10 folds and random_state of 42 for reproducibility\n",
    "model = LogisticRegression() #calling out the prediction algorithm\n",
    "results = cross_val_score(model, X, Y, cv=kfold) #estimating the model on new data and assigning it to results variable\n",
    "print(results.mean())  #getting the mean of the accuracy scores from cross validation scores\n",
    "\n",
    "model.fit(X,Y)\n",
    "output = model.predict(New_Test8.values) #testing the model on the test_set (Test.values)\n",
    "\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "mcc = matthews_corrcoef(model.predict(X),Y)\n",
    "print('MCC:',mcc)\n",
    "                       \n",
    "maria_logistic = pd.DataFrame(output)  #here we are converting the array into a pandas dataframe which will give a single column\n",
    "maria_logistic.columns = ['CLASS'] #renaming the output column to 'CLASS'\n",
    "maria_logistic.index.name = \"Index\"  #naming the index column as 'Index'\n",
    "maria_logistic['CLASS']=maria_logistic['CLASS'].map({0.0:False, 1.0:True}) #converting '0.0' to' False' and '1.0' to 'True'\n",
    "maria_logistic.to_csv(\"maria_logistic.csv\") #changing my output file as a 'csv' file\n",
    "\n",
    "print(maria_logistic['CLASS'].unique()) #checking out the unique instances in the 'CLASS' column\n",
    "print('False: ',maria_logistic.groupby('CLASS').size()[0].sum()) #summing up the '0' instances in the 'CLASS' column\n",
    "print('True: ',maria_logistic.groupby('CLASS').size()[1].sum()) #summing up the '1' instances in the 'CLASS' column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTE\n",
    "> I have noticed that for this particular dataset, the more I drop features, the prediction scores also keep decreasing.\n",
    "\n",
    "> With 4 features selected, the score was low, with 8 features, the score improved and with all features, the prediction score was high.\n",
    "\n",
    "> I therefore decided to work with all the features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying Linear Dicriminant Analysis (LDA) using all features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array = Train.values\n",
    "X = array[:,0:11]\n",
    "Y = array[:,11]\n",
    "num_folds = 10\n",
    "kfold = KFold(n_splits=10, random_state=42) #spliiting my data into 10 folds and random_state of 42 for reproducibility\n",
    "model = LinearDiscriminantAnalysis() #calling out the prediction algorithm\n",
    "results = cross_val_score(model, X, Y, cv=kfold) #estimating the model on new data and assigning it to results variable\n",
    "print(results.mean())  #getting the mean of the accuracy scores from cross validation scores\n",
    "\n",
    "model.fit(X,Y)\n",
    "output = model.predict(Test.values) #testing the model on the test_set (Test.values)\n",
    "\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "mcc = matthews_corrcoef(model.predict(X),Y)\n",
    "print('MCC:',mcc)\n",
    "                       \n",
    "maria_LDA = pd.DataFrame(output)  #here we are converting the array into a pandas dataframe which will give a single column\n",
    "maria_LDA.columns = ['CLASS'] #renaming the output column to 'CLASS'\n",
    "maria_LDA.index.name = \"Index\"  #naming the index column as 'Index'\n",
    "maria_LDA['CLASS']=maria_LDA['CLASS'].map({0.0:False, 1.0:True}) #converting '0.0' to' False' and '1.0' to 'True'\n",
    "maria_LDA.to_csv(\"maria_LDA.csv\") #changing my output file as a 'csv' file\n",
    "\n",
    "print(maria_LDA['CLASS'].unique())  #checking out the unique instances in the 'CLASS' column\n",
    "print('False: ',maria_LDA.groupby('CLASS').size()[0].sum()) #summing up the '0' instances in the 'CLASS' column\n",
    "print('True: ',maria_LDA.groupby('CLASS').size()[1].sum()) #summing up the '1' instances in the 'CLASS' column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NON LINEAR ALGORITHMS\n",
    "1. Naive Bayes\n",
    "2. Support Vector Machines\n",
    "3. K-Nearest Neighbours\n",
    "4. Classification and Regression Trees\n",
    "5. Learning Vector Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Naive Bayes and all the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array = Train.values\n",
    "X = array[:,0:11]\n",
    "Y = array[:,11]\n",
    "test_size = 0.35 \n",
    "\n",
    "kfold = KFold(n_splits=10) #spliiting my data into 10 folds \n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size, random_state=42) #random_state of 42 for reproducibility\n",
    "\n",
    "model = GaussianNB() #calling out the prediction algorithm\n",
    "results = cross_val_score(model, X, Y, cv=kfold) #estimating the model on new data and assigning it to results variable\n",
    "print(results.mean())  #getting the mean of the accuracy scores from cross validation scores\n",
    "\n",
    "\n",
    "model.fit(X,Y)\n",
    "output = model.predict(Test.values) #testing the model on the test_set (Test.values)\n",
    "\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "mcc = matthews_corrcoef(model.predict(X),Y)\n",
    "print('MCC:',mcc)\n",
    "                       \n",
    "maria2_bayes = pd.DataFrame(output)  #here we are converting the array into a pandas dataframe which will give a single column\n",
    "maria2_bayes.columns = ['CLASS'] #renaming the output column to 'CLASS'\n",
    "maria2_bayes.index.name = \"Index\"  #naming the index column as 'Index'\n",
    "maria2_bayes['CLASS']=maria2_bayes['CLASS'].map({0.0:False, 1.0:True}) #converting '0.0' to' False' and '1.0' to 'True'\n",
    "maria2_bayes.to_csv(\"maria2_bayes.csv\") #changing my output file as a 'csv' file\n",
    "\n",
    "\n",
    "print(maria2_bayes['CLASS'].unique()) #checking out the unique instances in the 'CLASS' column\n",
    "print('False: ',maria2_bayes.groupby('CLASS').size()[0].sum()) #summing up the '0' instances in the 'CLASS' column\n",
    "print('True: ',maria2_bayes.groupby('CLASS').size()[1].sum()) #summing up the '1' instances in the 'CLASS' column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines algorithm using all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array = Train.values\n",
    "X = array[:,0:11]\n",
    "Y = array[:,11]\n",
    "kfold = KFold(n_splits=10) #spliiting my data into 10 folds\n",
    "\n",
    "model = SVC() #calling out the prediction algorithm\n",
    "scoring = 'acuracy'\n",
    "results = cross_val_score(model, X, Y, cv=kfold) #estimating the model on new data and assigning it to results variable\n",
    "print(results.mean())  #getting the mean of the accuracy scores from cross validation scores\n",
    "\n",
    "model.fit(X, Y)\n",
    "output = model.predict(Test.values) #testing the model on the test_set (Test.values)\n",
    "\n",
    "mcc = matthews_corrcoef(model.predict(X), Y)\n",
    "print('MCC: ',mcc)\n",
    "\n",
    "maria_svc = pd.DataFrame(output)  #here we are converting the array into a pandas dataframe which will give a single column\n",
    "maria_svc.columns = ['CLASS'] #renaming the output column to 'CLASS'\n",
    "maria_svc.index.name = 'Index'  #naming the index column as 'Index'\n",
    "maria_svc['CLASS'] = maria_svc['CLASS'].map({0.0:False, 1.0:True}) #converting '0.0' to' False' and '1.0' to 'True'\n",
    "\n",
    "maria_svc.to_csv('maria_svc.csv') #changing my output file as a 'csv' file\n",
    "\n",
    "print(maria_svc['CLASS'].unique()) #checking out the unique instances in the 'CLASS' column\n",
    "print('False: ',maria_svc.groupby('CLASS').size()[0].sum()) #summing up the '0' instances in the 'CLASS' column\n",
    "print('True: ',maria_svc.groupby('CLASS').size()[1].sum()) #summing up the '1' instances in the 'CLASS' column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying Classification and Regression Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "array = Train.values\n",
    "X = array[:,0:11]\n",
    "Y = array[:,11]\n",
    "kfold = KFold(n_splits=10, random_state=42) #spliiting my data into 10 folds and random_state of 42 for reproducibility\n",
    "model = DecisionTreeClassifier() #calling out the prediction algorithm\n",
    "results = cross_val_score(model, X, Y, cv=kfold) #estimating the model on new data and assigning it to results variable\n",
    "print(results.mean())  #getting the mean of the accuracy scores from cross validation scores\n",
    "\n",
    "\n",
    "model.fit(X,Y)\n",
    "output = model.predict(Test.values) #testing the model on the test_set (Test.values)\n",
    "\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "mcc = matthews_corrcoef(model.predict(X),Y)\n",
    "print('MCC:',mcc)\n",
    "                       \n",
    "maria_tree = pd.DataFrame(output)  #here we are converting the array into a pandas dataframe which will give a single column\n",
    "maria_tree.columns = ['CLASS'] #renaming the output column to 'CLASS'\n",
    "maria_tree.index.name = \"Index\"  #naming the index column as 'Index'\n",
    "maria_tree['CLASS']=maria_tree['CLASS'].map({0.0:False, 1.0:True}) #converting '0.0' to' False' and '1.0' to 'True'\n",
    "maria_tree.to_csv(\"maria_tree.csv\") #changing my output file as a 'csv' file\n",
    "\n",
    "\n",
    "print(maria_tree['CLASS'].unique()) #checking out the unique instances in the 'CLASS' column\n",
    "print('False: ',maria_tree.groupby('CLASS').size()[0].sum()) #summing up the '0' instances in the 'CLASS' column\n",
    "print('True: ',maria_tree.groupby('CLASS').size()[1].sum()) #summing up the '1' instances in the 'CLASS' column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array = Train.values\n",
    "X = array[:,0:11]\n",
    "Y = array[:,11]\n",
    "\n",
    "kfold = KFold(n_splits=10, random_state=42) #spliiting my data into 10 folds and random_state of 42 for reproducibility\n",
    "model = KNeighborsClassifier() #calling out the prediction algorithm\n",
    "results = cross_val_score(model, X, Y, cv=kfold) #estimating the model on new data and assigning it to results variable\n",
    "print(results.mean()) #getting the mean of the accuracy scores from cross validation scores\n",
    "\n",
    "model.fit(X,Y) \n",
    "output = model.predict(Test.values) #testing the model on the test_set (Test.values)\n",
    "\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "mcc = matthews_corrcoef(model.predict(X),Y)\n",
    "print('MCC:',mcc)\n",
    "                       \n",
    "maria_knn = pd.DataFrame(output) #here we are converting the array into a pandas dataframe which will give a single column\n",
    "maria_knn.columns = ['CLASS'] #renaming the output column to 'CLASS' \n",
    "maria_knn.index.name = \"Index\" #naming the index column as 'Index'\n",
    "maria_knn['CLASS']=maria_knn['CLASS'].map({0.0:False, 1.0:True}) #converting '0.0' to' False' and '1.0' to 'True'\n",
    "maria_knn.to_csv(\"maria_knn.csv\") #changing my output file as a 'csv' file\n",
    "\n",
    "\n",
    "print(maria_knn['CLASS'].unique()) #checking out the unique instances in the 'CLASS' column\n",
    "print('False: ',maria_knn.groupby('CLASS').size()[0].sum()) #summing up the '0' instances in the 'CLASS' column\n",
    "print('True: ',maria_knn.groupby('CLASS').size()[1].sum()) #summing up the '1' instances in the 'CLASS' column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Esemble Algorithms\n",
    "1. Boosting and Adaboost\n",
    "2. Bagging and Random forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying Adaboost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array = Train.values\n",
    "\n",
    "X = array[:,0:11]\n",
    "Y = array[:,11]\n",
    "\n",
    "kfold = KFold(n_splits=10, random_state=42)\n",
    "\n",
    "model = AdaBoostClassifier(n_estimators=200, random_state=42) \n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "\n",
    "print(results.mean())\n",
    "\n",
    "\n",
    "test_set = Test.values\n",
    "model.fit(X, Y)\n",
    "output = model.predict(test_set)\n",
    "\n",
    "mcc = matthews_corrcoef(model.predict(X), Y)\n",
    "print('MCC: ',mcc)\n",
    "\n",
    "maria_AB= pd.DataFrame(output)\n",
    "maria_AB.columns = ['CLASS']\n",
    "maria_AB.index.name = 'Index'\n",
    "maria_AB['CLASS'] = maria_AB['CLASS'].map({0.0:False, 1.0:True})\n",
    "\n",
    "maria_AB.to_csv('maria_AB.csv')\n",
    "\n",
    "print(maria_AB['CLASS'].unique())\n",
    "print('False: ',maria_AB.groupby('CLASS').size()[0].sum())\n",
    "print('True: ',maria_AB.groupby('CLASS').size()[1].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array = Train.values\n",
    "\n",
    "X = array[:,0:11]\n",
    "Y = array[:,11]\n",
    "\n",
    "#num_trees = 200\n",
    "#seed =42\n",
    "\n",
    "kfold = KFold(n_splits=10, random_state=42)\n",
    "model = GradientBoostingClassifier(n_estimators=200, random_state=42)\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())\n",
    "\n",
    "model.fit(X,Y) \n",
    "output = model.predict(Test.values) #testing the model on the test_set (Test.values)\n",
    "\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "mcc = matthews_corrcoef(model.predict(X),Y)\n",
    "print('MCC:',mcc)\n",
    "                       \n",
    "maria_GBC = pd.DataFrame(output) #here we are converting the array into a pandas dataframe which will give a single column\n",
    "maria_GBC.columns = ['CLASS'] #renaming the output column to 'CLASS' \n",
    "maria_GBC.index.name = \"Index\" #naming the index column as 'Index'\n",
    "maria_GBC['CLASS']=maria_GBC['CLASS'].map({0.0:False, 1.0:True}) #converting '0.0' to' False' and '1.0' to 'True'\n",
    "maria_GBC.to_csv(\"maria_GBC.csv\") #changing my output file as a 'csv' file\n",
    "\n",
    "\n",
    "print(maria_GBC['CLASS'].unique()) #checking out the unique instances in the \n",
    "print('False: ',maria_GBC.groupby('CLASS').size()[0].sum()) #summing up the '0' instances in the 'CLASS' column\n",
    "print('True: ',maria_GBC.groupby('CLASS').size()[1].sum()) #summing up the '1' instances in the 'CLASS' column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Stochastic Gradient Boosting Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array = Train.values\n",
    "X = array[:,0:11]\n",
    "Y = array[:,11]\n",
    "\n",
    "kfold = KFold(n_splits=10, random_state=42)\n",
    "model = XGBClassifier(n_estimators=200, random_state=42)\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())\n",
    "\n",
    "model.fit(X,Y) \n",
    "output = model.predict(Test.values) #testing the model on the test_set (Test.values)\n",
    "\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "mcc = matthews_corrcoef(model.predict(X),Y)\n",
    "print('MCC:',mcc)\n",
    "                       \n",
    "maria_XGB = pd.DataFrame(output) #here we are converting the array into a pandas dataframe which will give a single column\n",
    "maria_XGB.columns = ['CLASS'] #renaming the output column to 'CLASS' \n",
    "maria_XGB.index.name = \"Index\" #naming the index column as 'Index'\n",
    "maria_XGB['CLASS']=maria_XGB['CLASS'].map({0.0:False, 1.0:True}) #converting '0.0' to' False' and '1.0' to 'True'\n",
    "maria_XGB.to_csv(\"maria_XGB.csv\") #changing my output file as a 'csv' file\n",
    "\n",
    "\n",
    "print(maria_GBC['CLASS'].unique()) #checking out the unique instances in the \n",
    "print('False: ',maria_XGB.groupby('CLASS').size()[0].sum()) #summing up the '0' instances in the 'CLASS' column\n",
    "print('True: ',maria_XGB.groupby('CLASS').size()[1].sum()) #summing up the '1' instances in the 'CLASS' column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Extra Trees Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array = Train.values\n",
    "\n",
    "X = array[:,0:11]\n",
    "Y = array[:,11]\n",
    "\n",
    "kfold = KFold(n_splits=10, random_state=42)\n",
    "model = ExtraTreesClassifier(n_estimators=200) # (max_features=11)\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())\n",
    "                             \n",
    "model.fit(X,Y) \n",
    "output = model.predict(Test.values) #testing the model on the test_set (Test.values)\n",
    "\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "mcc = matthews_corrcoef(model.predict(X),Y)\n",
    "print('MCC:',mcc)\n",
    "                       \n",
    "maria_ETX = pd.DataFrame(output) #here we are converting the array into a pandas dataframe which will give a single column\n",
    "maria_ETX.columns = ['CLASS'] #renaming the output column to 'CLASS' \n",
    "maria_ETX.index.name = \"Index\" #naming the index column as 'Index'\n",
    "maria_ETX['CLASS']=maria_ETX['CLASS'].map({0.0:False, 1.0:True}) #converting '0.0' to' False' and '1.0' to 'True'\n",
    "maria_ETX.to_csv(\"maria_ETX.csv\") #changing my output file as a 'csv' file\n",
    "\n",
    "\n",
    "print(maria_ETX['CLASS'].unique()) #checking out the unique instances in the \n",
    "print('False: ',maria_ETX.groupby('CLASS').size()[0].sum()) #summing up the '0' instances in the 'CLASS' column\n",
    "print('True: ',maria_ETX.groupby('CLASS').size()[1].sum()) #summing up the '1' instances in the 'CLASS' column                           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONCLUSION\n",
    "## From the algorithm comparisons and my prediction scores, Naive Bayes algorithm performed the best.\n",
    "## I think this is because the data was following a Gaussian distribution(normally distributed) as seen earlier from the density subplots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REFERENCES\n",
    "1. https://realpython.com/logistic-regression-python/\n",
    "2. https://www.tutorialspoint.com/machine_learning_with_python/machine_learning_with_python_extra_trees.htm\n",
    "3. https://stackabuse.com/gradient-boosting-classifiers-in-python-with-scikit-learn/\n",
    "4. https://machinelearningmastery.com/stochastic-gradient-boosting-xgboost-scikit-learn-python/\n",
    "5. https://machinelearningmastery.com/stochastic-gradient-boosting-xgboost-scikit-learn-python/\n",
    "6. https://machinelearningmastery.com/compare-machine-learning-algorithms-python-scikit-learn/\n",
    "7. https://machinelearningmastery.com/master-machine-learning-algorithms/\n",
    "8. https://machinelearningmastery.com/compare-machine-learning-algorithms-python-scikit-learn/\n",
    "9. https://github.com/search?q=data-analysis-and-visualization-with-python&type=Repositories\n",
    "10. https://stackabuse.com/applying-filter-methods-in-python-for-feature-selection/\n",
    "11. https://towardsdatascience.com/decision-tree-in-machine-learning-e380942a4c96\n",
    "12. https://machinelearningmastery.com/compare-machine-learning-algorithms-python-scikit-learn/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
