{"cells":[{"metadata":{},"cell_type":"markdown","source":"#### WASSWA RAZACK | MSBT | ML ALGORITHMS ASSIGNMENT 1 | 2019/HD07/24874U"},{"metadata":{},"cell_type":"markdown","source":"### Importing the necessary libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"#import the necessary libraries you are going to use\nimport warnings\nwarnings.filterwarnings('ignore')\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Loading the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"#reading the data\ntrain=pd.read_csv('../input/amp-data-set/AMP_TrainSet.csv')\ntest=pd.read_csv('../input/amp-data-set/Test.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### to view the training data"},{"metadata":{"trusted":true},"cell_type":"code","source":"#currently working on the training dataset to train the selected modules\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#getting the dimensions of the training dataset\ntrain.shape, test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"we observe that the training dataset has 3038 rows, and 12 columns, wheras the test dataset has 758 rows and 11 columns"},{"metadata":{},"cell_type":"markdown","source":"**to check for any missing data in the datasets**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Just to be sure I do not have 'NA' - blanks in the dataset\ntrain.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##checking for the columns of the data; this displays the columns of the dataset\ntrain.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The columns of the dataset are displayed as\n* FULL_Charge\n* FULL_AcidicMolPerc\n* FULL_AURR980107\n* FULL_DAYM780201\n* FULL_GEOR030101\n* FULL_OOBM850104\n* NT_EFC195\n* AS_MeanAmphuiMoment\n* AS_DAYM780201\n* AS_FUKS010112\n* CT_RACS820104\n* CLASS"},{"metadata":{"trusted":true},"cell_type":"code","source":"#checking datatypes \ntrain.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**observations**\n* all parameters' datatype are floats apart from NT_EFC195\n* the observations are in an array/dataframe\n"},{"metadata":{},"cell_type":"markdown","source":"## Descriptive statistics"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The command diplays the following basic statistics;\n* the counts - in this case 3038\n* the means \n* standard deviations\n* minimum and maximum values of each observation\n* 25th percentile\n* 50th percentile, and the \n* 75th percentile of all the observations in the dataset"},{"metadata":{},"cell_type":"markdown","source":"**Class Distibution**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#to observe the grouping basing on the outcome variable\ntrain.groupby('CLASS').size().plot(kind='bar')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Observations\n* I chose CLASS to be the outcome variable, and it's observed that the data is balanced between the outcome - thus no need for any manipulations."},{"metadata":{},"cell_type":"markdown","source":"### Pairwise-Correlation between the attributes"},{"metadata":{"trusted":true},"cell_type":"code","source":"#to have the correlations across all the observations in the dataset\ntrain.corr(method='pearson')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Observations:**\n\nThe pairwise correlations between all the attributes are observed.\nNotably:\nthere is a string correlation between the following\n* FULL_Charge & FULL_AcidicMolPerc (-0.6)\n* FULL_AcidicMolPerc & FULL_AURR980107 (0.79)\n* FULL_DAYM780201 & AS_DAYM780201"},{"metadata":{"trusted":true},"cell_type":"code","source":"# to find the correlations withrespect to the outcome variable\ntrain.corr(method='pearson')['CLASS']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#to view the relationship of the main parameters that have a correlation at least 0.5\ntrain[['FULL_Charge','FULL_AcidicMolPerc','FULL_AURR980107','FULL_DAYM780201','AS_MeanAmphiMoment']].head(10).plot.bar(title='main players in the data')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It's observed that **FULL_Charge, FULL_AcidicMolPerc FULL_AURR980107, FULL_DAYM780201, AS_MeanAmphiMoment** have correlations of **0.53, -0.59, -0.58, 0.55 & 0.69** respectively "},{"metadata":{"trusted":true},"cell_type":"code","source":"#the heat map to show the correlation of the data\nplt.figure(figsize=(10,10))\nsns.heatmap(train.corr(method='pearson'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#checking for skewness in the data\ntrain.skew().plot(kind='bar')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The skewness of the data tells how the raw data is distributed, just in case there is need for any  data pre processing\nA skew of 0 indicates the data is symmetrical\nThe skew can either be positive or negative\n\nA negative skew indicates the tail of the data is on the negative side of the mean.\nA positive skew indicates the tail of the data is on the positive side of the mean."},{"metadata":{},"cell_type":"markdown","source":"## Data visualizations"},{"metadata":{},"cell_type":"raw","source":"train[['FULL_Charge','FULL_AcidicMolPerc','FULL_AURR980107','FULL_DAYM780201','AS_MeanAmphiMoment']].head(10).plot.box(title='main players in the data')"},{"metadata":{},"cell_type":"markdown","source":"### density plots"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.plot(kind='density',subplots=True, layout=(3,4), sharex=False, figsize=(10,10))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The density plots demonstrate most of the data is follows Guassian distribution."},{"metadata":{},"cell_type":"markdown","source":"### Box and Whisker plots"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.plot(kind='box', subplots=True, layout=(3,4), sharex=False, sharey=False, figsize=(12,12))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature Selection; using the filter method"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,10))\ncor=train.corr()\nsns.heatmap(cor,annot=True, cmap=plt.cm.Reds)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#considering CLASS as my output variable\n#corelation with the output variable - CLASS\ntarget=abs(cor['CLASS'])\n\n#selecting highly correlated features\nrelevant_features=target[target>0.5]\nrelevant_features\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#to check the correlations between the selected features above:\n\nprint(train[['FULL_Charge','FULL_AcidicMolPerc']].corr())\nprint(train[['FULL_AcidicMolPerc','FULL_Charge']].corr())\nprint(train[['FULL_AcidicMolPerc','FULL_AURR980107']].corr())\nprint(train[['FULL_AcidicMolPerc','FULL_DAYM780201']].corr())\nprint(train[['FULL_AURR980107','FULL_DAYM780201']].corr())\nprint(train[['FULL_DAYM780201','FULL_AURR980107']].corr())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- from the above analysis, I discover that FULL_AURR980107 and FULL_AcidicMolPer are highly correlated with each other 0.794796. I will keep FULL_AcidicMolPerc since it has a higher correlation with the outcome variable CLASS\n- I dropped FULL_charge for the same reason\n\n\n- after dropping FULL_Charge and FULL_AURR980107 I remain with three features;\n\n\n1.  FULL_AcidicMolPerc\n2.  FULL_DAYM780201\n3.  AS_MeanAmpiMoment\n\nsource: [https://towardsdatascience.com/feature-selection-with-pandas-e3690ad8504b](http://)"},{"metadata":{},"cell_type":"markdown","source":"### Running regreression on the training data using the selected features"},{"metadata":{"trusted":true},"cell_type":"code","source":"# to drop all attributes and retain only the selected features\nnew_train = train.drop(['FULL_Charge','FULL_AURR980107', 'FULL_GEOR030101','FULL_OOBM850104','NT_EFC195','CT_RACS820104','AS_DAYM780201', 'AS_FUKS010112'], axis =1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#to show the selected features\nnew_train.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_train.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Logistic Regression on the selectd features only"},{"metadata":{"trusted":true},"cell_type":"code","source":"#tesing run using the selected features on Logistic regression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\n\narray = new_train.values\nX = array[:,0:3]\nY = array[:,3]\ntest_size = 0.40\nseed = 42\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size,\nrandom_state=seed)\nmodel = LogisticRegression()\nmodel.fit(X_train, Y_train)\nresult = model.score(X_test, Y_test)\nprint(\"Accuracy: \",  (result*100.0))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Logistic Regression on Complete data"},{"metadata":{"trusted":true},"cell_type":"code","source":"#running using the complete training dataset\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\n\narray = train.values\nX = array[:,0:11]\nY = array[:,11]\ntest_size = 0.40\nseed = 42\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size,\nrandom_state=seed)\nmodel = LogisticRegression()\nmodel.fit(X_train, Y_train)\nresult = model.score(X_test, Y_test)\nprint(\"Accuracy: \",  (result*100.0))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The accuracy decreases when the selected features are used; thus no need to drop. Will use the complete dataset henceforth."},{"metadata":{},"cell_type":"markdown","source":"**Support Vector Machine Model**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\n\narray = train.values\nX = array[:,0:11]\nY = array[:,11]\nkfold = KFold(n_splits=10, random_state=7)\nmodel = SVC()\nscoring='accuracy'\nresults = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\nprint(results.mean()*100.0,)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Summing ALL of the Algorithms **\n* Logistic Regression\n* Linear Discriminant Analysis\n* Quadratic Discriminant Analysis\n* K Neighbor Classifier\n* MLP Classifier\n* Desicion Trees Classifier\n* Guassian Naive Bayes\n* Suport Vector Machine\n* SGD Classifier\n* Random Forest Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"# importing all necessary packages for the models\nfrom sklearn.ensemble import RandomForestClassifier\nfrom pandas import read_csv\nfrom matplotlib import pyplot\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.neural_network import MLPClassifier\n\n#Load the data\narray=new_train.values\n#split the dataset \nX = array[:,0:3]\nY = array[:,3]\n\n# prepare models and add them to a list\nmodels = []\nmodels.append(('LR', LogisticRegression()))\nmodels.append(('LDA', LinearDiscriminantAnalysis()))\nmodels.append(('QDA', QuadraticDiscriminantAnalysis()))\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('MLPC', MLPClassifier()))\nmodels.append(('CART', DecisionTreeClassifier()))\nmodels.append(('NB', GaussianNB()))\nmodels.append(('SVM', SVC()))\nmodels.append(('SGD', SGDClassifier()))\nmodels.append(('RF', RandomForestClassifier()))\n\n\n# evaluate each model in turn\nresults = []\nnames = []\nscoring = 'accuracy'\n\nfor name, model in models:\n    kfold = KFold(n_splits=10, random_state=7)\n    cv_results = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n    results.append(cv_results)\n    names.append(name)\n    msg = (name, cv_results.mean(), cv_results.std())\n    print(msg)\n\n# boxplot algorithm comparison\nfig = pyplot.figure()\nfig.suptitle('Algorithm Comparison')\nax = fig.add_subplot(111)\npyplot.boxplot(results)\nax.set_xticklabels(names)\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# importing all necessary packages for the models\nfrom sklearn.ensemble import RandomForestClassifier\nfrom pandas import read_csv\nfrom matplotlib import pyplot\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.neural_network import MLPClassifier\n\n#Load the data\narray=train.values\n#split the dataset \nX = array[:,0:11]\nY = array[:,11]\n\n# prepare models and add them to a list\nmodels = []\nmodels.append(('LR', LogisticRegression()))\nmodels.append(('LDA', LinearDiscriminantAnalysis()))\nmodels.append(('QDA', QuadraticDiscriminantAnalysis()))\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('MLPC', MLPClassifier()))\nmodels.append(('CART', DecisionTreeClassifier()))\nmodels.append(('NB', GaussianNB()))\nmodels.append(('SVM', SVC()))\nmodels.append(('SGD', SGDClassifier()))\nmodels.append(('RF', RandomForestClassifier()))\n\n\n# evaluate each model in turn\nresults = []\nnames = []\nscoring = 'accuracy'\n\nfor name, model in models:\n    kfold = KFold(n_splits=10, random_state=7)\n    cv_results = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n    results.append(cv_results)\n    names.append(name)\n    msg = (name, cv_results.mean(), cv_results.std())\n    print(msg)\n\n# boxplot algorithm comparison\nfig = pyplot.figure()\nfig.suptitle('Algorithm Comparison')\nax = fig.add_subplot(111)\npyplot.boxplot(results)\nax.set_xticklabels(names)\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Developing a Model using Naive Bayes**"},{"metadata":{},"cell_type":"markdown","source":"I choose Naive Bayes since it had the highest accuracy from all the tested runs;"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\n\narray = train.values\nX = array[:,0:11]\nY = array[:,11]\n\nnum_folds = 10 #number of folds to use\nseed = 42 #reproducibility\n\nkfold = KFold(n_splits=num_folds, random_state=seed)\nmodel = GaussianNB()\nresults = cross_val_score(model, X, Y, cv=kfold)\n\nprint(f\"Accuracy:\", (results.mean()*100.0, results.std()*100.0))\n\n#fitting the model\nmodel.fit(X,Y)\n\narray_test=test.values\n\nClass=model.predict(array_test[:,0:11])\n\n\n\nreport=pd.DataFrame(Class)\nreport.columns=['CLASS']\nreport.index.name='Index'\nreport['CLASS']=report['CLASS'].map({0.0:False, 1.0:True})\nreport\n\n\nreport.to_csv('OUTPUT.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the analysis, Naive Bayes algorithm gave the best metrics."},{"metadata":{},"cell_type":"markdown","source":"References\n\nhttps://www.mathsisfun.com/data/skewness.html\n\nhttps://towardsdatascience.com/feature-selection-with-pandas-e3690ad8504b](http://)[](http://)"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}