{"cells":[{"metadata":{},"cell_type":"markdown","source":"# ACE CLASS ASSIGNMENT\nby `Davis Kiberu`  \n2019/HD07/24872U\n"},{"metadata":{},"cell_type":"markdown","source":"## Outline:\n  1. Import required packages\n  2. Reading in Datasets\n  3. Data exploration\n  4. Descriptive statistics for the data\n  5. Class distribution of the data \n  6. Checking for correlation between the different attributes\n  7. Cheking for skewdness of the data\n  8. Understanding Data with visualization\n  9. Preparing Data for Machine Learning\n  10. Feature selection\n  11. Predictions by algorithm\n  12. Improving performance with Ensembles\n  13. Comparing Machine Learning Algorithms used"},{"metadata":{},"cell_type":"markdown","source":"## 1. Import required packages\nFor this assignment, the required packages and modules are;\n* numpy;for computing scientific/mathematical data.\n* pandas; for data wrangling and manipulation\n* seaborn; for statistical data visualization\n* matplotlib.pyplot;  for plots"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Remove unnecessary warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n# Any results you write to the current directory are saved as output.","execution_count":52,"outputs":[{"output_type":"stream","text":"/kaggle/input/ace-class-assignment/AMP_TrainSet.csv\n/kaggle/input/ace-class-assignment/Test.csv\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## 2. Reading in Datasets\nComma Seperated Variables(CSV) data files are read into DataFrame objects using Pandas' read_csv() method.\nTwo datasets are read in: \n* The train set assigned to train\n* The test set assigned to test"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Reading in Datasets\ntrain = pd.read_csv(\"../input/ace-class-assignment/AMP_TrainSet.csv\")\ntest = pd.read_csv(\"../input/ace-class-assignment/Test.csv\")","execution_count":53,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. Data exploration\nTo get a feel of the data, we visualize a few of the rows(instances) and columns(attributes) of the data using the `head()` function which shows the first five rows.\n\na) We the check for the dimensions of the data using the `shape` property of the data which returns a tuple indicating number of rows and columns.\n\nb) We can then explore the names of the various attributes using the columns property and then check for the data type of each attribute using the `dtypes` property.\n\nc) We can also check for missing values (NAs and NaNs) using the `isnull()` function"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Viewing the first five rows of the data\ntrain.head()","execution_count":54,"outputs":[{"output_type":"execute_result","execution_count":54,"data":{"text/plain":"   FULL_Charge  FULL_AcidicMolPerc  FULL_AURR980107  FULL_DAYM780201  \\\n0          5.0               0.000            0.951           74.842   \n1          4.0               5.405            0.931           71.595   \n2          5.5               5.405            0.873           73.595   \n3          5.0               4.167            0.895           66.250   \n4          7.5               8.537            0.932           64.720   \n\n   FULL_GEOR030101  FULL_OOBM850104  NT_EFC195  AS_MeanAmphiMoment  \\\n0            0.975           -3.663          0               0.282   \n1            0.957           -4.011          1               0.600   \n2            0.961           -2.512          0               0.593   \n3            0.999           -1.362          0               0.614   \n4            0.979           -2.091          0               0.616   \n\n   AS_DAYM780201  AS_FUKS010112  CT_RACS820104  CLASS  \n0         73.444          5.661          1.041      1  \n1         68.222          6.537          1.453      1  \n2         69.444          4.934          1.722      1  \n3         67.222          4.316          1.382      1  \n4         72.944          4.540          1.539      1  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>FULL_Charge</th>\n      <th>FULL_AcidicMolPerc</th>\n      <th>FULL_AURR980107</th>\n      <th>FULL_DAYM780201</th>\n      <th>FULL_GEOR030101</th>\n      <th>FULL_OOBM850104</th>\n      <th>NT_EFC195</th>\n      <th>AS_MeanAmphiMoment</th>\n      <th>AS_DAYM780201</th>\n      <th>AS_FUKS010112</th>\n      <th>CT_RACS820104</th>\n      <th>CLASS</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>5.0</td>\n      <td>0.000</td>\n      <td>0.951</td>\n      <td>74.842</td>\n      <td>0.975</td>\n      <td>-3.663</td>\n      <td>0</td>\n      <td>0.282</td>\n      <td>73.444</td>\n      <td>5.661</td>\n      <td>1.041</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4.0</td>\n      <td>5.405</td>\n      <td>0.931</td>\n      <td>71.595</td>\n      <td>0.957</td>\n      <td>-4.011</td>\n      <td>1</td>\n      <td>0.600</td>\n      <td>68.222</td>\n      <td>6.537</td>\n      <td>1.453</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5.5</td>\n      <td>5.405</td>\n      <td>0.873</td>\n      <td>73.595</td>\n      <td>0.961</td>\n      <td>-2.512</td>\n      <td>0</td>\n      <td>0.593</td>\n      <td>69.444</td>\n      <td>4.934</td>\n      <td>1.722</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>5.0</td>\n      <td>4.167</td>\n      <td>0.895</td>\n      <td>66.250</td>\n      <td>0.999</td>\n      <td>-1.362</td>\n      <td>0</td>\n      <td>0.614</td>\n      <td>67.222</td>\n      <td>4.316</td>\n      <td>1.382</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7.5</td>\n      <td>8.537</td>\n      <td>0.932</td>\n      <td>64.720</td>\n      <td>0.979</td>\n      <td>-2.091</td>\n      <td>0</td>\n      <td>0.616</td>\n      <td>72.944</td>\n      <td>4.540</td>\n      <td>1.539</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":55,"outputs":[{"output_type":"execute_result","execution_count":55,"data":{"text/plain":"   FULL_Charge  FULL_AcidicMolPerc  FULL_AURR980107  FULL_DAYM780201  \\\n0          4.0               3.704            0.873           73.519   \n1          4.0               4.444            0.892           62.444   \n2          2.0               0.000            0.901           47.000   \n3          4.5               0.000            0.869           69.222   \n4         -4.0              21.591            1.061           71.682   \n\n   FULL_GEOR030101  FULL_OOBM850104  NT_EFC195  AS_MeanAmphiMoment  \\\n0            0.987           -4.833          0               0.382   \n1            0.931           -0.584          0               0.320   \n2            1.039           -5.664          0               0.164   \n3            0.982           -5.423          0               2.010   \n4            0.976           -2.002          0               2.758   \n\n   AS_DAYM780201  AS_FUKS010112  CT_RACS820104  \n0         74.556          7.225          1.234  \n1         56.056          4.942          1.853  \n2         47.000          5.969          1.174  \n3         69.222          5.462          1.138  \n4         66.000          5.582          1.453  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>FULL_Charge</th>\n      <th>FULL_AcidicMolPerc</th>\n      <th>FULL_AURR980107</th>\n      <th>FULL_DAYM780201</th>\n      <th>FULL_GEOR030101</th>\n      <th>FULL_OOBM850104</th>\n      <th>NT_EFC195</th>\n      <th>AS_MeanAmphiMoment</th>\n      <th>AS_DAYM780201</th>\n      <th>AS_FUKS010112</th>\n      <th>CT_RACS820104</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>4.0</td>\n      <td>3.704</td>\n      <td>0.873</td>\n      <td>73.519</td>\n      <td>0.987</td>\n      <td>-4.833</td>\n      <td>0</td>\n      <td>0.382</td>\n      <td>74.556</td>\n      <td>7.225</td>\n      <td>1.234</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4.0</td>\n      <td>4.444</td>\n      <td>0.892</td>\n      <td>62.444</td>\n      <td>0.931</td>\n      <td>-0.584</td>\n      <td>0</td>\n      <td>0.320</td>\n      <td>56.056</td>\n      <td>4.942</td>\n      <td>1.853</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2.0</td>\n      <td>0.000</td>\n      <td>0.901</td>\n      <td>47.000</td>\n      <td>1.039</td>\n      <td>-5.664</td>\n      <td>0</td>\n      <td>0.164</td>\n      <td>47.000</td>\n      <td>5.969</td>\n      <td>1.174</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4.5</td>\n      <td>0.000</td>\n      <td>0.869</td>\n      <td>69.222</td>\n      <td>0.982</td>\n      <td>-5.423</td>\n      <td>0</td>\n      <td>2.010</td>\n      <td>69.222</td>\n      <td>5.462</td>\n      <td>1.138</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-4.0</td>\n      <td>21.591</td>\n      <td>1.061</td>\n      <td>71.682</td>\n      <td>0.976</td>\n      <td>-2.002</td>\n      <td>0</td>\n      <td>2.758</td>\n      <td>66.000</td>\n      <td>5.582</td>\n      <td>1.453</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### a) Data dimensions"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking for number of rows and columns in the datasets\ntrain.shape, test.shape","execution_count":56,"outputs":[{"output_type":"execute_result","execution_count":56,"data":{"text/plain":"((3038, 12), (758, 11))"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"### b) Data type for each attribute"},{"metadata":{"trusted":true},"cell_type":"code","source":"# First lets take a look at the column names.\ntrain.columns","execution_count":57,"outputs":[{"output_type":"execute_result","execution_count":57,"data":{"text/plain":"Index(['FULL_Charge', 'FULL_AcidicMolPerc', 'FULL_AURR980107',\n       'FULL_DAYM780201', 'FULL_GEOR030101', 'FULL_OOBM850104', 'NT_EFC195',\n       'AS_MeanAmphiMoment', 'AS_DAYM780201', 'AS_FUKS010112', 'CT_RACS820104',\n       'CLASS'],\n      dtype='object')"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.columns","execution_count":58,"outputs":[{"output_type":"execute_result","execution_count":58,"data":{"text/plain":"Index(['FULL_Charge', 'FULL_AcidicMolPerc', 'FULL_AURR980107',\n       'FULL_DAYM780201', 'FULL_GEOR030101', 'FULL_OOBM850104', 'NT_EFC195',\n       'AS_MeanAmphiMoment', 'AS_DAYM780201', 'AS_FUKS010112',\n       'CT_RACS820104'],\n      dtype='object')"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Next, determine the data type for each column\ntrain.dtypes","execution_count":59,"outputs":[{"output_type":"execute_result","execution_count":59,"data":{"text/plain":"FULL_Charge           float64\nFULL_AcidicMolPerc    float64\nFULL_AURR980107       float64\nFULL_DAYM780201       float64\nFULL_GEOR030101       float64\nFULL_OOBM850104       float64\nNT_EFC195               int64\nAS_MeanAmphiMoment    float64\nAS_DAYM780201         float64\nAS_FUKS010112         float64\nCT_RACS820104         float64\nCLASS                   int64\ndtype: object"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.dtypes","execution_count":60,"outputs":[{"output_type":"execute_result","execution_count":60,"data":{"text/plain":"FULL_Charge           float64\nFULL_AcidicMolPerc    float64\nFULL_AURR980107       float64\nFULL_DAYM780201       float64\nFULL_GEOR030101       float64\nFULL_OOBM850104       float64\nNT_EFC195               int64\nAS_MeanAmphiMoment    float64\nAS_DAYM780201         float64\nAS_FUKS010112         float64\nCT_RACS820104         float64\ndtype: object"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"### c) Missing values\nMissing values include the standard `NaN` and `Na` "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check for missing values.\ntrain.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.isnull().sum().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Another way to check for null values and datatypes is using the .info() function. \ntest.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# visualization to check for missing data\nimport missingno as msno\nmsno.bar(train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The above plot indicates that there are no missing values in the `train` dataset"},{"metadata":{},"cell_type":"markdown","source":"In summary, the train data has 3028 instances and 12 attributes while the test dataset has 758 instances and 11 attributes. \nThe data types for each instance are numerical i.e. integers(int64) and floating point numbers(float64).\nThere are no missing values in the datasets."},{"metadata":{},"cell_type":"markdown","source":"## 4. Descriptive statistics for the data\nFor descriptive statistics, a summary is obtained using the `describe()` function.\nThe statistics summary show the counts for each attribtute, the mean, standard deviation, the minimum value for numerical atrributes, the 25th, 50th and 75th percentile for each numeric attribute andthe maximum value. \n\nThis offers insight into the spread of the data and represents the data in a form that is readily understandable."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Obtain summary statistics\ntrain.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5. Class distribution of the data\nThe data being used has a `CLASS` attribute.\nWe therefore have to determine the number of clases, their labels/values and distribution of observations within these classes."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking for the number of classes in the data\ntrain['CLASS'].nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking for the class values\ntrain['CLASS'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# There are two classes i.e 1 and 0.\n# checking for number of instances in each class \nclass1 = train.groupby('CLASS').size()[0].sum()\nclass0 = train.groupby('CLASS').size()[1].sum()\nprint(\"There are %s instances in class 1 and %s instances in class 0.\" % (class1, class0))\n# Visualizing the distribution of the data in the different classes\ntrain.groupby('CLASS').size().plot(kind='bar', color=('skyblue', 'pink'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The Classes 1 and 0 have equal instances and as such our observations are balanced.  \nDue to this uniform distribution, chances of algorithm bias are reduced."},{"metadata":{},"cell_type":"markdown","source":"## 6. Checking for correlation between the different attributes\nCorrelation helps us determine how the different attributes relate with each other. It also helps us when deciding which attributes to select especially in the presence of highly correlated attributes in which case one of the two attributes would be sufficient.\n\nThe `corr` function is used with Pearson's correlation method since most of the attributes are continous with the exception of `NT_EFC195` and `CLASS`.\n "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Obtaining correlation coefficient values for the different attribute pairs\ntrain.corr(method='pearson')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Visualizing the correlation of attribute pairs using a heatmap\nplt.figure(figsize=(6,6))\nsns.heatmap(train.corr(method='pearson'),cmap='coolwarm')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# correlation of the different attributes with the CLASS attribute\ntrain.corr(method='pearson')['CLASS']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualizing correlation of the different attributes with the CLASS attribute\ntrain.corr(method='pearson')['CLASS'].plot(kind='bar', color=('skyblue'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the graph above, it can be observed that most attributes are negatively correlated with the `CLASS` attribute.\n\nCorrelation does not imply causation but highly correlated features to the outcome or class could be indicative of the outcome's cause. \nCorrelation can be useful in feature selection since attributes that have a very low correlation with the class can be considered non-informative and as such discarded."},{"metadata":{},"cell_type":"markdown","source":"## 7. Cheking for skewdness of the data\nA skew is a deviation from the normal/Gausian ditribution such that the bell shaped curve is shifted to the right or the left.\n* Positively skewed: If the tail is on the right.\n* Negatively skewed: If the tail is on the left.\n\nTo check for skewdness of the data, the `skew()` function is used."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking if the data is normally distributed\ntrain.skew().plot(kind='bar', color='orange')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the plot it can be observed that most of the attributes are positively skewed with the exception of attributes 4,6,9 and 10. \n\nSkewed data can affect the performance of a machine learning algorithm and as such skewed data should be normalized using different transformation methods such as square transformation for negatively skewed data and log transfomations."},{"metadata":{},"cell_type":"markdown","source":"## 8. Understanding Data with Visualization\nIn order to better understand the data, different visualizations can be used.\nThis can be done using;\n\n#### a) Univariate plots\nThese are plots for singular variables per plot.\n* Histograms\n* Density plots\n* Box plots\n\n#### b) Multivariate plots\nThese represent more than one variable per plot.\n* Correlation Matrix Plot\n* Scatter Plot Matrix"},{"metadata":{},"cell_type":"markdown","source":"### a) Univariate plots"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Histograms\nplt.figure(figsize=(15,15))\ntrain.hist(color='orange')\n\nplt.subplots_adjust(bottom=1, right=2, top=3)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above plot, the distribution of the attributes can be observed.\nNoteworthy is the `CLASS` attribute which is categorical and uniformly distributed and the `NT_EFC195`attribute which also appears categorical but not uniformly distributed."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Zooming in on the NT_EFC195 attribute \ncont = train['NT_EFC195'].unique()\nprint('The NT_EFC195 attribute has 2 values: %s and %s.' % (cont[0],cont[1]))\ngp0 = train.groupby('NT_EFC195').size()[0].sum()\ngp1 = train.groupby('NT_EFC195').size()[1].sum()\nprint(\"There are %s instances with 0's and %s instances with 1's in the NT_EFC195 attribute.\" % (gp0, gp1))\n# Visualizing the distribution of the instances in the NT_EFC195 attribute \ntrain.groupby('NT_EFC195').size().plot(kind='bar', color=('orange'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above plot indicates that the variable NT_EFC195 is not uniformly distributed between the classes."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Density plots\ntrain.plot(kind='density', subplots=True, layout=(4,3), sharex=False)\nplt.subplots_adjust(bottom=1, right=2, top=3)\nplt.show","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The density plots convey the same information as the histograms. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Box plots\ntrain.plot(kind='box', subplots=True, layout=(4,3), sharex=False,sharey=False)\nplt.subplots_adjust(bottom=1, right=2, top=3)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Box and whisker plots are used to review the distribution of each attribute while indicating the median, 25th and 75th percentiles and the outliers for each attribute."},{"metadata":{},"cell_type":"markdown","source":"#### b) Multivariate plots"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualizing using scatter plot matrix\nsns.pairplot(train, hue='CLASS',vars=['FULL_Charge','FULL_AcidicMolPerc'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The above graphs show the distribution of the 2 classes within the features FULL_Charge and FULL_AcidicMolPerc.\nThis plot can be used in selection of features that clearly distinguish between the 2 classes."},{"metadata":{},"cell_type":"markdown","source":"# 9. Preparing Data for Machine Learning\n## Rescaling data\nSince the data has attributes with varying scales,rescalling is done so the attributes have the same scale.\nFrom `numpy`, the `set_printoptions` module is imported to determine the display of number of floating point values other Nump objects and from `scikit learn preprocessing` module, the `MinMaxScaler` option is imported to rescale the data. \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from numpy import set_printoptions\nfrom sklearn.preprocessing import MinMaxScaler\n\narray = train.values\n# seperate array into input, X and output, Y components\nX = array[:,0:11]\nY = array[:,11]\nscaler = MinMaxScaler(feature_range=(0,1))\n# set range of the scale to between 0 and 1.\nrescaledX = scaler.fit_transform(X)\n# summarize transformed data\nset_printoptions(precision=3)\nprint(rescaledX[0:5,:])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the end, no rescaling of data was done since predictions with rescaled data had a poor result."},{"metadata":{},"cell_type":"markdown","source":"#### PCA plot\nPrincipal component analysis is a dimensionality reduction technique. It uses linear algebra to transform the dataset into a compressed form."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nimport seaborn as sns\n\nX = array[:,0:11]\nY = array[:,11]\n\nhigh_dim = train.drop(columns='CLASS')\n\nscaler = StandardScaler()\n\nscaler.fit(high_dim)\nscaled_data = scaler.transform(high_dim)\n\nfrom sklearn.decomposition import PCA\npca = PCA(n_components=2)\npca.fit(scaled_data)\n\nx_pca = pca.transform(scaled_data)\n\nplt.figure(figsize=(8,6))\nplt.scatter(x_pca[:,0], x_pca[:,1], c=train['CLASS'], cmap='Spectral')\nplt.xlabel('First Principal Component')\nplt.ylabel('First Principal Component')\nlabels = np.unique(Y)\nplt.legend(labels)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the pca plot, 2 distinct classes are obseved. Noteworthy is the class distribution and region of overlap between the two classes.  \nThe overlap problem is a critical problem in which instances appear as valid for more than one class which may be responsible for the presence of noise in datasets and thus affect the performance of the learning algorithm (Gupta and Gupta, 2018)."},{"metadata":{},"cell_type":"markdown","source":"## 10. Feature selection\nPerformance of machine learning algorithms is dependent on selected features. Highly informative features improve model performance while irrelevant, partially irrelevant and redundant features negatively impact model performance.\n\nFeature selection reduces overfitting,improves accuracy and reduces training time. \n\nFeature selection can be done using:\n* Univariate Selection\n* Recursive Feature Elimination\n* Principle Component Analysis\n* Feature Importance\n\nFor this assignment, `Recursive Feature Elimination (RFE)` is used.\n`RFE` works by recursively removing attributes and building a model on those that remain. It uses the model accuracy to indentify the attributes, and their combinations, that contribute the most to predicting the target attribute."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LogisticRegression\n\nmodel = LogisticRegression()\nrfe = RFE(model,8)\nfit = rfe.fit(X,Y)\nprint(\"Num Features: \", fit.n_features_)\nprint(\"Selected Feature: \", fit.support_)\nprint(\"Feature Ranking: \", fit.ranking_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Selecting features from train and test data\ntrain_data = X[:,fit.support_]\ntest_d = test.values\ntest_data = test_d[:,fit.support_]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 11. Prediction by algorithms"},{"metadata":{},"cell_type":"markdown","source":"Algorithms can be linear or non-linear.  \n**Linear algorithms include:**  \n* Logistic Regression\n* Linear Discriminant Analysis  \n\n**Non Linear algorithms include:**\n* K-Nearest Neigbours\n* Naive bayes\n* Classification and Regression Trees\n* Support vector machines\n"},{"metadata":{},"cell_type":"markdown","source":"### Linear algorithms"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Logistic regression\n## Logistic regression model is a binary dependent variable.\n## Applying model with all features.\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import matthews_corrcoef\nfrom sklearn.metrics import plot_confusion_matrix\n\n# setting up the model\nmodel = LogisticRegression()\n\n# Cross validating the model on data split using Kfold approach\nnum_folds = 10\nseed=42\nkfold = KFold(n_splits=num_folds, random_state=seed,shuffle=True)\nscoring='accuracy'\nresults = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\nprint('Accuracy: ',results.mean()*100)\n\n# Training the model and using it to make a prediction\nmodel.fit(X,Y)\nopt = model.predict(test_d)\n\n# Applying Matthews correlation coefficient on the model\nmcc = matthews_corrcoef(model.predict(X),Y)\nprint('MCC: ',mcc)\n\n# converting model prediction to a dataframe and then a csv\nreport = pd.DataFrame(opt)\nreport.columns = [\"CLASS\"]\nreport.index.name = \"Index\"\nreport.replace(0.0,'False')\nreport['CLASS']=report['CLASS'].map({0.0:False,1.0:True})\n\nreport.to_csv(\"report_lr.csv\")\n# Confirming number of classes in model prediction dataframe\nprint(report['CLASS'].unique())\n# Checking for number of instances in each class of the model prediction dataframe\nprint('False: ',report.groupby('CLASS').size()[0].sum()) \nprint('True: ',report.groupby('CLASS').size()[1].sum())\n\n#plot_confusion_matrix(model, train_data, Y, values_format = '.1g', cmap = 'Blues')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The logistic regression algorithm is then applied on RFE selected features."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Logistic regression\n## Logistic regression models a binary dependent variable.\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import matthews_corrcoef\nfrom sklearn.metrics import plot_confusion_matrix\n\n# setting up the model\nmodel = LogisticRegression()\n\n# Cross validating the model on data split using Kfold approach\nnum_folds = 10\nseed=42\nkfold = KFold(n_splits=num_folds, random_state=seed,shuffle=True)\nscoring='accuracy'\nresults = cross_val_score(model, train_data, Y, cv=kfold, scoring=scoring)\nprint('Accuracy: ',results.mean()*100)\n\n# Training the model and using it to make a prediction\nmodel.fit( train_data,Y)\nopt = model.predict(test_data)\n\n# Applying Matthews correlation coefficient on the model\nmcc = matthews_corrcoef(model.predict(train_data),Y)\nprint('MCC: ',mcc)\n\n# converting prediction to a dataframe and then a csv\nreport = pd.DataFrame(opt)\nreport.columns = [\"CLASS\"]\nreport.index.name = \"Index\"\nreport.replace(0.0,'False')\nreport['CLASS']=report['CLASS'].map({0.0:False,1.0:True})\n\nreport.to_csv(\"report_lr1.csv\")\n# Confirming number of classes in model prediction dataframe\nprint(report['CLASS'].unique())\n# Checking for number of instances in each class of the model prediction dataframe\nprint('False: ',report.groupby('CLASS').size()[0].sum())\nprint('True: ',report.groupby('CLASS').size()[1].sum())\n\n#plot_confusion_matrix(model, train_data, Y, values_format = '.1g', cmap = 'Blues')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From both models, the model run with all features has the higher accuracy and Matthews correlation coefficient."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Linear Discriminant Analysis\n## LDA is used for binary and multiclass classification\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.metrics import matthews_corrcoef\n\n# setting up the model\nmodel = LinearDiscriminantAnalysis()\n\n# Cross validating the model on data split using Kfold approach\nnum_folds = 10\nseed=42\nkfold = KFold(n_splits=num_folds, random_state=seed,shuffle=True)\nscoring='accuracy'\nresults = cross_val_score(model, train_data, Y, cv=kfold, scoring=scoring)\nprint('Accuracy: ',results.mean()*100)\n\n# Training the model and using it to make a prediction\nmodel.fit( train_data,Y)\nopt = model.predict(test_data)\n\n# Applying Matthews correlation coefficient on the model\nmcc = matthews_corrcoef(model.predict(train_data),Y)\nprint('MCC: ',mcc)\n\n# converting model prediction to a dataframe and then a csv\nreport = pd.DataFrame(opt)\nreport.columns = [\"CLASS\"]\nreport.index.name = \"Index\"\nreport.replace(0.0,'False')\nreport['CLASS']=report['CLASS'].map({0.0:False,1.0:True})\n\nreport.to_csv(\"report_lda.csv\")\n# Confirming number of classes in model prediction dataframe\nprint(report['CLASS'].unique())\n# Checking for number of instances in each class of the model prediction dataframe\nprint('False: ',report.groupby('CLASS').size()[0].sum()) \nprint('True: ',report.groupby('CLASS').size()[1].sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Non-linear algorithms"},{"metadata":{"trusted":true},"cell_type":"code","source":"# K-Nearest Neighbours\n## This stores all available cases and classiies new cases based on a similarity measure.\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom numpy import set_printoptions\nfrom sklearn.metrics import matthews_corrcoef\n\nnum_folds = 10\nseed=42\nkfold = KFold(n_splits=num_folds, random_state=seed,shuffle=True)\nmodel = KNeighborsClassifier(n_neighbors=5)\nscoring='accuracy'\nresults = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\nprint('Accuracy: ',results.mean()*100)\n\nmodel.fit(X,Y)\nopt1 = model.predict(test_d)\n\nmcc = matthews_corrcoef(model.predict(X),Y)\nprint('MCC: ',mcc)\n\nreport = pd.DataFrame(opt1)\nreport.columns = [\"CLASS\"]\nreport.index.name = \"Index\"\nreport.replace(0.0,'False')\nreport['CLASS']=report['CLASS'].map({0.0:False,1.0:True})\n\nreport.to_csv(\"report_knn.csv\")\nprint(report['CLASS'].unique())\nprint('False: ',report.groupby('CLASS').size()[0].sum())\nprint('True: ',report.groupby('CLASS').size()[1].sum())\n\nset_printoptions(precision=3)\n#plot_confusion_matrix(model, X, Y, values_format = '.1g', cmap = 'Blues')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Compared to the linear algorithms, the K-nearest neighbours algorithm has a lower percentage accuracy but a higher MCC."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Naive Bayes\n## This calculates the probability of each class and the conditional probability of each class given each input value.\n## Assumes independence of estimated probabilities for new data.\nfrom sklearn.naive_bayes import GaussianNB\n\nnum_folds=10\nseed=42\nkfold = KFold(n_splits=num_folds, random_state=seed)\nmodel = GaussianNB()\nresults = cross_val_score(model, X, Y, cv=kfold)\nprint('Accuracy: ',results.mean()*100)\n\nmodel.fit(X,Y)\nopt = model.predict(test_d)\n\nmcc = matthews_corrcoef(model.predict(X),Y)\nprint('MCC: ',mcc)\n\nreport = pd.DataFrame(opt)\nreport.columns = [\"CLASS\"]\nreport.index.name = \"Index\"\nreport.replace(0.0,'False')\nreport['CLASS']=report['CLASS'].map({0.0:False,1.0:True})\n\nreport.to_csv(\"report_nb.csv\")\nprint(report['CLASS'].unique())\nprint('False: ',report.groupby('CLASS').size()[0].sum())\nprint('True: ',report.groupby('CLASS').size()[1].sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Classiffication and regression trees (CART)\n## CART/decision trees construct a binary tree where split points are chosen greedily by evaluating each attribute and value to minimize a cost function\nfrom sklearn.tree import DecisionTreeClassifier\n\nnum_folds=10\nseed=42\nkfold = KFold(n_splits=num_folds, random_state=seed)\nmodel = DecisionTreeClassifier(random_state=seed)\nresults = cross_val_score(model, X, Y, cv=kfold)\nprint('Accuracy: ',results.mean())\n\nmodel.fit(X,Y)\nopt = model.predict(test_d)\n\nmcc = matthews_corrcoef(model.predict(X), Y)\nprint('MCC: ',mcc)\n\nreport = pd.DataFrame(opt)\nreport.columns = [\"CLASS\"]\nreport.index.name = \"Index\"\nreport.replace(0.0,'False')\nreport['CLASS']=report['CLASS'].map({0.0:False,1.0:True})\n\nreport.to_csv(\"report_cart.csv\")\nprint(report['CLASS'].unique())\nprint('False: ',report.groupby('CLASS').size()[0].sum())\nprint('True: ',report.groupby('CLASS').size()[1].sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"CART with RFE selected features."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Classiffication and regression trees (CART)\n## CART/decision trees construct a binary tree where split points are chosen greedily by evaluating each attribute and value to minimize a cost function\nfrom sklearn.tree import DecisionTreeClassifier\n\nnum_folds=10\nseed=42\nkfold = KFold(n_splits=num_folds, random_state=seed)\nmodel = DecisionTreeClassifier(random_state=40)\nresults = cross_val_score(model, train_data, Y, cv=kfold)\nprint('Accuracy: ',results.mean())\n\nmodel.fit(train_data,Y)\nopt = model.predict(test_data)\n\nmcc = matthews_corrcoef(model.predict(train_data), Y)\nprint('MCC: ',mcc)\n\nreport = pd.DataFrame(opt)\nreport.columns = [\"CLASS\"]\nreport.index.name = \"Index\"\nreport.replace(0.0,'False')\nreport['CLASS']=report['CLASS'].map({0.0:False,1.0:True})\n\nreport.to_csv(\"report_cart_rfe.csv\")\nprint(report['CLASS'].unique())\nprint('False: ',report.groupby('CLASS').size()[0].sum())\nprint('True: ',report.groupby('CLASS').size()[1].sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Support vector machine (SVM) algorithm  \nSVM uses the rbf kernel by default.  \nRBF stands for Radial Basic Function and works best for non-linear problems.\nAs the problem at hand appears linear, the kernel can be changed to `linear` and the performance compared."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Support Vector Machines\n## Using the default rbf kernel\nfrom sklearn.svm import SVC\n\nnum_folds=10\nseed=42\nkfold = KFold(n_splits=num_folds, random_state=seed, shuffle=True)\nmodel = SVC()\nscoring='accuracy'\nresults = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\nprint(results.mean())\n\nmodel.fit(X,Y)\nopt = model.predict(test_d)\n\nmcc = matthews_corrcoef(model.predict(X),Y)\nprint('MCC: ',mcc)\n\n\nreport = pd.DataFrame(opt)\nreport.columns = [\"CLASS\"]\nreport.index.name = \"Index\"\nreport.replace(0.0,'False')\nreport['CLASS']=report['CLASS'].map({0.0:False,1.0:True})\n\nreport.to_csv(\"report_svm_rbf.csv\")\nprint(report['CLASS'].unique())\nprint('False: ',report.groupby('CLASS').size()[0].sum())\nprint('True: ',report.groupby('CLASS').size()[1].sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Support Vector Machines\n## Setting the kernel to linear\nfrom sklearn.svm import SVC\n\nnum_folds=10\nseed=42\nkfold = KFold(n_splits=num_folds, random_state=seed, shuffle=True)\nmodel = SVC(kernel='linear')\nscoring='accuracy'\nresults = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\nprint(results.mean())\n\nmodel.fit(X,Y)\nopt = model.predict(test_d)\n\nmcc = matthews_corrcoef(model.predict(X),Y)\nprint('MCC: ',mcc)\n\n\nreport = pd.DataFrame(opt)\nreport.columns = [\"CLASS\"]\nreport.index.name = \"Index\"\n#report.replace(0.0,'False')\nreport['CLASS']=report['CLASS'].map({0.0:False,1.0:True})\n\nreport.to_csv(\"report_svm_linear.csv\")\n\nprint(report['CLASS'].unique())\nprint('False: ',report.groupby('CLASS').size()[0].sum())\nprint('True: ',report.groupby('CLASS').size()[1].sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the results obtained, the model performs better with the linear kernel than the rbf kernel as indicated by the accuracy and MCC."},{"metadata":{},"cell_type":"markdown","source":"### Learning curves for radial basic function kernel and linear kernel of Support vector machines."},{"metadata":{"trusted":true},"cell_type":"code","source":"## comparing svm rbf and linear kernel\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.svm import SVC\nfrom sklearn.datasets import load_digits\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.model_selection import ShuffleSplit\n\n\ndef plot_learning_curve(estimator, title, X, y, axes=None, ylim=None, cv=None,\n                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):\n   \n    if axes is None:\n        _, axes = plt.subplots(1, 3, figsize=(20, 5))\n\n    axes[0].set_title(title)\n    if ylim is not None:\n        axes[0].set_ylim(*ylim)\n    axes[0].set_xlabel(\"Training examples\")\n    axes[0].set_ylabel(\"Score\")\n\n    train_sizes, train_scores, test_scores, fit_times, _ = \\\n        learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs,\n                       train_sizes=train_sizes,\n                       return_times=True)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    fit_times_mean = np.mean(fit_times, axis=1)\n    fit_times_std = np.std(fit_times, axis=1)\n\n    # Plot learning curve\n    axes[0].grid()\n    axes[0].fill_between(train_sizes, train_scores_mean - train_scores_std,\n                         train_scores_mean + train_scores_std, alpha=0.1,\n                         color=\"r\")\n    axes[0].fill_between(train_sizes, test_scores_mean - test_scores_std,\n                         test_scores_mean + test_scores_std, alpha=0.1,\n                         color=\"g\")\n    axes[0].plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n                 label=\"Training score\")\n    axes[0].plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n                 label=\"Cross-validation score\")\n    axes[0].legend(loc=\"best\")\n\n    # Plot n_samples vs fit_times\n    axes[1].grid()\n    axes[1].plot(train_sizes, fit_times_mean, 'o-')\n    axes[1].fill_between(train_sizes, fit_times_mean - fit_times_std,\n                         fit_times_mean + fit_times_std, alpha=0.1)\n    axes[1].set_xlabel(\"Training examples\")\n    axes[1].set_ylabel(\"fit_times\")\n    axes[1].set_title(\"Scalability of the model\")\n\n    # Plot fit_time vs score\n    axes[2].grid()\n    axes[2].plot(fit_times_mean, test_scores_mean, 'o-')\n    axes[2].fill_between(fit_times_mean, test_scores_mean - test_scores_std,\n                         test_scores_mean + test_scores_std, alpha=0.1)\n    axes[2].set_xlabel(\"fit_times\")\n    axes[2].set_ylabel(\"Score\")\n    axes[2].set_title(\"Performance of the model\")\n\n    return plt\n\n\nfig, axes = plt.subplots(3, 2, figsize=(10, 15))\n\nX, y = load_digits(return_X_y=True)\n\ntitle = \"Learning Curves (SVM, rbf kernel)\"\n# Cross validation with 100 iterations to get smoother mean test and train\n# score curves, each time with 20% data randomly selected as a validation set.\ncv = ShuffleSplit(n_splits=100, test_size=0.2, random_state=0)\n\nestimator = SVC(kernel='rbf')\nplot_learning_curve(estimator, title, X, y, axes=axes[:, 0], ylim=(0.7, 1.01),\n                    cv=cv, n_jobs=4)\n\ntitle = r\"Learning Curves (SVM, linear kernel)\"\n# SVC is more expensive so we do a lower number of CV iterations:\ncv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)\nestimator = SVC(kernel='linear')\nplot_learning_curve(estimator, title, X, y, axes=axes[:, 1], ylim=(0.7, 1.01),\n                    cv=cv, n_jobs=4)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"3 plots are generated for each model: \n* the test and training learning curve  \nThis shows the training and cross-valdation scores with increasing training examples.\n\n* the training samples vs fit times curve   \nThis shows the times required by the models to train with various sizes of training dataset.\n* the fit times vs score curve  \nThis shows how much time was required to train the models.\n\nReviewing learning curves of models during training can be used to diagnose problems with learning, such as an underfit or overfit model, as well as whether the training and validation datasets are suitably representative."},{"metadata":{},"cell_type":"markdown","source":"# 12. Improving performance with Ensembles  \nTo boost accuracy, ensembles can be used.\nThese include;   \n* `Bagging` (bagged decision trees, random forests and extra trees) where multiple models usually of the same type are built from different subsamples of the training dataset.\n* `Boosting` (AdaBoost, stochastic gradient) where multiple models usually of the same type are built, each of which learns to fix the prediction errors of a prior model in the sequence of models.\n* `Voting` which build multiple models typically of differing types and use simple statistics such as mean to combine predictions. \n\nFor this work, `Boosting` and `voting` algorithms are ensembles."},{"metadata":{},"cell_type":"markdown","source":"## Boosting Ensembles"},{"metadata":{"trusted":true},"cell_type":"code","source":"# AdaBoost Classification\n## It generally works by weighting instances in the dataset by how easy or difficult they are to classify.\n## This allows the algorithm to pay more or less attention to these instances in the construction of subsequent models.\nfrom pandas import read_csv\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import AdaBoostClassifier\n\narray = train.values\n# seperate array into input, X and output, Y components\nX = array[:,0:11]\nY = array[:,11]\n\nnum_trees = 30\nseed=42\n\nkfold = KFold(n_splits=10, random_state=seed)\n\nmodel = AdaBoostClassifier(n_estimators=num_trees, random_state=seed)\nresults = cross_val_score(model, X, Y, cv=kfold)\n\nprint(results.mean())\n\nmodel.fit(X,Y)\nopt = model.predict(test_d)\n\nmcc = matthews_corrcoef(model.predict(X),Y)\nprint('MCC: ',mcc)\n\n\nreport = pd.DataFrame(opt)\nreport.columns = [\"CLASS\"]\nreport.index.name = \"Index\"\nreport.replace(0.0,'False')\nreport['CLASS']=report['CLASS'].map({0.0:False,1.0:True})\n\nreport.to_csv(\"report_AB.csv\")\nprint(report['CLASS'].unique())\nprint('False: ',report.groupby('CLASS').size()[0].sum())\nprint('True: ',report.groupby('CLASS').size()[1].sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Stochastic Gradient Boosting classification\n\nfrom pandas import read_csv\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import GradientBoostingClassifier\n\n\nseed = 42\nnum_trees = 100\n\nkfold = KFold(n_splits=10, random_state=seed)\nmodel = GradientBoostingClassifier(n_estimators=num_trees, random_state=seed)\nresults = cross_val_score(model, X, Y, cv=kfold)\nprint(results.mean())\n\nmodel.fit(X,Y)\nopt = model.predict(test_d)\n\nmcc = matthews_corrcoef(model.predict(X),Y)\nprint('MCC: ',mcc)\n\n\nreport = pd.DataFrame(opt)\nreport.columns = [\"CLASS\"]\nreport.index.name = \"Index\"\nreport.replace(0.0,'False')\nreport['CLASS']=report['CLASS'].map({0.0:False,1.0:True})\n\nreport.to_csv(\"report_GBC.csv\")\nprint(report['CLASS'].unique())\nprint('False: ',report.groupby('CLASS').size()[0].sum())\nprint('True: ',report.groupby('CLASS').size()[1].sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Stochastic X Gradient Boosting Classification\nfrom pandas import read_csv\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import confusion_matrix\n\nseed = 42\nnum_trees = 100\n\nkfold = KFold(n_splits=10, random_state=seed)\nmodel = XGBClassifier(n_estimators=num_trees, random_state=seed)\nresults = cross_val_score(model, X, Y, cv=kfold)\nprint(results.mean())\n\nmodel.fit(X,Y)\nopt = model.predict(test_d)\n\nmcc = matthews_corrcoef(model.predict(X),Y)\nprint('MCC: ',mcc)\n\n\nreport = pd.DataFrame(opt)\nreport.columns = [\"CLASS\"]\nreport.index.name = \"Index\"\nreport['CLASS']=report['CLASS'].map({0.0:False,1.0:True})\nreport.to_csv(\"report_XGB.csv\")\n\nprint(report['CLASS'].unique())\nprint('False: ',report.groupby('CLASS').size()[0].sum())\nprint('True: ',report.groupby('CLASS').size()[1].sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Voting Ensemble"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Voting Ensemble for Classification\nfrom pandas import read_csv\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import VotingClassifier\n\n# create the sub models\nestimators = []\nestimators.append(('LR', LogisticRegression()))\nestimators.append(('LDA', LinearDiscriminantAnalysis()))\nestimators.append(('KNN', KNeighborsClassifier()))\nestimators.append(('NB', GaussianNB()))\nestimators.append(('CART', DecisionTreeClassifier()))\nestimators.append(('SVM_rbf', SVC()))\nestimators.append(('SVM_linear', SVC(kernel='linear')))\nestimators.append(('AB', AdaBoostClassifier())) \nestimators.append(('GBC', GradientBoostingClassifier()))\nestimators.append(('XGB', XGBClassifier()))\n\n# create the ensemble model\nensemble = VotingClassifier(estimators)\nresults = cross_val_score(ensemble, X, Y, cv=kfold)\nprint(results.mean())\n\nensemble.fit(X,Y)\nopt = ensemble.predict(test_d)\n\nmcc = matthews_corrcoef(ensemble.predict(X),Y)\nprint('MCC: ',mcc)\n\n\nreport = pd.DataFrame(opt)\nreport.columns = [\"CLASS\"]\nreport.index.name = \"Index\"\nreport['CLASS']=report['CLASS'].map({0.0:False,1.0:True})\nreport.to_csv(\"report_vote.csv\")\n\nprint(report['CLASS'].unique())\nprint('False: ',report.groupby('CLASS').size()[0].sum())\nprint('True: ',report.groupby('CLASS').size()[1].sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 13. Comparing Machine Learning Algorithms used"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compare perfomance of the different algorithms used\nfrom matplotlib import pyplot\nfrom sklearn.metrics import matthews_corrcoef\n\nmodels = []\nmodels.append(('LR', LogisticRegression()))\nmodels.append(('LDA', LinearDiscriminantAnalysis()))\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('NB', GaussianNB()))\nmodels.append(('CART', DecisionTreeClassifier(random_state=40)))\nmodels.append(('SVM_rbf', SVC()))\nmodels.append(('SVM_linear', SVC(kernel='linear')))\nmodels.append(('AB', AdaBoostClassifier())) \nmodels.append(('GBC', GradientBoostingClassifier()))\nmodels.append(('XGB', XGBClassifier()))\nmodels.append(('VC',VotingClassifier(estimators)))\n\nresults = []\nnames = []\nscoring = 'accuracy'\n\nnum_folds=10\nseed=42\nfor name, model in models:\n    kfold = KFold(n_splits=num_folds, random_state=seed, shuffle=True)\n    cv_results = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n    model.fit(X,Y)\n    results.append(cv_results)\n    names.append(name)\n    msg = (name, cv_results.mean(),'MCC: ',matthews_corrcoef(model.predict(X),Y))\n    print(msg)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# A plot for comparing the different algorithms\nfig = pyplot.figure()\nfig.suptitle('Algorithm Comparison')\nax = fig.add_subplot(111)\npyplot.boxplot(results)\nax.set_xticklabels(names)\nplt.xticks(rotation=45)\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"From the above graph, it can be observed that models implementing gradient boost classification have higher percentage accuracies.  \nHowever, other metrics have to be put in place to measure performance of a model such as Matthews correlation coefficient, area under the ROC curve or logarithmic loss since a single parameter is not enough to measure model performance and thus assist in model choice."},{"metadata":{},"cell_type":"markdown","source":"# *Grid Search*  \nGrid Search is used to optimize estimators by hyper-parameter tuning. Hyper-parameters are parameters are parameters that are not directly learnt with estimators but are instead passed as arguments to the constructor of the estimator classes. \n\nThe grid search provided by `GridSearchCV` exhaustively considers all parameter combinations from a grid of parameter values specified with the `param_grid` parameter.\nThe parameter combination with the best score is retained."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Grid search\n\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import GradientBoostingClassifier\n\nmodel = GradientBoostingClassifier()\n\n# establishing parameters to grid-search.\n## These change depending on the model.\ncriterion = ['friedman_mse', 'mse','mae']\nloss = ['deviance','exponential']\nn_estimators = [50,100,150]\n\ngrid = GridSearchCV(estimator=model, cv=3, param_grid=dict(criterion=criterion,loss=loss, n_estimators=n_estimators))\ngrid.fit(X,Y)\n\nprint('Best score:', grid.best_score_)\nprint('Best parameters:', grid.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Using parameters selected by the Grid search\nmodel = GradientBoostingClassifier(criterion='friedman_mse', loss='deviance',n_estimators=50)\nmodel.fit(X,Y)\n\nopt = model.predict(test_d)\n\nmcc = matthews_corrcoef(model.predict(X),Y)\nprint('MCC: ',mcc)\n\n\nreport = pd.DataFrame(opt)\nreport.columns = [\"CLASS\"]\nreport.index.name = \"Index\"\nreport.replace(0.0,'False')\nreport['CLASS']=report['CLASS'].map({0.0:False,1.0:True})\n\nreport.to_csv(\"report_GBC_GS.csv\")\nprint(report['CLASS'].unique())\nprint('False: ',report.groupby('CLASS').size()[0].sum())\nprint('True: ',report.groupby('CLASS').size()[1].sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# *References*\n1. Gupta, S., & Gupta, A. (2018). Handling class overlapping to detect noisy instances in classification. The Knowledge Engineering Review, 33, E8. doi:10.1017/S0269888918000115\n2. https://github.com/atwine/ace-class-notes/blob/master/Thur%2020%20Feb/ACE_ClassML%20Pipeline_edit.ipynb\n3. https://machinelearningmastery.com/naive-bayes-for-machine-learning/\n4. https://scikit-learn.org/stable/auto_examples/model_selection/plot_learning_curve.html\n5. https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n6. https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier\n7. https://towardsdatascience.com/introduction-to-logistic-regression-66248243c148htmlhtmlhtmlhtmlhtmlhtmlhtmlhtml\n8. https://www.analyticsvidhya.com/blog/2017/09/understaing-support-vector-machine-example-code/"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}